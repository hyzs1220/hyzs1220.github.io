

<!DOCTYPE html>
<html lang="zh-CN" data-default-color-scheme=auto>



<head>
  <meta charset="UTF-8">
  <link rel="apple-touch-icon" sizes="76x76" href="/img/fluid.png">
  <link rel="icon" href="/img/fluid.png">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=5.0, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  
  <meta name="theme-color" content="#2f4154">
  <meta name="author" content="hyzs1220">
  <meta name="keywords" content="">
  
    <meta name="description" content="论文的阅读笔记： 《 Zero-Reference Deep Curve Estimation for Low-Light Image Enhancement 》[code] ,  CVPR 2020 《 Learning to Restore Low-Light Images via Decomposition-and-Enhancement 》，CVPR 2020">
<meta property="og:type" content="article">
<meta property="og:title" content="论文阅读-0x06">
<meta property="og:url" content="http://example.com/2020/12/01/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-0x06/index.html">
<meta property="og:site_name" content="hyzsのblog">
<meta property="og:description" content="论文的阅读笔记： 《 Zero-Reference Deep Curve Estimation for Low-Light Image Enhancement 》[code] ,  CVPR 2020 《 Learning to Restore Low-Light Images via Decomposition-and-Enhancement 》，CVPR 2020">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://example.com/2020/12/01/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-0x06/1.png">
<meta property="og:image" content="http://example.com/2020/12/01/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-0x06/2.png">
<meta property="og:image" content="http://example.com/2020/12/01/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-0x06/3.png">
<meta property="og:image" content="http://example.com/2020/12/01/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-0x06/4.png">
<meta property="og:image" content="http://example.com/2020/12/01/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-0x06/5.png">
<meta property="og:image" content="http://example.com/2020/12/01/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-0x06/6.png">
<meta property="og:image" content="http://example.com/2020/12/01/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-0x06/7.png">
<meta property="og:image" content="http://example.com/2020/12/01/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-0x06/1.jpg">
<meta property="og:image" content="http://example.com/2020/12/01/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-0x06/8.png">
<meta property="og:image" content="http://example.com/2020/12/01/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-0x06/9.png">
<meta property="og:image" content="http://example.com/2020/12/01/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-0x06/10.png">
<meta property="og:image" content="http://example.com/2020/12/01/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-0x06/11.png">
<meta property="og:image" content="http://example.com/2020/12/01/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-0x06/12.png">
<meta property="og:image" content="http://example.com/2020/12/01/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-0x06/15.png">
<meta property="og:image" content="http://example.com/2020/12/01/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-0x06/13.png">
<meta property="og:image" content="http://example.com/2020/12/01/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-0x06/14.png">
<meta property="article:published_time" content="2020-12-01T13:27:15.000Z">
<meta property="article:modified_time" content="2024-03-23T04:15:56.579Z">
<meta property="article:author" content="hyzs1220">
<meta property="article:tag" content="图像增强与图像恢复">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:image" content="http://example.com/2020/12/01/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-0x06/1.png">
  
  
  
  <title>论文阅读-0x06 - hyzsのblog</title>

  <link  rel="stylesheet" href="https://lib.baomitu.com/twitter-bootstrap/4.6.1/css/bootstrap.min.css" />



  <link  rel="stylesheet" href="https://lib.baomitu.com/github-markdown-css/4.0.0/github-markdown.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/hint.css/2.7.0/hint.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.css" />



<!-- 主题依赖的图标库，不要自行修改 -->
<!-- Do not modify the link that theme dependent icons -->

<link rel="stylesheet" href="//at.alicdn.com/t/font_1749284_hj8rtnfg7um.css">



<link rel="stylesheet" href="//at.alicdn.com/t/font_1736178_lbnruvf0jn.css">


<link  rel="stylesheet" href="/css/main.css" />


  <link id="highlight-css" rel="stylesheet" href="/css/highlight.css" />
  
    <link id="highlight-css-dark" rel="stylesheet" href="/css/highlight-dark.css" />
  




  <script id="fluid-configs">
    var Fluid = window.Fluid || {};
    Fluid.ctx = Object.assign({}, Fluid.ctx)
    var CONFIG = {"hostname":"example.com","root":"/","version":"1.9.7","typing":{"enable":true,"typeSpeed":70,"cursorChar":"_","loop":false,"scope":[]},"anchorjs":{"enable":true,"element":"h1,h2,h3,h4,h5,h6","placement":"left","visible":"hover","icon":""},"progressbar":{"enable":true,"height_px":3,"color":"#29d","options":{"showSpinner":false,"trickleSpeed":100}},"code_language":{"enable":true,"default":"TEXT"},"copy_btn":true,"image_caption":{"enable":true},"image_zoom":{"enable":true,"img_url_replace":["",""]},"toc":{"enable":true,"placement":"right","headingSelector":"h1,h2,h3,h4,h5,h6","collapseDepth":0},"lazyload":{"enable":true,"loading_img":"/img/loading.gif","onlypost":false,"offset_factor":2},"web_analytics":{"enable":false,"follow_dnt":true,"baidu":null,"google":{"measurement_id":null},"tencent":{"sid":null,"cid":null},"woyaola":null,"cnzz":null,"leancloud":{"app_id":null,"app_key":null,"server_url":null,"path":"window.location.pathname","ignore_local":false}},"search_path":"/local-search.xml","include_content_in_search":true};

    if (CONFIG.web_analytics.follow_dnt) {
      var dntVal = navigator.doNotTrack || window.doNotTrack || navigator.msDoNotTrack;
      Fluid.ctx.dnt = dntVal && (dntVal.startsWith('1') || dntVal.startsWith('yes') || dntVal.startsWith('on'));
    }
  </script>
  <script  src="/js/utils.js" ></script>
  <script  src="/js/color-schema.js" ></script>
  


  
<meta name="generator" content="Hexo 6.3.0"></head>


<body>
  

  <header>
    

<div class="header-inner" style="height: 70vh;">
  <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand" href="/">
      <strong>hyzsのblog</strong>
    </a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/" target="_self">
                <i class="iconfont icon-home-fill"></i>
                <span>首页</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/archives/" target="_self">
                <i class="iconfont icon-archive-fill"></i>
                <span>归档</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/categories/" target="_self">
                <i class="iconfont icon-category-fill"></i>
                <span>分类</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/tags/" target="_self">
                <i class="iconfont icon-tags-fill"></i>
                <span>标签</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/about/" target="_self">
                <i class="iconfont icon-user-fill"></i>
                <span>关于</span>
              </a>
            </li>
          
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" target="_self" href="javascript:;" data-toggle="modal" data-target="#modalSearch" aria-label="Search">
              <i class="iconfont icon-search"></i>
            </a>
          </li>
          
        
        
          <li class="nav-item" id="color-toggle-btn">
            <a class="nav-link" target="_self" href="javascript:;" aria-label="Color Toggle">
              <i class="iconfont icon-dark" id="color-toggle-icon"></i>
            </a>
          </li>
        
      </ul>
    </div>
  </div>
</nav>

  

<div id="banner" class="banner" parallax=true
     style="background: url('/img/default.png') no-repeat center center; background-size: cover;">
  <div class="full-bg-img">
    <div class="mask flex-center" style="background-color: rgba(0, 0, 0, 0.3)">
      <div class="banner-text text-center fade-in-up">
        <div class="h2">
          
            <span id="subtitle" data-typed-text="论文阅读-0x06"></span>
          
        </div>

        
          
  <div class="mt-3">
    
    
      <span class="post-meta">
        <i class="iconfont icon-date-fill" aria-hidden="true"></i>
        <time datetime="2020-12-01 21:27" pubdate>
          2020年12月1日 晚上
        </time>
      </span>
    
  </div>

  <div class="mt-1">
    
      <span class="post-meta mr-2">
        <i class="iconfont icon-chart"></i>
        
          6.2k 字
        
      </span>
    

    
      <span class="post-meta mr-2">
        <i class="iconfont icon-clock-fill"></i>
        
        
        
          52 分钟
        
      </span>
    

    
    
  </div>


        
      </div>

      
    </div>
  </div>
</div>

</div>

  </header>

  <main>
    
      

<div class="container-fluid nopadding-x">
  <div class="row nomargin-x">
    <div class="side-col d-none d-lg-block col-lg-2">
      

    </div>

    <div class="col-lg-8 nopadding-x-md">
      <div class="container nopadding-x-md" id="board-ctn">
        <div id="board">
          <article class="post-content mx-auto">
            <h1 id="seo-header">论文阅读-0x06</h1>
            
            
              <div class="markdown-body">
                
                <p>论文的阅读笔记：</p>
<p>《 Zero-Reference Deep Curve Estimation for Low-Light Image Enhancement 》<a target="_blank" rel="noopener" href="https://github.com/Li-Chongyi/Zero-DCE">[code]</a> ,  CVPR 2020</p>
<p>《 Learning to Restore Low-Light Images via Decomposition-and-Enhancement 》，CVPR 2020</p>
<span id="more"></span>
<h2 id="Zero-Reference-Deep-Curve-Estimation-for-Low-Light-Image-Enhancement">Zero-Reference Deep Curve Estimation for Low-Light Image Enhancement</h2>
<h3 id="Abstract">Abstract</h3>
<ul>
<li>本文提出了一个新颖的方法—— 零参考深度曲线估计 (Zero-Reference Deep Curve Estimation，Zero-DCE)，通过深层网络将光增强公式化为特定于图像的曲线估计任务（ 图像作为输入，曲线作为输出 ）</li>
<li>训练一个轻量级深度网络DCE-Net来估计像素和高阶曲线，以便对给定图像进行动态范围调整</li>
<li>Zero-DCE的优点在于它对参考图像的宽松假设，即在训练过程中不需要任何配对或未配对的数据，而是通过一组精心制定的非参考损失函数来实现的，隐式地测量增强质量并驱动网络的学习</li>
<li>其方法是有效的，因为图像增强可以实现直观和简单的非线性曲线映射</li>
<li>该方法以一个弱光图像作为输入，生成高阶曲线作为输出。这些曲线然后用于对输入的动态范围进行像素调整，以获得增强的图像</li>
<li>同时作者发现该曲线是可微的，因此可以通过一个深度卷积神经网络来学习曲线的可调参数</li>
<li>并且该方法是零参考的，即在训练过程中不需要任何配对甚至是非配对的数据，仅使用CNN和GAN方法，以及一套特别设计的非参考损失函数
<ul>
<li>空间一致性损失、曝光控制损失、色彩稳定性损失和照明平滑损失，所有这些都考虑到光增强的多因素</li>
</ul>
</li>
<li>主要贡献总结如下：
<ul>
<li>提出了第一种不依赖于配对和非配对训练数据的弱光增强网络，避免了过拟合的风险，可以很好地适用于各种照明条件</li>
<li>设计了一种特定于图像的曲线，它可以迭代地逼近像素和高阶曲线（ pixel-wise and higher-order ）。这种图像特异性曲线可以有效地在较宽的动态范围内进行映射</li>
<li>展示了在没有参考图像的情况下，通过任务特异性的非参考损失函数来间接评估增强质量，训练深度图像增强模型的潜力</li>
</ul>
</li>
</ul>
<p>相关工作</p>
<ul>
<li>传统方法
<ul>
<li>HE-based，基于图像直方图分布，在全局和局部对图像的直方图分布进行调整</li>
<li>基于Retinex 理论的方法 ，分解一个图像的反射率和光照。反射分量通常被假定在任何光照条件下是一致的</li>
<li>本文提出的Zero-DCE方法通过图像特定的曲线映射产生增强的结果</li>
<li>Zero-DCE是一种纯数据驱动的方法，在设计非参考损耗函数时考虑了多个光增强因素，因此具有更好的鲁棒性，更宽的图像动态范围调整，更低的计算量</li>
</ul>
</li>
<li>数据驱动方法
<ul>
<li>大多数基于cnn的解决方案依赖配对数据进行监督训练，因此它们是资源密集型的。通常情况下，配对的数据是通过自动光降解、在数据捕捉期间改变相机的设置、或通过图像润色合成数据来穷尽地收集的</li>
<li>因此这些基于以上这种配对数据的方法通常是high cost(数据收集)，并且数据集还存在一些虚假/不真实的图片(人工合成)，这样会影响到模型的泛化能力(当模型应用在真实世界的不同光照下，通常会出现色偏等现象)</li>
<li>基于GAN的无监督方法具有消除配对数据用于训练的优点，不过需要仔细选择未配对的训练数据</li>
<li>本文提出的Zero-DCE相比于上面的data-driven方法有三个优点</li>
<li>探索了一种全新的学习策略(<strong>zero reference</strong>)，消除了对成对和非成对数据的需求</li>
<li>设置非参考损失函数来对输出图像进行间接的评估</li>
<li>提出的方法是高效和经济的</li>
</ul>
</li>
</ul>
<h3 id="Methodology——DCE-Net">Methodology——DCE-Net</h3>
<p><img src="/2020/12/01/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-0x06/1.png" srcset="/img/loading.gif" lazyload alt></p>
<ul>
<li>DCE-Net：用于估计给定输入图像的一组最佳拟合光增强曲线(LE-curves)</li>
<li>然后框架迭代应用曲线， 对输入图像的RGB通道中所有像素进行映射，从而获得最后的增强图像</li>
<li>关键组件：LE-curve、DCE-Net和non-reference loss functions</li>
</ul>
<h4 id="Light-Enhancement-Curve-LE-curve">Light-Enhancement Curve (LE-curve)</h4>
<ul>
<li>尝试设计一种曲线，可以自动将弱光图像映射到增强版本，其中自适应曲线参数完全依赖于输入图像
<ul>
<li>增强图像的每个像素值都应在归一化范围内[0,1]，避免溢出截断造成的信息损失</li>
<li>设计的曲线应该是单调的，从而保留相邻像素间的差异(对比度)</li>
<li>曲线应该尽可能地简单和可微，使得其在梯度反向传播过程中是可导的</li>
</ul>
</li>
<li>为此设计了一个二次曲线
<ul>
<li>$\operatorname{LE}(I(\mathbf{x}) ; \alpha)=I(\mathbf{x})+\alpha I(\mathbf{x})(1-I(\mathbf{x}))$</li>
<li>x表示像素坐标，$\operatorname{LE}(I(\mathbf{x}) ; \alpha)$ 是输入I(x)的增强结果</li>
<li>α ∈ [−1,1] 是可训练曲线参数，调节LE-curve的大小，控制曝光水平</li>
<li>每个像素归一化到[0,1]，操作都是在像素上进行，将LE-curve应用到RGB三通道上，从而保留固有色彩，同时避免过拟合</li>
<li><img src="/2020/12/01/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-0x06/2.png" srcset="/img/loading.gif" lazyload alt></li>
</ul>
</li>
<li>可以看出来LE-curve损失符合上述三个目标的。此外，LE-curve能够增大或减小输入图像的动态范围
<ul>
<li>不仅有助于增强弱光区域，也有助于消除过度曝光的伪影</li>
</ul>
</li>
<li>Higher-Order Curve
<ul>
<li>对LE-curve反复使用可以得到其高阶表示形式</li>
<li>$\operatorname{LE_n}(\mathbf{x} )= \operatorname{LE_{n-1}}(\mathbf{x}) +\alpha_n \operatorname{LE_{n-1}}(\mathbf{x}) (1-\operatorname{LE_{n-1}}(\mathbf{x}))$
<ul>
<li>n为迭代次数，控制曲率（更大的调节能力，更大的曲率），作者将n的值设为8，可以满足大多数情况下的处理</li>
</ul>
</li>
</ul>
</li>
<li>Pixel-Wise Curve
<ul>
<li>高阶曲线可以在比较宽的范围内进行图像调整，由于所有像素都使用了 α ，所以仍然相当于进行了全局操作，往往会对局部区域进行过强或过弱的增强</li>
<li>为此采用逐像素参数的方式来表示出图像的坐标，即给定输入图像的每一个像素都有一个对应的曲线，以最佳拟合的坐标进行动态范围的调整</li>
<li>$L E_{n}(\mathbf{x})=L E_{n-1}(\mathbf{x})+\mathcal{A}<em>{n}(\mathbf{x}) L E</em>{n-1}(\mathbf{x})\left(1-L E_{n-1}(\mathbf{x})\right)$
<ul>
<li>其中A为与给定图像大小相同的参数图</li>
<li>假设一个局部区域的像素具有相同的强度(同样是相同的调整曲线)，这样在输出结果中相邻的像素仍然保持单调的关系</li>
<li>实验中作者这里使用的迭代次数为8，为了方便理解代码化简一下，大概形式为$x_n = x_{n-1} + A_n*(x^2 - x)$， 然后在这个基础上进行迭代</li>
</ul>
</li>
<li><img src="/2020/12/01/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-0x06/3.png" srcset="/img/loading.gif" lazyload alt></li>
<li>上图中给出了三个通道的估计曲线参数图的例子。从图中可以看出，不同通道的最佳拟合参数映射具有相似的调整趋势，但值不同，说明了低光图像的三个通道之间的相关性和差异性</li>
</ul>
</li>
</ul>
<h4 id="DCE-Net">DCE-Net</h4>
<ul>
<li>学习输入图像与其最佳拟合曲线参数图之间的映射</li>
<li>网络结构
<ul>
<li>具有对称连接的七个卷积层。每层由32个大小为3×3的卷积核和stride=1以及ReLU激活</li>
<li>抛弃了破坏相邻像素关系的下采样和批处理归一化层</li>
<li>在最后一个卷积层之后是Tanh激活函数，它为**8次迭代(n = 8)**生成24个参数图，其中每次迭代需要3个通道的曲线参数图</li>
</ul>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 代码实现比较简单</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x</span>):<br><br>	x1 = self.relu(self.e_conv1(x))<br>	<span class="hljs-comment"># p1 = self.maxpool(x1)</span><br>	x2 = self.relu(self.e_conv2(x1))<br>	<span class="hljs-comment"># p2 = self.maxpool(x2)</span><br>	x3 = self.relu(self.e_conv3(x2))<br>	<span class="hljs-comment"># p3 = self.maxpool(x3)</span><br>	x4 = self.relu(self.e_conv4(x3))<br><br>	x5 = self.relu(self.e_conv5(torch.cat([x3,x4],<span class="hljs-number">1</span>)))<br>	<span class="hljs-comment"># x5 = self.upsample(x5)</span><br>	x6 = self.relu(self.e_conv6(torch.cat([x2,x5],<span class="hljs-number">1</span>)))<br><br>	x_r = F.tanh(self.e_conv7(torch.cat([x1,x6],<span class="hljs-number">1</span>)))<br>	<span class="hljs-comment"># 迭代了8次，24个参数层，三个一组，分位8组</span><br>	r1,r2,r3,r4,r5,r6,r7,r8 = torch.split(x_r, <span class="hljs-number">3</span>, dim=<span class="hljs-number">1</span>)<br><br>	<span class="hljs-comment"># 迭代增强部分</span><br>	x = x + r1*(torch.<span class="hljs-built_in">pow</span>(x,<span class="hljs-number">2</span>)-x)<br>	x = x + r2*(torch.<span class="hljs-built_in">pow</span>(x,<span class="hljs-number">2</span>)-x)<br>	x = x + r3*(torch.<span class="hljs-built_in">pow</span>(x,<span class="hljs-number">2</span>)-x)<br>    <span class="hljs-comment"># 取了一个中间的增强结果</span><br>	enhance_image_1 = x + r4*(torch.<span class="hljs-built_in">pow</span>(x,<span class="hljs-number">2</span>)-x)<br>	x = enhance_image_1 + r5*(torch.<span class="hljs-built_in">pow</span>(enhance_image_1,<span class="hljs-number">2</span>)-enhance_image_1)<br>	x = x + r6*(torch.<span class="hljs-built_in">pow</span>(x,<span class="hljs-number">2</span>)-x)<br>	x = x + r7*(torch.<span class="hljs-built_in">pow</span>(x,<span class="hljs-number">2</span>)-x)<br>	enhance_image = x + r8*(torch.<span class="hljs-built_in">pow</span>(x,<span class="hljs-number">2</span>)-x)<br>	r = torch.cat([r1,r2,r3,r4,r5,r6,r7,r8],<span class="hljs-number">1</span>)<br>	<span class="hljs-keyword">return</span> enhance_image_1,enhance_image,r<br></code></pre></td></tr></table></figure>
<h4 id="Non-Reference-Loss-Functions">Non-Reference Loss Functions</h4>
<ul>
<li>为了在DCE-Net中实现零参考学习，提出了一组可区分的非参考损失，可以用来评估增强图像的质量</li>
<li>Spatial Consistency Loss（空间一致性损失）
<ul>
<li>通过保持输入图像与增强图像相邻区域的差异（对比度）来促进增强图像的空间一致性</li>
<li>$L_{s p a}=\frac{1}{K} \sum_{i=1}^{K} \sum_{j \in \Omega(i)}\left(\left|\left(Y_{i}-Y_{j}\right)\right|-\left|\left(I_{i}-I_{j}\right)\right|\right)^{2}$
<ul>
<li>K为局部区域的数量，Ω(i)是以i为中心的四个相邻区域（上下左右）， Y和I分别为增强图像和输入图像的局部区域平均强度值。<strong>这个局部区域的Size经验性地设置为4x4</strong></li>
</ul>
</li>
</ul>
</li>
<li>Exposure Control Loss（曝光控制损失）
<ul>
<li>为了抑制曝光不足/过度区域</li>
<li>衡量局部区域的平均强度值到良好曝光水平E之间的距离。遵循现有的做法，将E设为RGB颜色空间的灰度，在本文中作者将其设为0.6</li>
<li>$L_{e x p}=\frac{1}{M}  \sum_{k = 1}^M \left(\left|\left(Y_{k}-E\right)\right|\right)$
<ul>
<li>M为大小为16×16的不重叠局部区域的个数，Y为增强图像中某个局部区域的平均强度值</li>
</ul>
</li>
</ul>
</li>
<li>Color Constancy Loss（色彩恒等损失）
<ul>
<li>根据灰度世界的颜色恒等假设
<ul>
<li>即每个传感器通道的颜色在整个图像上平均为灰度</li>
</ul>
</li>
<li>用于校正增强图像中可能出现的颜色偏差，并建立了三个调整通道之间的关系</li>
<li>$L_{c o l}=\sum_{\forall(p, q) \in \varepsilon}\left(J^{p}-J^{q}\right)^{2}, \varepsilon={(R, G),(R, B),(G, B)}$
<ul>
<li>式中$j^p$表示增强图像通道p的平均强度值，(p,q)表示一对通道</li>
</ul>
</li>
</ul>
</li>
<li>Illumination Smoothness Loss（光照平滑度损失）
<ul>
<li>为了保持相邻像素间的单调关系</li>
<li>$L_{t v_{\mathcal{A}}}=\frac{1}{N} \sum_{n=1}^{N} \sum_{c \in \xi}\left(\left|\nabla_{x} \mathcal{A}<em>{n}^{c}\right|+\nabla</em>{y} \mathcal{A}_{n}^{c} \mid\right)^{2}, \xi={R, G, B}$
<ul>
<li>N为迭代次数，$\nabla_{x}, \nabla_{y}$分别代表水平和垂直方向的梯度操作</li>
</ul>
</li>
</ul>
</li>
<li>Total Loss
<ul>
<li>$L_{total} = L_{spa} + L_{exp} + W_{col}L_{col} + W_{t v_{\mathcal{A}}}L_{t v_{\mathcal{A}}}$</li>
<li>不过源码中曝光控制损失前面也是有个权重参数</li>
</ul>
</li>
<li>代码实现有点多，不过基本都是上面这些公式的具体实现</li>
</ul>
<h3 id="Experiment">Experiment</h3>
<ul>
<li>batch size= 8，2080Ti，使用(0, 0.02)高斯函数初始化权重，bias初始为常量，使用ADAM优化器(lr=1e-4)，$W_{col}$ 为0.5，$W_{t v_{\mathcal{A}}}$ 为20，从而平衡loss间尺度差距</li>
<li>消融实验</li>
<li><img src="/2020/12/01/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-0x06/4.png" srcset="/img/loading.gif" lazyload alt></li>
<li><img src="/2020/12/01/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-0x06/5.png" srcset="/img/loading.gif" lazyload alt>
<ul>
<li>L-F-N代表Zero-DCE有L层卷积，每层有F个feature map以及迭代次数为N</li>
</ul>
</li>
<li><img src="/2020/12/01/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-0x06/6.png" srcset="/img/loading.gif" lazyload alt>
<ul>
<li>使用不同数据集对Zero-DCE进行训练</li>
<li>原训练集中(2422)的900张low-light图像Zero-DCELow</li>
<li>DARK FACE中9000张未标注的low-light图像Zero-DCELargeL</li>
<li>SICE数据集Part 1 and Part2组合的4800张多重曝光图像Zero-DCELargeLH</li>
<li>在去除过度曝光的训练数据后，使用更多的低光图像，Zero-DCE仍倾向于过度增强光照良好的区域(如人脸)。这些结果表明了在我们的网络训练过程中使用多暴露训练数据的合理性和必要性。此外，当使用更多的多曝光训练数据时，ZeroDCE可以更好地恢复暗区，如图6(e)所示</li>
</ul>
</li>
<li>Benchmark Evaluations（基准评价）
<ul>
<li>在多个数据集(NPE LIME MEF DICM VV以及SICE的Part2)上与目前SOAT的方法进行了对比</li>
<li><img src="/2020/12/01/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-0x06/7.png" srcset="/img/loading.gif" lazyload alt></li>
<li>在实验数据上面，基本上就是又快又好</li>
<li><img src="/2020/12/01/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-0x06/1.jpg" srcset="/img/loading.gif" lazyload alt>
<ul>
<li>用户研究(US)↑/感知指数(PI)↓在图像集(NPE、LIME、MEF、DICM、VV)上得分，US得分越高，说明人主观视觉质量越好，PI值越低，说明感知质量越好。在每种情况下，最好的结果是红色的，而第二好的结果是蓝色的。</li>
</ul>
</li>
<li><img src="/2020/12/01/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-0x06/8.png" srcset="/img/loading.gif" lazyload alt></li>
</ul>
</li>
</ul>
<h3 id="😝😜😋">😝😜😋</h3>
<p>以后想着看一篇论文多少都写一点自己的感受，不然感觉没啥吸收内容。</p>
<p>这篇论文感觉比较精彩的地方就是图像增强的实现方式以及那几个损失函数的组合使用，这样就能达到一种无参考的函数优化，感觉特别新颖，很有创新型。网络结构也不是特别复杂，但是能想到这种方式真的很强，好好记录一下，以后说不定会翻出来再看看。</p>
<h2 id="Learning-to-Restore-Low-Light-Images-via-Decomposition-and-Enhancement">Learning to Restore Low-Light Images via Decomposition-and-Enhancement</h2>
<h3 id="Abstract-2">Abstract</h3>
<ul>
<li>弱光图像（Low-light images）存在两个问题
<ul>
<li>能见度低(即像素值小)</li>
<li>由于低信噪比，噪声十分明显，干扰了图像内容</li>
</ul>
</li>
<li>不过大多数现有的低光图像增强方法都是从可忽略噪声的数据集学习的（像上面那篇）</li>
<li>这些方法在增强微光图像并去除其噪声是存在问题的，我们观察到噪声在不同的频率层表现出不同的对比度，并且<strong>在低频层比在高频层更容易检测到噪声</strong></li>
<li>在此基础上提出了一种新的网络，该网络首先在低频层学习恢复目标图像，然后基于恢复的目标图像增加高频细节</li>
<li>作者还准备了一个新的具有真实噪声的弱光图像数据集</li>
<li>由于量化的原因，在标准RGB (sRGB, 24位/像素)空间中，弱光图像的可见度与人的感知不匹配</li>
<li>本文解决了弱光sRGB图像增强问题
<ul>
<li>涉及两个问题:图像增强和去噪</li>
<li>动机基于两个观察
<ul>
<li>图像低频层保存了更多的信息，如物体和颜色，与比图像高频层(图1(d))相比，受噪声的影响较小(图1©)，这说明对低频图像层进行增强要比直接对整个图像进行增强容易</li>
<li>图像基元的固有维数非常低，使得神经网络有可能学习图像基元的全部知识</li>
</ul>
</li>
</ul>
</li>
<li><img src="/2020/12/01/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-0x06/9.png" srcset="/img/loading.gif" lazyload alt>
<ul>
<li>在给定24位色深（a）的低光照sRGB图像的情况下，典型的增强方法（这里使用的Hist就是直方图均化）无法产生令人愉悦的图像，并且细节得以恢复并且噪声得到抑制（b，e，f）。 为了说明我们的想法，我们应用高斯滤波器将（b）分解为低频层（c）和高频层（d），并观察到低频层保留了足够的信息以恢复对象和颜色，可以用于增强高频细节。 这激发我们学习弱光图像的分解和增强方法（h）</li>
</ul>
</li>
<li>给定一个图像基元的低频信息，通过推测相应的高频信息，网络可以重构出整个基元。有了这样的先验，可以从恢复的低频层中学习增强高频细节</li>
<li>为此作者提出一种新的神经网络，利用 关注上下文编码(Attention to Context Encoding， ACE)模块
<ul>
<li>第一阶段自适应地选择低频信息用于恢复低频层和去除噪声</li>
<li>第二阶段自适应地选择高频信息用于细节增强</li>
<li>还提出了一个跨域变换(Cross Domain Transformation， CDT)模块，以利用多尺度基于频率的特征，在两个阶段进行噪声抑制和细节增强</li>
</ul>
</li>
<li>作者归纳的三个贡献点
<ul>
<li>提出了一种新的基于频率的分解和增强模型来增强弱光图像
<ul>
<li>在抑制噪声的同时恢复低频层的图像内容，然后恢复高频图像细节</li>
</ul>
</li>
<li>提出了一种网络，其中包含一个关注上下文编码(ACE)模块，用于分解输入图像以自适应增强高频/低频层，以及一个跨域转换(CDT)模块，用于噪声抑制和细节增强</li>
<li>准备了一个具有真实噪声的低光图像数据集和相应的ground  truth图像，以方便学习过程</li>
</ul>
</li>
<li>低光增强的传统方法
<ul>
<li>上面那篇论文介绍了一些，就稍微在记录一下</li>
<li>直方图均化、伽马校正</li>
<li>基于深度学习去学习一个映射函数；基于retinx的图像增强，将输入图像分解为光照和反射率，然后增强图像</li>
<li>不过作者认为这些方法都是针对低噪声的低光图像的，不能增强噪声低光图像</li>
</ul>
</li>
<li>然而，简单地通过使用现有的去噪方法进行 前/后处理 来去除低光图像中的噪声是有一定复杂度的
<ul>
<li>低像素值使得难以在增强弱光图像之前提供用于检测/去除噪声的足够的上下文信息</li>
<li>在应用现有的增强方法之后，噪声会被不可预测地放大，从而产生仍然具有低SNR的图像，因此难以进一步去噪</li>
<li>为了解决这个限制，作者在本文中提出学习一种深度增强模型，以端到端的递归方式增强弱光图像同时去除噪声</li>
</ul>
</li>
</ul>
<h3 id="Net-Model">Net Model</h3>
<p><img src="/2020/12/01/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-0x06/10.png" srcset="/img/loading.gif" lazyload alt></p>
<ul>
<li>作者提出的方法受到两个观察结果的启发
<ul>
<li>与直接增强整个图像相比，增强噪声低光图像的低频层更容易
<ul>
<li>低频层的噪声更容易检测和抑制。通过分析图像低频层的全局属性，可以正确地估计图像的光照/颜色</li>
</ul>
</li>
<li>自然图像中最原始的部分，例如边缘和角落，具有非常低的固有维数
<ul>
<li>如此低的维数意味着少量的图像示例就足以很好地表示图像原语。因此，给定基元的低频信息，可以推断出相应的高频信息</li>
</ul>
</li>
</ul>
</li>
<li>作者提出的模型有两个主要阶段
<ul>
<li>低频图像的增强：提出学习低频<strong>图像增强函数</strong>C(·)，然后学习用于<strong>颜色恢复放大函数</strong>A(·)
<ul>
<li>通过联合建模从C(·)到A(·)的映射，网络不必同时学习全局信息(如光照)和局部信息(如颜色)，增强效果更好</li>
<li>给定一个弱光sRGB图像I，第一阶段的增强可以写成</li>
<li>$I^{a}=\alpha A(C(I)) \cdot C(I)$
<ul>
<li>其中$I^{a}$ 为放大的低频层</li>
<li>这里的A与基于retinex方法中的光照图（illumination map）不同， 其中α是一个全局增强系数，通过C获得的</li>
<li>即αA(·)可以被解释为自注意方式增强C的误差图</li>
</ul>
</li>
</ul>
</li>
<li>高频图像的增强：于$I^{a}$ 学习高频细节增强函数D(·)，而不是直接从有噪声的原始输入图像 I 恢复高频细节。然后对D(·)进行残差建模
<ul>
<li>$I^{c}= I^{a} + D(I^{a})$</li>
</ul>
</li>
<li>各阶段的可视化图
<ul>
<li><img src="/2020/12/01/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-0x06/11.png" srcset="/img/loading.gif" lazyload alt></li>
</ul>
</li>
</ul>
</li>
</ul>
<h4 id="ACE-Module">ACE Module</h4>
<p><img src="/2020/12/01/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-0x06/12.png" srcset="/img/loading.gif" lazyload alt></p>
<ul>
<li>Attention to Context Encoding注意上下文编码ACE</li>
<li>目标是学习用于图像分解的频率感知特征，为此作者扩展了最初用于编码长程关系的非局部运算（non-local）来选择频率自适应上下文信息</li>
<li>给定一个输入$x_{i n} \in R^{H \times W \times C}$ ，首先使用两组扩张卷积（卷积核大小/扩张率为1/1和3/2），分别记为 $f_{d1}$ 和 $f_{d2}$ ，用于提取不同接受域的特征，然后计算这两个特征之间的对比感知注意映射 $C_a$
<ul>
<li>$C_a = sigmoid (f_{d1}(x_{in}) - f_{d2}(x_{in}))$</li>
<li>$C_a$ 表示像素级的相对对比度信息，其中高对比度的像素被认为属于高频层，取反的话就是得到低对比度部分，即低频信息</li>
<li>然后计算其逆映射$\overline{C_a} = 1 - C_a$ ，从$x_{in}$中选择特征来表示低频内容
<ul>
<li>$x_c = \overline{C_a} \cdot x_{in}$</li>
</ul>
</li>
<li>接着通过最大值池化来进一步缩小所选择的特征$x_{c}$从而得到 $x_{c}^{\downarrow}$ ，从而减少计算量和分部局的像素依赖</li>
<li>对于给定 $x_{c}^{\downarrow} \in R^{H’ \times W’ \times C’}$ ，non-local上下文编码为
<ul>
<li>$x_{c}^{r}=g\left(x_{c}^{\downarrow}\right)^{\top} \times h\left(x_{c}^{\downarrow}\right) \times f\left(x_{c}^{\downarrow}\right)^{\top}$
<ul>
<li>其中g，h，f为一组运算（卷积，重塑和矩阵转置）</li>
<li>首先计算像素之间的关系表 $M \in R^{H’  W’ \times H’  W’}$ ，然后考虑每个像素与其他所有像素的关系，计算非局部增强特征 $x_c^r$</li>
</ul>
</li>
</ul>
</li>
<li>最后我们便得到了频率感知的非局部增强特征$x_{out}$
<ul>
<li>$x_{out} = Unpool(x_c^R) +x_c^r$
<ul>
<li>使用残差结构</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><img src="/2020/12/01/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-0x06/15.png" srcset="/img/loading.gif" lazyload alt></li>
<li>在整体网络框架中的两个ACE共享权重，不过第二个ACE模块中使用对比度感知映射算法（不是反向感知注意映射算法，即使用高频信息）从代表高频层的特征中学习图像细节</li>
<li><img src="/2020/12/01/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-0x06/13.png" srcset="/img/loading.gif" lazyload alt></li>
</ul>
<h4 id="CDT-module">CDT module</h4>
<ul>
<li><img src="/2020/12/01/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-0x06/14.png" srcset="/img/loading.gif" lazyload alt></li>
<li>了解低光图像的全局属性有助于恢复光照和图像内容，为此设计了CDT模块，在缩小弱光域和增强域中的特征之间的差距的同时，增加感受野
<ul>
<li>在噪声弱光域提取的频率感知特征与在增强域提取的频率感知特征之间的差异</li>
</ul>
</li>
<li>在第一阶段，首先通过自推导的反对比度感知映射$\overline{C_a}$ 对编码器 $x_{en}$ 的噪声特征进行空间重加权，以滤除高对比度信息，然后 $x_{en}$ 与相应解码器的特征 $x_{de}$ 进行拼接</li>
<li>然后从拼接后的$[ x_{en}, x_{de} ]$ 中计算全局缩放向量 v ，以通道方式自适应地缩放来自不同域的特征</li>
<li>使用v与拼接后的结果进行相乘得到最后的 $x_{out}$</li>
<li>在第二阶段，使用对比度感知的注意映射算法（高频）来学习图像细节，而不是使用反映射算法（低频），类似于ACE模块</li>
</ul>
<h4 id="Proposed-Dataset">Proposed Dataset</h4>
<ul>
<li>为了便于学习所提出的模型，作者准备了一个新的包含噪声信息的低光数据集：低光图像和对应的ground  truth sRGB图像对</li>
<li>基于SID数据集准备训练数据，该数据集由原始数据和ground truth图像对组成，其中ground truth的拍摄时间较长，所以噪声可忽略不计
<ul>
<li>不过线性相机原始数据与非线性sRGB数据有显著的不同，特别是在噪声和图像强度方面</li>
<li>为此作者考虑了图像生成管道中的几个关键步骤(即曝光补偿Exposure compensation、白平衡White balance和去线性化De-linearization)，并对它们的操作进行了操作，以模拟来自不同相机的真实噪声低光sRGB图像</li>
</ul>
</li>
</ul>
<h4 id="Training">Training</h4>
<ul>
<li>在两阶段的训练过程中，使用L2损失来测量重建的准确性
<ul>
<li>在第一阶段,鼓励网络关注预测输入图像的低频分量，准备相应的ground truth，用$I_{f}^{gt}$ 表示，通过使用引导滤波器过滤高频细节,保持ground truth图像的主要结构和内容</li>
<li>$L_{a c c}=\lambda_{1}\left|C-I_{f}^{g t}\right|<em>{2}+\lambda</em>{2}\left|I^{c}-I^{g t}\right|_{2}$
<ul>
<li>$C,I^c,I_f^{gt},I^{gt}$ 分别为重构图像内容、恢复后的图像、低频层的ground truth、恢复图像的ground truth</li>
<li>λ1和λ2是平衡参数</li>
<li>网络结构图中的两个红色箭头（supervision，监督）</li>
<li>让低频图像和恢复图都接近ground truth</li>
</ul>
</li>
</ul>
</li>
<li>通过使用L1损失比较$I^c and I^{gt}$的VGG特征距离来合并感知损失
<ul>
<li>$L_{vgg}=\lambda_{3}\left| \Phi (I^c) - \Phi (I^{g t}) \right|_{2}$
<ul>
<li>$\Phi$ 为vgg网络</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="😝😜😋-2">😝😜😋</h3>
<p>emm没啥写的，不写了</p>
<ul>
<li>作者观察到，在不同的频率层中，噪声对图像的影响是不同的。基于此提出了一种新的基于频率的图像分解增强模型，在抑制噪声的同时，自适应增强不同频率层的图像内容和细节</li>
<li>提出了一个网络，该网络包含了关注上下文编码(ACE)模块，用于自适应增强高频和低频层，以及跨域转换(CDT)模块，用于噪声抑制和细节增强</li>
<li>准备了一个新的弱光图像数据集</li>
</ul>

                
              </div>
            
            <hr/>
            <div>
              <div class="post-metas my-3">
  
    <div class="post-meta mr-3 d-flex align-items-center">
      <i class="iconfont icon-category"></i>
      

<span class="category-chains">
  
  
    
      <span class="category-chain">
        
  <a href="/categories/study/" class="category-chain-item">study</a>
  
  

      </span>
    
  
</span>

    </div>
  
  
    <div class="post-meta">
      <i class="iconfont icon-tags"></i>
      
        <a href="/tags/%E5%9B%BE%E5%83%8F%E5%A2%9E%E5%BC%BA%E4%B8%8E%E5%9B%BE%E5%83%8F%E6%81%A2%E5%A4%8D/" class="print-no-link">#图像增强与图像恢复</a>
      
    </div>
  
</div>


              
  

  <div class="license-box my-3">
    <div class="license-title">
      <div>论文阅读-0x06</div>
      <div>http://example.com/2020/12/01/论文阅读-0x06/</div>
    </div>
    <div class="license-meta">
      
        <div class="license-meta-item">
          <div>作者</div>
          <div>hyzs1220</div>
        </div>
      
      
        <div class="license-meta-item license-meta-date">
          <div>发布于</div>
          <div>2020年12月1日</div>
        </div>
      
      
      
        <div class="license-meta-item">
          <div>许可协议</div>
          <div>
            
              
              
                <a class="print-no-link" target="_blank" href="https://creativecommons.org/licenses/by/4.0/">
                  <span class="hint--top hint--rounded" aria-label="BY - 署名">
                    <i class="iconfont icon-by"></i>
                  </span>
                </a>
              
            
          </div>
        </div>
      
    </div>
    <div class="license-icon iconfont"></div>
  </div>



              
                <div class="post-prevnext my-3">
                  <article class="post-prev col-6">
                    
                    
                      <a href="/2020/12/03/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-0x07/" title="论文阅读-0x07">
                        <i class="iconfont icon-arrowleft"></i>
                        <span class="hidden-mobile">论文阅读-0x07</span>
                        <span class="visible-mobile">上一篇</span>
                      </a>
                    
                  </article>
                  <article class="post-next col-6">
                    
                    
                      <a href="/2020/11/21/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-0x05/" title="论文阅读-0x05">
                        <span class="hidden-mobile">论文阅读-0x05</span>
                        <span class="visible-mobile">下一篇</span>
                        <i class="iconfont icon-arrowright"></i>
                      </a>
                    
                  </article>
                </div>
              
            </div>

            
          </article>
        </div>
      </div>
    </div>

    <div class="side-col d-none d-lg-block col-lg-2">
      
  <aside class="sidebar" style="margin-left: -1rem">
    <div id="toc">
  <p class="toc-header">
    <i class="iconfont icon-list"></i>
    <span>目录</span>
  </p>
  <div class="toc-body" id="toc-body"></div>
</div>



  </aside>


    </div>
  </div>
</div>





  



  



  



  



  


  
  









    

    
      <a id="scroll-top-button" aria-label="TOP" href="#" role="button">
        <i class="iconfont icon-arrowup" aria-hidden="true"></i>
      </a>
    

    
      <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">搜索</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v" for="local-search-input">关键词</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>

    

    
  </main>

  <footer>
    <div class="footer-inner">
  
    <div class="footer-content">
       <a href="https://hexo.io" target="_blank" rel="nofollow noopener"><span>Hexo</span></a> <i class="iconfont icon-love"></i> <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener"><span>Fluid</span></a> 
    </div>
  
  
  
  
</div>

  </footer>

  <!-- Scripts -->
  
  <script  src="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.js" ></script>
  <link  rel="stylesheet" href="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.css" />

  <script>
    NProgress.configure({"showSpinner":false,"trickleSpeed":100})
    NProgress.start()
    window.addEventListener('load', function() {
      NProgress.done();
    })
  </script>


<script  src="https://lib.baomitu.com/jquery/3.6.4/jquery.min.js" ></script>
<script  src="https://lib.baomitu.com/twitter-bootstrap/4.6.1/js/bootstrap.min.js" ></script>
<script  src="/js/events.js" ></script>
<script  src="/js/plugins.js" ></script>


  <script  src="https://lib.baomitu.com/typed.js/2.0.12/typed.min.js" ></script>
  <script>
    (function (window, document) {
      var typing = Fluid.plugins.typing;
      var subtitle = document.getElementById('subtitle');
      if (!subtitle || !typing) {
        return;
      }
      var text = subtitle.getAttribute('data-typed-text');
      
        typing(text);
      
    })(window, document);
  </script>




  
    <script  src="/js/img-lazyload.js" ></script>
  




  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/tocbot/4.20.1/tocbot.min.js', function() {
    var toc = jQuery('#toc');
    if (toc.length === 0 || !window.tocbot) { return; }
    var boardCtn = jQuery('#board-ctn');
    var boardTop = boardCtn.offset().top;

    window.tocbot.init(Object.assign({
      tocSelector     : '#toc-body',
      contentSelector : '.markdown-body',
      linkClass       : 'tocbot-link',
      activeLinkClass : 'tocbot-active-link',
      listClass       : 'tocbot-list',
      isCollapsedClass: 'tocbot-is-collapsed',
      collapsibleClass: 'tocbot-is-collapsible',
      scrollSmooth    : true,
      includeTitleTags: true,
      headingsOffset  : -boardTop,
    }, CONFIG.toc));
    if (toc.find('.toc-list-item').length > 0) {
      toc.css('visibility', 'visible');
    }

    Fluid.events.registerRefreshCallback(function() {
      if ('tocbot' in window) {
        tocbot.refresh();
        var toc = jQuery('#toc');
        if (toc.length === 0 || !tocbot) {
          return;
        }
        if (toc.find('.toc-list-item').length > 0) {
          toc.css('visibility', 'visible');
        }
      }
    });
  });
</script>


  <script src=https://lib.baomitu.com/clipboard.js/2.0.11/clipboard.min.js></script>

  <script>Fluid.plugins.codeWidget();</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/anchor-js/4.3.1/anchor.min.js', function() {
    window.anchors.options = {
      placement: CONFIG.anchorjs.placement,
      visible  : CONFIG.anchorjs.visible
    };
    if (CONFIG.anchorjs.icon) {
      window.anchors.options.icon = CONFIG.anchorjs.icon;
    }
    var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
    var res = [];
    for (var item of el) {
      res.push('.markdown-body > ' + item.trim());
    }
    if (CONFIG.anchorjs.placement === 'left') {
      window.anchors.options.class = 'anchorjs-link-left';
    }
    window.anchors.add(res.join(', '));

    Fluid.events.registerRefreshCallback(function() {
      if ('anchors' in window) {
        anchors.removeAll();
        var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
        var res = [];
        for (var item of el) {
          res.push('.markdown-body > ' + item.trim());
        }
        if (CONFIG.anchorjs.placement === 'left') {
          anchors.options.class = 'anchorjs-link-left';
        }
        anchors.add(res.join(', '));
      }
    });
  });
</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.js', function() {
    Fluid.plugins.fancyBox();
  });
</script>


  <script>Fluid.plugins.imageCaption();</script>

  
      <script>
        if (!window.MathJax) {
          window.MathJax = {
            tex    : {
              inlineMath: { '[+]': [['$', '$']] }
            },
            loader : {
              load: ['ui/lazy']
            },
            options: {
              renderActions: {
                insertedScript: [200, () => {
                  document.querySelectorAll('mjx-container').forEach(node => {
                    let target = node.parentNode;
                    if (target.nodeName.toLowerCase() === 'li') {
                      target.parentNode.classList.add('has-jax');
                    }
                  });
                }, '', false]
              }
            }
          };
        } else {
          MathJax.startup.document.state(0);
          MathJax.texReset();
          MathJax.typeset();
          MathJax.typesetPromise();
        }

        Fluid.events.registerRefreshCallback(function() {
          if ('MathJax' in window && MathJax.startup.document && typeof MathJax.startup.document.state === 'function') {
            MathJax.startup.document.state(0);
            MathJax.texReset();
            MathJax.typeset();
            MathJax.typesetPromise();
          }
        });
      </script>
    

  <script  src="https://lib.baomitu.com/mathjax/3.2.2/es5/tex-mml-chtml.js" ></script>

  <script  src="/js/local-search.js" ></script>





<!-- 主题的启动项，将它保持在最底部 -->
<!-- the boot of the theme, keep it at the bottom -->
<script  src="/js/boot.js" ></script>


  

  <noscript>
    <div class="noscript-warning">博客在允许 JavaScript 运行的环境下浏览效果更佳</div>
  </noscript>
</body>
</html>

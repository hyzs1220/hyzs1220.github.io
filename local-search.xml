<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>生活小记-对生活的迷茫</title>
    <link href="/2024/03/22/%E7%94%9F%E6%B4%BB%E5%B0%8F%E8%AE%B0-%E5%AF%B9%E7%94%9F%E6%B4%BB%E7%9A%84%E8%BF%B7%E8%8C%AB/"/>
    <url>/2024/03/22/%E7%94%9F%E6%B4%BB%E5%B0%8F%E8%AE%B0-%E5%AF%B9%E7%94%9F%E6%B4%BB%E7%9A%84%E8%BF%B7%E8%8C%AB/</url>
    
    <content type="html"><![CDATA[<p>刚刚仅仅是度过了定岗实习的第一个周，但仿佛是过了一个月那么久，自己整个人的身体状态和精神状态都格外的疲惫，在这一周里面，也不知道自己抑郁了多少次，自我安慰了多少次，和别人吐槽了多少次，但每一次情绪的宣泄，都没有给自己带来状态上的改变。</p><p>思来想去，与其给别人带去负能量，还不如这样写一篇博客来自己吐槽发泄一下，把心里所想的，心里黑暗的那一面都宣泄出来，可能反而会有作用。</p><span id="more"></span><h3 id="落差感">落差感</h3><p>围绕着定岗这件事情，陆陆续续也发生了很多故事，也是第一次这么贴切地感受到了这个社会的一些本来面目。从入职开始，身边就已经有同事开始为定岗开始铺路，可能是通过不断表现自己，可能是通过拉近和人资的关系，当然，我觉得更多的是靠着家里的关系和人脉。相较之下，我也有了自知之明，觉得对于这些自己无力改变的事情，那就听从命运的安排吧。从填志愿开始，我就不愿意和大家争抢，也是因为明白没有什么争抢的资本，所以尽量避开大家都想去的部门。当然在日常轮岗工作中，也不愿意去表现出争名夺利的样子，所以也就更像是个不争不抢的“老实人”，和所有人都尽量保持一个良好的关系，真的可以说是一路如履薄冰，希望可以顺利定岗，进入到一个顺心的工作环境中，然后努力实现自己以后的很多梦想和追求吧。</p><p>直到上周五，告知了定岗结果，仔细一琢磨，就会发现，周围所有有关系，家里有钱有人脉的同事，都留在了总公司。当然也有一两个“普通人”也留在了总公司，但大部分都到了分公司。其实自己也能预料到这一步，也算是给自己打过了预防针，有过这样一个预期的结果，但真正要面对他的时候，才发觉，这个结果背后带来的很多自我怀疑以及对公司和大环境的失望，让我痛苦了许久。</p><p>该怎么说呢，可能也是因为在学校呆久了，让我对很多事情的看法变得十分单纯和简单，但是在社会这样一个大熔炉里面，尤其是像这样一个国企里面，其中杂糅的污秽和黑暗早已根深蒂固。当我带着这样一种思维去期待和面对这件事情的时候，这种落差感，让我对一直以来的很多信念和想法产生了极大的怀疑。对于一些事情和真相的了解和接受，有着相差甚远的距离，从这一段走到那一段，可能真的只有靠时间和鞭挞才会到达吧。</p><p>而也就在这短短的一周之中，我竟逐渐感受到了定岗在总公司的同事对我们分公司的一种鄙视，那种自上而下的瞧不起。我不理解这种优越感的由来，我更不理解一周的时间为什么会让人产生这种改变，难道“屁股决定脑袋”这句话，真的适用于任何地方，任何人嘛？为此，我特别失落，一周前还在一块轮岗的同事，如今就已经站在了不同的高台上，要说为什么失落，可能就是对这短时间内发生的这些事情的无奈和悲伤，以及对自己的失望和怀疑吧。</p><p>对于分公司，说不上有多么反感，也说不上有啥讨厌的地方，但总感觉对于一些比较虚幻的概念，比如氛围、人际关系和环境压力，有着那么一丝不舒服和不适应。尤其是当想到定岗以后，可能会在这里一直工作下去很久很久，这一丝不好的感觉也就被无限放大了。当然，在分公司也有着分公司的优势和好处，这些也是在我轮岗期间更愿意来到分公司的原因。但对于目前而言，此刻的痛苦更是让刚定岗的我十分难受——通勤的痛。由于最近要去孙村那边参观学习（至今不理解为啥要有这种安排），但宿舍还是停留在市中区这边，也就导致了漫长的通勤时间，为此便需要早起，会牺牲掉晚上许多的个人时间。当这一周过完，此刻的我想不到这周自己除了面对工作和通勤之外，自己还有什么其他的娱乐和生活。</p><p>可以说，以上这些都给我带了很深的身体煎熬和精神折磨，而上述这些，也只是此时此刻回忆起来比较深切的几个地方，除此之外的点滴琐事也就不想再一一赘述了。</p><h3 id="孤独感">孤独感</h3><p>其实感觉用孤单感来写下面的这些心态变化和感受并不是十分合适，但又想不到什么更好的词语来形容，所以就姑且当作是一种内心的孤单吧。</p><p>无论什么时候，只要自己突然很抑郁或者很痛苦，无一例外，都会牵连着感情的问题。每次的热情主动和感情投入，到最后一看，全都是小丑行为，但每次又都会深陷其中。每次我都以为这次真的遇到了爱情，每次我都以为对方也拥有着同样的想法，每次我都以为彼此的行为表明了很多彼此的想法，于是我开始耗费心力，开始去转变自己的行为，在这个过程中，自己似乎心情也都变得很开心，也对未来有了很多期望和向往。我真的不直到该怎么去叙述自己的这种想法和行为，每段我自认为的感情都会耗光我积攒了许久的快乐和力量，直到慢慢被告知不可能的结局，而我也死在了这段经历里面。之后的自己，浑身散发着消极和负面的情绪，感觉世界都变成了灰色，不知道该怎么宣泄和倾述，开始不断内耗。</p><p>那天杰锅发给我了一段<a href="https://www.zhihu.com/question/346881155/answer/3341649859">回答</a>，但真的看的我很破防，我特别想不明白，为什么我每次自以为的感情，都会是这样的结局，仿佛都是命中注定了的剧本。</p><p>（写点内心的幽暗和愤懑吧）我突然开始理解为什么大家都会说上学是最美好的回忆。开始工作，步入社会之后慢慢发觉，人和人之间的差距，比人和狗之间的差距都大。身边同事同学和一些朋友，大家似乎都是同一起跑线——刚毕业工作，但实际上，大家都已经开始买房买车，已经开始准备结婚生子，已经开始四处旅游享受生活，已经开始为下一代铺路。嫉妒，这两个字，看着就不好看，听起来更是扎耳朵，但此刻我好像真的有些嫉妒了。我并不怪我爸妈，只是自己觉得自己很无能，曾经有过太多美好的幻想，直到此刻才意识到自己的幼稚和愚昧。说着说着，又想到了定岗的事情，有关系的同事们，并没有很好的学历，并没有很出众的能力，但是他们却都留在了总公司，真的很无奈，深深的无力感。</p><h3 id="迷茫感">迷茫感</h3><p>其实就是对于未来的迷茫和无力。</p><p>感情上的受挫，真的让我对未来找对象结婚这些事情充满了困顿和怀疑。所以也就再也不想奢求啥了，只能是希望以后可以顺顺利利走完这一生吧。当然了，可能对我最大的困难和迷茫都是来自于贫穷吧，对于未来买房买车，该如何攒够这样一大笔资金。</p><p>算了，不太像写这一部分了，其实平常每天自己独处的时候，外出溜达的时候，想的都会是这样一些问题，每次给自己的答案也只是走一步看一步，慢慢积攒，所以也就没有太多想说的了。</p><p>今天的抱怨也就到此为止吧，无论怎样，还是希望自己可以永远乐观的面对生活吧，逃避解决不了任何事情，加油！</p>]]></content>
    
    
    <categories>
      
      <category>life</category>
      
    </categories>
    
    
    <tags>
      
      <tag>life</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>2024新年期待</title>
    <link href="/2023/12/18/2024%E6%96%B0%E5%B9%B4%E6%9C%9F%E5%BE%85/"/>
    <url>/2023/12/18/2024%E6%96%B0%E5%B9%B4%E6%9C%9F%E5%BE%85/</url>
    
    <content type="html"><![CDATA[<p>2023年可以算是人生中很重要的转折一年，我走出了生活学习了七年的校园，步入了完全陌生又烦杂的社会。其中的生活改变和心态影响，让我总有这说不尽的感慨和无奈。</p><p>可能对于一个初入工作的小白，就应该积极向上，在年轻的岁月里面拼搏一把，努力表现自己，积极主动。可是现在的我反而有点抗拒这种心态。该怎么说呢，刚进公司之后，自己就一直保持一个少说多看多学的心态，和大家接触也都比较保持一个谦逊的状态，也向师哥师姐请教学习了许多。慢慢地，我意识到，在电网这个体系里面，想要往上走真的挺难的，没有关系，没有资历，我觉得当个普通的打工仔也挺好的，至少你可以保证你基础的生活。</p><p>但也并不是完全失去了向上拼搏的心，不过对于很多事情，自己的心态摆的更正了一些。而对于即将到来的2024年，其实也还是有很多期待的，无论是工作、生活还是感情，都希望自己可以更进一步，去找到自己的方向和节奏~</p><span id="more"></span><p>工作后的状态终究是不同于上学那时候，所以可能很多思想也都有了些许转变，但终归是希望自己能够努力生活好每一天~</p><ul><li>攒钱。虽然23年8月份就入职工作了，当时工作到现在反而几乎没有攒下来钱。除了日常各种开支，花费的大头主要集中在手机、随礼、送礼、请客、衣服这些方面吧，另外还完了自己的所有欠款（1.6w）。其实这样想来，似乎自己仿佛“攒”了不少钱。不过对于马上到来的24年，自己更希望能攒下来一笔钱，手上能够有一笔随时调用的钱吧。如果说，仅仅是如果，明年会谈恋爱的话，那就尽量攒攒吧。。</li><li>读书。其实去年还真的读了几本书，在总公司轮岗没事的时候，就找了几本书看，不过看着看着后面就有点看不下去了，感觉还是缺少那份恒心吧，心态格外的浮躁。不过每年都还是想着要多读书，尤其是自己现在处在这么迷茫的状态。</li><li>练字。前段时间突然翻出来了之前买的练字帖，再加上自己的字属实是难以启齿，所以还是要抽空多练练字。</li><li>爱情。随缘吧，也不太想有太多期待，而且自己现在越来越感觉自己对于这些事情有种难言的抗拒。</li><li>工作。明年就会定岗了，到时候就知道自己到底去到哪个部门，干些什么工作。现在因为分公司的存在，对于轮岗，大家反而敏感了许多，主要也是因为大家都不想去分公司吧。虽然对我而言，总公司和分公司没有太多区别，不过如果可以的话，我挺想去到能源的，毕竟能感受到那边整体的工作氛围还是很偏向技术的，而且经理在跟我聊天的过程中，也对我的发展提供了许多建议和指导，所以也是怀着感激和向往之心希望能够在这边找到自己未来发展的道路。对于分公司其实也没有太多抗拒，毕竟让我写代码我也是可以接受的，但最近轮岗，我慢慢发现，分公司这边的氛围并不是很好，而且感觉有些项目比我想象中要差的多。其实也是考虑了许多，但最后也很难给自己一个十分准确的答案，不过这基本就是我目前的志愿情况了。但最后定岗具体啥情况，自己也说了不算，而且师哥师姐也都说，这个很看领导的意思，个人志愿并不一定有用。。所以说，还是摆平心态，听从安排吧，反正在哪打工不是打工。定岗转正之后，也希望自己可以慢慢适应工作节奏，然后兼顾好自己的生活和工作吧，加油。</li><li>消费。其实感觉和攒钱那部分应该合在一起，不过还是想着单独想一想，写一下。因为要攒钱，而且今年已经有了很大的花销在衣食住行上，所以在明年一定要控制好自己的消费，没必要的东西真的就没必要买了，很多需要的东西今年已经买过了。</li><li>旅行。明年有机会多出去转转玩玩吧，有几个地方还是挺想去的：泰山、北京、杭州、内蒙古。</li></ul><p>“比起有人左右情绪的日子，我更喜欢无人问津的时光。独处让自己的本心更自由，更潇洒。不用周旋别人的情绪，也不用刻意判断他人的心思！人终其一生的追求大概就是自由吧！”</p>]]></content>
    
    
    <categories>
      
      <category>life</category>
      
    </categories>
    
    
    <tags>
      
      <tag>life</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>生活小记-惆怅与迷茫</title>
    <link href="/2023/10/12/%E7%94%9F%E6%B4%BB%E5%B0%8F%E8%AE%B0-%E6%83%86%E6%80%85%E4%B8%8E%E8%BF%B7%E8%8C%AB/"/>
    <url>/2023/10/12/%E7%94%9F%E6%B4%BB%E5%B0%8F%E8%AE%B0-%E6%83%86%E6%80%85%E4%B8%8E%E8%BF%B7%E8%8C%AB/</url>
    
    <content type="html"><![CDATA[<p>自来到济南开始上班工作后，到现在也是两月有余了，期间发生了许多事情，说不上给自己带来了多么大的改变，但确实让我有了许许多多的感慨和困惑。在很多时候就会不自觉地陷入到这些漩涡之中，感到无尽的孤单和落寞。</p><span id="more"></span><h2 id="惆怅">惆怅</h2><p>大学毕业那时候，自己特别兴奋，和朋友们天天把酒言欢，为了彼此美好的前程而欢庆，更是为了即将到来的离别而尽兴。工作之后我也认识了新的小伙伴，毕竟大家都是初入职场，能感受到彼此的青涩和友好（当然也有个别例外）。但是慢慢地我发现这和大学终究是不一样的。在大学，我可以根据自己的性格喜好而去选择认识朋友，找到志同道合、合得来的好朋友，从而无忧无虑地玩耍，可以没心没肺的哈哈大笑，可以像个孩子那样在兄弟们面前发疯，却不会有太多顾虑。但在工作之后，我意识到认识的这些朋友似乎只是简单的朋友，我们可能会一起吃饭，会一起上下班，但却能感受到彼此之间有着一层隔膜。其实自己也说不清这层隔膜从何而来，但在生活中，会发现彼此的交集极少，之间的友谊也很脆弱，更别提在彼此面前无所顾虑的玩耍发疯。你要时刻保持一份清醒，不能乱说话，不能说错话，要经常提醒自己，我们之间只是同事关系，在没有完全熟络认识一个人之前，永远不知道对方背后是怎样一个人。当然啦，我也明白，其实大家绝大部分都是很好的，彼此之间的关系也没有那么复杂，但因为工作，就难免会涉及到很多竞争和利益关系，所以就总是给自己悬着这么一根线。至于在工作中，那更是复杂烦乱的多了，每个人可能都有着自己的雄心抱负，都想去争一些东西，由此而导致的竞争关系也使得彼此之间关系变得很微妙，但在表面上大家又似乎都是笑呵呵的。至于我自己呢，上班之前就给自己找好了定位，所以上班之后也是一直比较沉默，很多事情知道不是自己努力或者争取就能得到的，所以也就放弃了很多想法，因此生活也平静了许多，没有牵扯到太多错综复杂的关系中去。不过不知道是不是因为自己这样的表现，隐约中能感觉到一些人对我的不屑和鄙视（尤其是女生），搞得我也很无奈。</p><p>所以说到现在的惆怅，更多地就是身边朋友和环境的改变吧。目前处于轮岗阶段，并没有太繁重的工作任务，所以很清闲，下班后也有很多充裕的时间，所以可能就会有些胡思乱想吧。加上身边没有可以诉说的人和发泄的途径，也就让自己更加抑郁了些。</p><p>如果说上面很多因素或者事情是导致特别惆怅的缘由，那一些感情问题的出现，就是点燃这份惆怅的一把烈火了。每次都不知从哪说起好，感觉由头太多，槽点也太多，还是说说自己的心态吧。我慢慢意识到，自己接触过的女生都很厉害，家庭条件等方面也都格外优越，在学校时候可能还没觉得，毕竟自己当时还当着一些职务啥的，所以自己自信心也膨胀的厉害。但是现在工作之后，也慢慢认清了自己，每当再有机会接触到昔日的这些朋友，也就愈发地自卑了。而且我也意识到大家似乎正如我想的那样，并不愿意搭理我这样一个“黑矬穷”，不愿意和我有太多接触。可能感情是我永远难以跨越的一个障碍吧，我实在想不明白为什么我会一步步走到如此这般地步，每每想起，都止不住的悲伤和委屈。</p><p>所以在很多个夜晚和闲暇之余，都会问自己很多问题，反思日常生活中发生的许多事情，想一想接下来要走的路，到头来只有摆脱不去的惆怅。既然我能意识到我的惆怅和许多缘由，所以日常也时常安慰自己，毕竟工作生活和自己都还是挺不错，虽然并不起什么作用。翻来覆去，还是想通过文字记录下自己当前的部分所思所想，可能未来的有一天，我终会明白自己此刻的困惑的解决方式。</p><h2 id="迷茫">迷茫</h2><p>对未来的迷茫。不过我也意识到，钱似乎能解决我目前的所有的困惑，，，可惜我没有。</p><p>在济南买房、买车，可以说是无法避免的两个问题了，而等我攒够房子首付，似乎也要三四年之后了。但这段时间，又会是家里催促我找对象结婚的一段时间，当两者挤压在一起，想想我就已经感觉到头疼，脑袋大了。如果说，假设我会遇到一个体谅我、愿意和我一块努力成长的对象，那肯定会特别美好，但感觉可能性好低，我对我找对象这件事已经彻底是去希望了，能找到就已经是谢天谢地了。确实不太想在这方面幻想太多。。</p><p>写着写着，感觉似乎自己更多地是自己的惆怅和困惑，而不是迷茫，毕竟未来的路是确定的，只是现在的自己找不好自己的定位，稳不住自己的那份心态。说到底，可能还是自己太闲了，总是胡思乱想，给自己没事找事，把别人看的太重，太在意别人对自己的看法和态度。</p><p>好疲惫，写着写着突然感觉好心累。所以就到此为之吧，总之还是要积极面对生活，努力去打拼属于自己的一切。一些不好的想法和怨气也就在这里吐槽写一写吧，希望以后自己回来翻阅的时候，能一笑带过吧：）</p>]]></content>
    
    
    <categories>
      
      <category>life</category>
      
    </categories>
    
    
    <tags>
      
      <tag>life</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>生活小记-雨夜</title>
    <link href="/2023/09/07/%E7%94%9F%E6%B4%BB%E5%B0%8F%E8%AE%B0-%E9%9B%A8%E5%A4%9C/"/>
    <url>/2023/09/07/%E7%94%9F%E6%B4%BB%E5%B0%8F%E8%AE%B0-%E9%9B%A8%E5%A4%9C/</url>
    
    <content type="html"><![CDATA[<p>现在是2023年9月7日20:40:56。</p><span id="more"></span><p>我想今天会是我不会忘记的一天，原本和同学约好要请她吃饭。但当我下班到公司楼下后，突然发现外面下起了雨，而且还不小。那时候我单纯地感觉应该不会下很大，更不会下很久。因为有饭局，我便打算直接骑个共享单车回宿舍，在路上，雨越下越大，到最后我已经是完全睁不开眼。</p><p>到宿舍后，我早已是浑身湿透的状态，而外面的雨仍在哗啦啦的下。于是我便先去洗了个澡，洗完澡后，感觉特别舒服，和同学说好看看等会儿雨的情况，再决定要不要出门。</p><p>大概就在七点那时候（因为舍友回来了？因为洗澡着凉了？因为济南空气质量不好？因为下雨天？还是别的原因？），我突然感觉嗓子不舒服，然后开始呼吸困难。那时候外面依旧还在下着大雨，于是先和同学说了声等下次再约。</p><p>在那之后，我就感觉到症状愈发严重，一直要大口呼吸，嗓子里持续发出鸣声，，然后越来越难受，感觉即将撑不住的收，我拿着伞决定下楼去买点药看看。</p><p>到了楼下的药店，医生告诉我，这类喷剂里面有激素，属于非处方药，不能给开，让我直接去诊所。没有办法，我导航去了最近的一个诊所，诊所的医生一口咬定我之前有过哮喘病历，便直接给我拿了几个治疗哮喘的药。但事实上，我之前完全没有这样过这种症状。平常的我也会坚持锻炼，还跑过半马，所以十分迷茫为什么会突发哮喘。</p><p>从诊所出来的那一刻，我突然很崩溃。</p><p>我在我们群里说我好难受，不想活了。我能感觉到我整个肺部都要炸了，嗓子更是难受的不行，我能意识到我整个身体都在努力保持呼吸。那一刻，我想到了爸妈，上周末回家的时光仍然历历在目，在父母心中，我永远是个孩子。在家的两天，我能时刻感受到爸妈对我无微不至的关心，对我说不尽的叮嘱。我还想起了我姐，我觉得我好委屈，我从来没有对爸妈那么大方过，但我的大方对她而言似乎只是打水漂，像是个笑话。我也想起了之前的朋友，我感觉很庆幸，我们没有形同陌路，偶尔还会有短暂的联系，聊聊近况。不断震动的手机也让我意识到，群里家人们对我的关心。回到宿舍吃完药之后，我拿出手机给家人们回复。那时候是我最难受，最崩溃，也最暖心的阶段。</p><p>那一刻真的想起了好多，我一路流着泪回到了宿舍，强撑着身体看了下说明书，吃了药。等自己吃完药，放下手机，等待药效起作用的时候，已经是一身的汗，我挺怕的，我怕我突然嘎了，，我还有很多想做的事情，还有很多想去的地方，有很多想见的人。</p><p>还好。</p><p>当我开始写这篇文章的时候，我已经恢复过来了，感觉好庆幸。</p><p>其实话说回来，我挺不想写这个博客的，因为我怕以后的我再看到这篇博客时，会认为自己太矫揉做作了，然后就把这篇文章删掉了。但此时此刻，真的好想找人直抒胸臆，不过也知道大家都会有自己的忙碌和生活，何必再去传递出这样一份消沉。</p><p>九点半了，时间过得好快，一边发呆，一边写着这篇博客。那就这样吧，晚安。</p>]]></content>
    
    
    <categories>
      
      <category>life</category>
      
    </categories>
    
    
    <tags>
      
      <tag>life</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>游记-青岛大珠山</title>
    <link href="/2023/04/16/%E6%B8%B8%E8%AE%B0-%E9%9D%92%E5%B2%9B%E5%A4%A7%E7%8F%A0%E5%B1%B1/"/>
    <url>/2023/04/16/%E6%B8%B8%E8%AE%B0-%E9%9D%92%E5%B2%9B%E5%A4%A7%E7%8F%A0%E5%B1%B1/</url>
    
    <content type="html"><![CDATA[<p>搬校区来西海岸之后，总想着出去玩一玩，游山玩水。这个想法在疫情结束之后也愈发强烈。不过也是对西海岸了解不多，所以一直不知道该去哪里玩耍，后来常听大家再提大珠山，所以决定要去大珠山看一看风景。</p><p>说来也神奇，这次爬大珠山也是自己离校前的一天，哈哈哈哈。</p><p><img src="/2023/04/16/%E6%B8%B8%E8%AE%B0-%E9%9D%92%E5%B2%9B%E5%A4%A7%E7%8F%A0%E5%B1%B1/IMG_20230329_100130.jpg"></p><span id="more"></span><p>从西海岸校区去到大珠山并不远，有公交只打，也可以直接打车过去，并不算贵。</p><p>这次去大珠山也是赶着免费门票的的尾巴了（三月底），所以哪怕那天并不是周末，但依旧不少人，以叔叔阿姨居多。可能真的是疫情期间，大家都憋坏了，现在到处都是旅游的，哪怕大珠山也是很多人，但相较之下，大珠山这个地方并没有太多风景，门口的检票处都还没建好，特别简陋。可能也是没有想到会有这么多人，成为热门景点？</p><p><img src="/2023/04/16/%E6%B8%B8%E8%AE%B0-%E9%9D%92%E5%B2%9B%E5%A4%A7%E7%8F%A0%E5%B1%B1/1.jpg"></p><p>进到大珠山风景区（9:10到达），会看到一片大水湖，去的那天有点雾，不然肯定会特别好看。沿着湖走，就到了大珠山脚下。正值春天开花的时候，大珠山上的迎春花和杜鹃花开的到处都是，真的很好看，不过感觉有点稀疏，还是荒凉占大部分，，，感觉自己太刁钻了，到处找茬，哈哈哈哈，不过确实很好看的。</p><p><img src="/2023/04/16/%E6%B8%B8%E8%AE%B0-%E9%9D%92%E5%B2%9B%E5%A4%A7%E7%8F%A0%E5%B1%B1/2.jpg"></p><p>其中最漂亮的就是上面这个山坡了（9:57到达），整整一个山坡都开满了杜鹃花，不过可惜的是没有特别合适的赏花角度，对面的山坡也没有专门的路过去。</p><p>这个山坡比较陡，爬的时候都不太敢停下来好好欣赏漫山的杜鹃花，另一方面原因也是人比较多，好多阿姨都在拍照。等爬过这个山坡，真的就没啥风景了，只有漫长无聊的爬山过程了，而且感觉大珠山并不是爬山-下山，而是爬山-下山-爬山-下山。。。这样，比较蜿蜒曲折吧。</p><p><img src="/2023/04/16/%E6%B8%B8%E8%AE%B0-%E9%9D%92%E5%B2%9B%E5%A4%A7%E7%8F%A0%E5%B1%B1/3.jpg"></p><p>从山坡出来，到这个山顶，大概又用了四十多分钟。其实爬完那个山坡就直接累了，上了那个山坡之后，有两个方向，应该是东西向，当时看指示牌有个石门寺，所以想去看看，后来才发现上大当了。</p><p>石门寺方向应该是往西走了（从山坡上来右转），然后是下山的感觉，以为要下山结束了，结果走着走着发现前面的路又成了上坡，离谱，走过这段路，就是到了上面这张图所在的山顶。</p><p>之后的路其实都算是下山的路了，但很多地方都是平着走，再加上这时候已经比较累了，所以就感觉这段路很长很长。而且想去的那个石门寺是在山脚下，，，另一个大珠山入口。</p><p>不过石门寺这个地方，感觉还是蛮有东西的，也算是自己第一次到真正的寺庙这种地方，尤其是看到有的人很虔诚地拜佛啥的，感受真的很不一样。</p><p><img src="/2023/04/16/%E6%B8%B8%E8%AE%B0-%E9%9D%92%E5%B2%9B%E5%A4%A7%E7%8F%A0%E5%B1%B1/4.jpg"></p><p>我上山的地方是 珠山秀谷入口，但是回来的地方是 石门寺入口，这两个地方相差还是很远的，如果只是自己步行啥的话，问题还不大，如果自驾，记得原路返回。。。石门寺入口 这个地方，出来之后，是有一个地铁口的，但这个地铁口还有好几公里，如果不想打车的话，只能步行过去了。所以说，出去玩还是得提前做好攻略。</p>]]></content>
    
    
    <categories>
      
      <category>life</category>
      
    </categories>
    
    
    <tags>
      
      <tag>travel</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>游记-青岛巨峰</title>
    <link href="/2022/08/16/%E6%B8%B8%E8%AE%B0-%E9%9D%92%E5%B2%9B%E5%B7%A8%E5%B3%B0/"/>
    <url>/2022/08/16/%E6%B8%B8%E8%AE%B0-%E9%9D%92%E5%B2%9B%E5%B7%A8%E5%B3%B0/</url>
    
    <content type="html"><![CDATA[<p>离校前的一天，莫名心血来潮，决定去崂山的最高峰——巨峰上去看看。</p><p>在此之前，去过崂山的北九水和仰口风景区，但可能更多的是陪同学朋友，似乎并没有很用心地享受爬山这一过程，也没有真正地领略山上的风景。</p><p><img src="/2022/08/16/%E6%B8%B8%E8%AE%B0-%E9%9D%92%E5%B2%9B%E5%B7%A8%E5%B3%B0/1.jpg"></p><span id="more"></span><p>说到爬山，我心里总是充满向往的，在体力充沛的前提下，总是想爬到高处看一看未曾看过的风景，所谓“一览众山小”的风景真的会让心灵得到一种空前的释放。在爬山的过程中，或是疲惫或是环境影响，心思也特别单一，跳脱出各种杂绪的困扰，拥有远离尘世的清透。直到即将到达山顶的那一刻，会看到山顶人们的欢声笑语和灵魂的升华，回头看去，如若有眼神的对视，也会不由得浮现出善意鼓励的笑容。当真正站在山顶的时候，便会被眼前的风景所震撼，写到这就真的是词穷了，没有古人的文采，更谈不上文人墨客的才华，说不上什么漂亮华丽的话，就只能将这份美景和情绪好好保留在心里，算作自己的一份宝贵财富吧。</p><p>时间线：</p><blockquote><p>8:00 北龙口站上车，113路公交（有空调），公交票价1.5</p><p>到大河东公交站下车，这里是个崂山游客服务中心，乘坐大巴车到达各风景区爬山入口</p><p>检完票上大巴车的时候已经 8:45</p><p>等到 8:15 大巴车出发</p><p>tip：上山的时候我个人感觉做大巴车左侧比较好，被晒的少</p><p>沿途其实也并不是很无聊，也是有一些风景雕塑的</p><p>9:10 下了大巴，到达目的地</p><p>买了上山的缆车票 40 块钱</p><p>全程风景都不错，能看到很多地貌风景，可惜没有空调，有些热，全程大概13分钟</p><p>9:31 下了缆车，开始爬山！</p><p>崂山巨峰山顶的路线是一个圈，没有办法登顶巨峰最高处（1132.7m），可到达的最高处在灵旗峰（1033m），具体路线可以看下面的图</p><p>如果只是想要到达灵旗峰的话，有较短的路线；如果想看全景的话，也可以完整地绕一圈。</p><p>因为我就自己，而且好不容易来一趟，肯定选择要绕一圈看全景啦</p><p>9:53 从离门出发，绕了一圈回到离门 12:20</p><p>tip：这时候已经很累了，右小腿也开始有些打颤了，不过影响不大</p><p>之后开始下山</p><p>13:18 达到山脚，又累又饿</p><p>13:50 开始等公交回学校，结束了这次巨峰之旅</p></blockquote><p><img src="/2022/08/16/%E6%B8%B8%E8%AE%B0-%E9%9D%92%E5%B2%9B%E5%B7%A8%E5%B3%B0/2.jpg"></p><p>在巨峰风景区，有五个门，分别对应乾坤震巽坎离兑，所谓五行八卦。也符合崂山作为道教起源地的一个风格吧，沿途的风景很美，也很震撼，哪怕说治愈心灵，我也不觉得过分，还是放些图吧。</p><center class="half"><img src="/2022/08/16/%E6%B8%B8%E8%AE%B0-%E9%9D%92%E5%B2%9B%E5%B7%A8%E5%B3%B0/3.jpg" width="200"><img src="/2022/08/16/%E6%B8%B8%E8%AE%B0-%E9%9D%92%E5%B2%9B%E5%B7%A8%E5%B3%B0/8.jpg" width="200"><img src="/2022/08/16/%E6%B8%B8%E8%AE%B0-%E9%9D%92%E5%B2%9B%E5%B7%A8%E5%B3%B0/10.jpg" width="200"><img src="/2022/08/16/%E6%B8%B8%E8%AE%B0-%E9%9D%92%E5%B2%9B%E5%B7%A8%E5%B3%B0/11.jpg" width="200"></center><p>五个门，我唯独没有看到乾门，可能是错过了吧，不过对我来说，抬头不就是天（乾，八卦的首卦：天）嘛</p><p>回忆爬山，没有什么特别印象深刻的，但似乎每一刻都印在脑海中，都是一副绝美的画面。说不出什么特别的感受，脑海中也想不出很美的诗词话语，但真的很放空，真的很享受。</p><center class="half"><img src="/2022/08/16/%E6%B8%B8%E8%AE%B0-%E9%9D%92%E5%B2%9B%E5%B7%A8%E5%B3%B0/4.jpg" width="200"><img src="/2022/08/16/%E6%B8%B8%E8%AE%B0-%E9%9D%92%E5%B2%9B%E5%B7%A8%E5%B3%B0/6.jpg" width="200"><img src="/2022/08/16/%E6%B8%B8%E8%AE%B0-%E9%9D%92%E5%B2%9B%E5%B7%A8%E5%B3%B0/7.jpg" width="200"></center><center class="half"><img src="/2022/08/16/%E6%B8%B8%E8%AE%B0-%E9%9D%92%E5%B2%9B%E5%B7%A8%E5%B3%B0/12.jpg" width="200"><img src="/2022/08/16/%E6%B8%B8%E8%AE%B0-%E9%9D%92%E5%B2%9B%E5%B7%A8%E5%B3%B0/13.jpg" width="200"><img src="/2022/08/16/%E6%B8%B8%E8%AE%B0-%E9%9D%92%E5%B2%9B%E5%B7%A8%E5%B3%B0/9.jpg" width="200"></center><p>今天写这篇游记，已经是8.16号了，放完暑假回到学校了。总是会有很多感慨和想法思绪，也不知道该如何和他人诉说，哎，太多顾虑和挂在心上的事情了，可能是太多遗憾了吧，很多时候真的心累。</p><p>但是呢，还是要往前看啊，总是要开心啊，要享受生活，要追求美好啊！</p>]]></content>
    
    
    <categories>
      
      <category>life</category>
      
    </categories>
    
    
    <tags>
      
      <tag>travel</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>论文阅读-0x21</title>
    <link href="/2022/03/01/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-0x21/"/>
    <url>/2022/03/01/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-0x21/</url>
    
    <content type="html"><![CDATA[<p>论文阅读笔记：</p><p>《Not all patches are what you need: Expediting vision transformersvia token reorganizations》，<a href="https://github.com/youweiliang/evit">[code]</a> ，ICLR2022</p><p>《HerosNet: Hyperspectral Explicable Reconstruction and OptimalSampling Deep Network for Snapshot Compressive Imaging》，<a href="https://github.com/jianzhangcs/HerosNet">[code]</a> , CVPR2022</p><span id="more"></span><h2 id="not-all-patches-are-what-you-need-expediting-vision-transformers-via-token-reorganizations">Notall patches are what you need: Expediting vision transformers via tokenreorganizations</h2><h3 id="abstract">Abstract</h3><ul><li>之前的VIT（visiontransfomer）将图像块作为token，来建立多头自注意力（multi-headself-attention，MHSA）<ul><li>但随着VIT的发展，其计算量也随着不断增长。与传统CNN相比，VIT的 globalself-attention between image tokens and long-range dependency使得模型收敛较慢</li><li>不过却很少有人关注VIT加速，因为和CNN的差异，CNN加速的一些方法（蒸馏和修建等操作）无法应用于VIT</li></ul></li><li>但本文作者认为这种方式会包含冗余计算，因为其包含了语义上无意义或分散注意力的图像背景的token</li><li><figure><img src="/2022/03/01/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-0x21/1646114994442.png" alt="1646114994442"><figcaption aria-hidden="true">1646114994442</figcaption></figure></li><li>因此作者希望在前反馈阶段中，重新对这些token进行组织<ul><li>对于每个前向推理，我们在MHSA和FFD（feed-forwardnetwork）之间标记图像块，这些操作是通过相应的类标记注意力引导实现的</li><li>然后我们<strong>保留关注的图像tokens和融合不关注的图像tokens</strong>来重组图像tokens，以加快后续MHSA和FFN计算</li><li>这样的话就允许梯度反向传播通过不关注的tokens，从而更好地识别关注tokens。通过这种方式，随着网络的深入，逐渐减少图像tokens的数量，以降低计算成本</li><li>此外，在不引入额外参数的情况下，可以通过识别过程灵活控制ViT主干的容量</li></ul></li><li>作者在本文中的两个改进角度：<ul><li>在相同数量的输入图像token情况下，该方法减少了MHSA和FFD计算量，实现高效推理<ul><li>ImageNet分类：在牺牲DeiT-S模型0.3%精度的基础上，将速度提高了50%</li></ul></li><li>通过保持相同的计算成本，该方法可以容许更多的token输入，以提高识别精度，适用于高分辨率图像<ul><li>ImageNet分类：保留和DeiT-S模型计算成本相同的基础上，提高了1%的精度</li><li>此外在 multiply-accumulate computation (MAC)指标下，计算成本降低36%的同时，DeiT-S精度从79.8% 提高到 80.7%</li></ul></li></ul></li></ul><h3 id="method">Method</h3><h4 id="vit">VIT</h4><p>VIT相关的具体细节可以参考链接内容，感觉写的都很详细：<a href="https://blog.csdn.net/qq_37541097/article/details/118242600">link_1</a>，<a href="https://python.iitter.com/other/121733.html">link_2</a></p><ul><li>VIT通过将输入图像分割成patch并将每个patch投影到token来执行标记化</li><li>此外还有一个额外的类标记 [CLS]被添加到图像tokens集合中，并负责聚合全局图像信息和最终分类</li><li>所有的tokens都通过 learnable vector (positional encoding)进行添加，并输入到transfomer中</li><li>在MHSA中，token进一步被映射成三个矩阵QKV，进行注意力操作<ul><li><span class="math inline">\(\operatorname{Attention}(\boldsymbol{Q},\boldsymbol{K},\boldsymbol{V})=\operatorname{Softmax}\left(\frac{\boldsymbol{Q}\boldsymbol{K}^{\top}}{\sqrt{d}}\right) \boldsymbol{V}\)</span><ul><li>其中 d 是Q向量的长度， <span class="math inline">\(\operatorname{Softmax}\left(\boldsymbol{QK}^{\top} / \sqrt{d}\right)\)</span> 是注意力矩阵</li><li>注意力矩阵的第一行 [CLS] 表示对所有 tokens 的注意力，用于确定每个token 的注意力</li></ul></li></ul></li><li>MHSA的输出进入到FFN<ul><li>FFN由两个全连接层组成，中间有一个GELU激活层</li></ul></li><li>在最终的Transformer encoder，提取 [CLS] 并用于对象类别预测</li></ul><h4 id="attention-token-identification">Attention tokenidentification</h4><ul><li>在VIT中，最终是通过 [CLS]来进行分类，和其他tokens的交互通过encoder的注意力机制完成<ul><li><span class="math inline">\(\boldsymbol{x}_{\text {class}}=\operatorname{Softmax}\left(\frac{\boldsymbol{q}_{\mathrm{class}}\cdot \boldsymbol{K}^{\top}}{\sqrt{d}}\right)\boldsymbol{V}=\boldsymbol{a} \cdot \boldsymbol{V}\)</span><ul><li>这里的 <span class="math inline">\(q_{class}\)</span> 表示[CLS]</li></ul></li><li>因此 [CLS] token 的输出 <span class="math inline">\(\boldsymbol{x}_{\text {class }}\)</span> 是 <span class="math inline">\(\boldsymbol{V}=\left[\boldsymbol{v}_{1},\boldsymbol{v}_{2}, \ldots, \boldsymbol{v}_{n}\right]^{\top}\)</span>向量的线性组合</li><li>组合系数 <span class="math inline">\(\boldsymbol{a}\)</span> 是[CLS] 对所有 tokens 的注意力值</li></ul></li><li>由于 <span class="math inline">\(\boldsymbol{v}_{i}\)</span> 表示第<span class="math inline">\(i\)</span> 个token， 因此 <span class="math inline">\(\boldsymbol{a}_i\)</span> 决定了该 token会有多少信息通过线性组合的方式融合到 [CLS] 的输出 <span class="math inline">\(\boldsymbol{x}_{\text {class }}\)</span> 中，因此很容易注意到可以<strong>用该注意力值来表示 token的重要性</strong></li><li>此外还有相关工作表明，ViTs中的 [CLS]更关注特定于类的标记（具有更大的 attention 值）</li><li><figure><img src="/2022/03/01/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-0x21/1646123079425.png" alt="欧泽"><figcaption aria-hidden="true">欧泽</figcaption></figure></li><li>因此作者使用 [CLS] 来识别 tokens的重要程度，基于这些参数来移除注意力值最小的token，但从上表可以看出，这会严重影响分类精度，所以作者在训练阶段加入了图像token 重组</li><li>在MHSA中，因为 multi-head 会并行计算SA，所以也就存在多个 [CLS]注意力向量 <span class="math inline">\(\boldsymbol{a}^h\)</span><ul><li>所以在这里作者计算了所有 head 的平均注意力值 <span class="math inline">\(\overline{\boldsymbol{a}}=\sum_{h=1}^{H}\boldsymbol{a}^{(h)} / H\)</span></li><li>如下图所示，作者保留了 <span class="math inline">\(k\)</span>个注意力值 $ $ 最大的tokens（ <span class="math inline">\(top-k\)</span>），将其成为注意力tokens，同时将其他tokens融合成为一个新的 token</li></ul></li></ul><figure><img src="/2022/03/01/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-0x21/1646124753547.png" alt="1646124753547"><figcaption aria-hidden="true">1646124753547</figcaption></figure><h4 id="inattentive-token-fusion">Inattentive token fusion</h4><ul><li>虽然图像背景tokens包含较少的信息，可以在不显著影响VIT性能的情况下丢弃，但作者认为它们仍然是有助于预测结果</li><li>另一方面，作者是选择保留了固定数量的tokens，所以当图像中的对象较大时，移除与之相关的部分，会对性能造成负面影响</li><li>综上，作者将不重要的tokens <span class="math inline">\(\mathcal{N}\)</span>进行融合，以补充所关注的tokens</li><li>具体操作就是加权平均运算<ul><li><span class="math inline">\(x_{\text {fused }}=\sum_{i \in\mathcal{N}} a_{i} x_{i}\)</span></li><li>融合之后的token将继续向后传播</li><li>与VIT的批量运算比，这种计算成本可以忽略不计</li></ul></li></ul><h3 id="code">Code</h3><figure><img src="/2022/03/01/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-0x21/1646208998163.png" alt="1646208998163"><figcaption aria-hidden="true">1646208998163</figcaption></figure><h3 id="analysis-experiments">Analysis &amp; Experiments</h3><h4 id="training-with-higher-resolution-images">Training with higherresolution images</h4><ul><li>这里因为对tokens进行了融合减少，所以在维持相同计算量的情况下，允许更多地tokens输入</li><li>因此该方法可以输入更高分辨率图像</li><li>而对于ImageNet，作者通过差值方法将标准输入图像 224×224 调整为256×256</li><li><figure><img src="/2022/03/01/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-0x21/1646201662471.png" alt="1646201662471"><figcaption aria-hidden="true">1646201662471</figcaption></figure><ul><li>multiply-accumulate computations(MACs) —— 乘加累积操作数<ul><li>1MACs包含一个乘法操作与一个加法操作，大约包含2FLOPs，通常MACs与FLOPs存在一个2倍的关系</li></ul></li></ul></li></ul><h4 id="visualization">Visualization</h4><ul><li>本文作者提出的 EViT 通过识别 attention tokens 和丢弃 inattentiontokens 的冗余计算来加速VIT</li><li>因此为了进一步研究 EViT 的可解释性，作者进行了可视化实验</li><li><figure><img src="/2022/03/01/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-0x21/1646201800669.png" alt="1646201800669"><figcaption aria-hidden="true">1646201800669</figcaption></figure></li></ul><h4 id="implementation-details">Implementation details</h4><ul><li>本文的 token identification module 被添加到 DeiT-S 和 DeiT-B的第4、7、10层中，以及 LV -ViT-S 的第5、9、13层</li><li>另外作者采用了 warm up 的策略来进行 tokens 的标记。让 attentiontokens 的保留率由1逐渐降低到目标值</li><li><figure><img src="/2022/03/01/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-0x21/1646202716459.png" alt="1646202716459"><figcaption aria-hidden="true">1646202716459</figcaption></figure></li></ul><h4 id="inattentive-token-fusion-1">Inattentive token fusion</h4><ul><li>为了缓解 tokens 移除过程中的信息丢失，作者提出了 inattention tokens融合，根据 [CLS] 的注意力值来进行融合</li><li><figure><img src="/2022/03/01/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-0x21/1646202878260.png" alt="1646202878260"><figcaption aria-hidden="true">1646202878260</figcaption></figure><ul><li>虽然性能提升上不是很大，但这种融合方法基本没有计算量</li><li>此外作者认为，这也表明 attention token identification的有效性，因为大部分关注的tokens被保留下来</li></ul></li><li><figure><img src="/2022/03/01/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-0x21/1646203107405.png" alt="1646203107405"><figcaption aria-hidden="true">1646203107405</figcaption></figure></li></ul><h4 id="epochs-of-training">Epochs of training</h4><ul><li>在有效的计算机制中，训练时间越长，越有益于VIT，如上面表四</li></ul><h4 id="trainingfinetuning-on-higher-resolution-images">Training/Finetuningon higher resolution images</h4><ul><li>通过融合 inattentiontokens，可以在相同计算成本下输入更多tokens，如上面表五</li></ul><h4 id="training-with-an-oracle-vit">Training with an oracle ViT</h4><ul><li>本文中，token 选择的标准是 [CLS] 的注意力值，因此提前学习到每个tokens 对预测任务的重要性会十分有帮助</li><li>作者引入了 oracle VIT 来指导 [CLS] 完成tokens的选择</li><li>使用经过充分训练的 DeiT-S/B 作为 oracle ，并使用 oracle ViT的参数初始化 EViT-DeiT-S/B</li><li><figure><img src="/2022/03/01/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-0x21/1646205627366.png" alt="1646205627366"><figcaption aria-hidden="true">1646205627366</figcaption></figure></li></ul><h4 id="comparison-with-dynamicvit">Comparison with DynamicViT</h4><figure><img src="/2022/03/01/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-0x21/1646205832834.png" alt="1646205832834"><figcaption aria-hidden="true">1646205832834</figcaption></figure><h4 id="comparison-with-other-vision-transformers">Comparison with otherVision Transformers</h4><figure><img src="/2022/03/01/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-0x21/1646208727006.png" alt="1646208727006"><figcaption aria-hidden="true">1646208727006</figcaption></figure><h4 id="using-tokens-to-tokens-attentive-scores-for-inattentive-tokens-identification">Usingtokens-to-tokens attentive scores for inattentive tokensidentification</h4><ul><li>作者使用 [CLS] 作为注意力来区别 tokens 的动机是因为VIT的最终预测就是通过 [CLS] 来决定的</li><li>但同时作者也发现可以用 tokens-to-tokens 注意力来区别 tokens（也就是每个 tokens 之间的关联程度QK），其注意力计算为所有 tokens 和tokens 之间的注意力均值<ul><li><span class="math inline">\(\boldsymbol{A}=\operatorname{Softmax}\left(\frac{\boldsymbol{Q}\boldsymbol{K}^{\top}}{\sqrt{d}}\right)\)</span></li><li>注意力分数计算为 <span class="math inline">\(\boldsymbol{a}=\frac{1}{n} \sum_{i=1}^{n}\boldsymbol{A}_{i,:}\)</span><ul><li>其中 n 表示 token 数目，<span class="math inline">\(\boldsymbol{A}_{i}\)</span> 是 attention map 的第<span class="math inline">\(i\)</span> 行</li><li>对于多头注意力 <span class="math inline">\(\overline{\boldsymbol{a}}=\sum_{h=1}^{H}\boldsymbol{a}^{(h)} / H\)</span></li></ul></li></ul></li><li>因此可以看出来，在 tokens-to-tokens 策略中，每个 token块都参与到了投票中；而在 [CLS] 中，只有 [CLS] token 参与到了投票中</li><li>实验结果标明，两者性能差不多，但 tokens-to-tokens 策略效率较低</li><li><figure><img src="/2022/03/01/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-0x21/1646226160508.png" alt="1646226160508"><figcaption aria-hidden="true">1646226160508</figcaption></figure></li></ul><h4 id="training-time-of-evit">Training time of EViT</h4><figure><img src="/2022/03/01/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-0x21/1646226237984.png" alt="1646226237984"><figcaption aria-hidden="true">1646226237984</figcaption></figure><h4 id="finetuning-of-evit-on-high-resolution-images">Finetuning of EViTon high resolution images</h4><ul><li>为了将EViT-DeiT-S从低分辨率图像微调到高分辨率图像，作者在训练低分辨率图像时进行了差值操作</li><li>但作者发现，直接在高分辨率的图像上直接训练似乎更好，因为它需要更少的训练时间（没有执行Fine-tune ），并且通常比 Fine-tune 获得更高的精度</li><li><figure><img src="/2022/03/01/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-0x21/1646226399364.png" alt="1646226399364"><figcaption aria-hidden="true">1646226399364</figcaption></figure></li></ul><h4 id="token-reorganization-locations">Token reorganizationlocations</h4><ul><li>在这里作者觉得 tokens的保留率和重组位置不同可能会达到相同的计算效率，结果如下图<ul><li>将重组 token 移动到浅层，并增加保留率</li><li>将重组 token 移动到深层，并减少保留率，以保持计算成本近似不变</li></ul></li><li><figure><img src="/2022/03/01/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-0x21/1646235085279.png" alt="1646235085279"><figcaption aria-hidden="true">1646235085279</figcaption></figure></li><li>实验揭示了以下观点：<ul><li>将重组模块移动到较浅的层会降低精度<ul><li>当重组 token置于第三层之前，即使计算成本相同，识别精度也会显著下降</li><li>这表明 VIT 在前面的阶段无法识别重要 tokens</li><li>既在浅层中，注意力图对于 tokens 的移除融合并不是可靠的</li></ul></li><li>将重组模块放置在不同的更深层次上，对准确性的影响微乎其微<ul><li>这表明，只要重组模块位置不在浅层，EViT 就具有稳定的性能</li></ul></li></ul></li><li>在本文中，作者采用了一种简单的策略来确定重组模块的位置<ul><li>让重组模块的位置均匀分割 ViT</li><li>保留率则都设置为相同的值</li></ul></li></ul><h4 id="adopting-the-dino-attention-map-for-inattentive-token-identification">Adoptingthe DINO attention map for inattentive token identification</h4><ul><li>在 DINO 工作中，作者表明，自监督训练 ViT 产生的 [CLS]注意力会更集中在图像中的物体部位</li><li>因此作者探究了 DINO 能否可以用于 inattention tokens 的识别</li><li>作者将 DINO 最后一层产生的 [CLS] 来指导 EViT 中 tokens 的选择</li><li>而且这里作者直接把重组模块放在了第一层，而且保留率为0.5</li><li><figure><img src="/2022/03/01/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-0x21/1646268113321.png" alt="1646268113321"><figcaption aria-hidden="true">1646268113321</figcaption></figure></li><li>但也因为需要使用 DINO 模型，所以也会带来额外的计算负担</li></ul><h4 id="the-correlation-between-the-quality-of-the-attention-masks-and-the-performance-of-evit">Thecorrelation between the quality of the attention masks and theperformance of EViT</h4><ul><li>从上面的可视化结果图中可以看出来，mask的效果非常直观有效，意味着[CLS] 对 tokens 的注意值与图像的语义分割有很好的相关性</li><li>为此作者在 EViT 中故意使用低质量的 mask ，以查看其性能变化<ul><li>topk：选择 [CLS] 注意力中 k 个最大的</li><li>mink：与之相反，取最小的 k 个</li><li>random：随机选</li></ul></li><li><figure><img src="/2022/03/01/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-0x21/1646269110667.png" alt="1646269110667"><figcaption aria-hidden="true">1646269110667</figcaption></figure></li></ul><h4 id="the-attention-scores-of-the-inattentive-tokens-w.r.t.-the-layer-depth">Theattention scores of the inattentive tokens w.r.t. the layer depth</h4><ul><li>为了发现 inattention tokens的注意力分数是如何演变的，作者随机抽样了几张图像，并绘制了 [CLS]的变化</li><li>使用一个预训练的 DeiT-S ，并用最小 [CLS] 来标识tokens，并进行跟踪</li><li><figure><img src="/2022/03/01/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-0x21/1646269735688.png" alt="1646269735688"><figcaption aria-hidden="true">1646269735688</figcaption></figure><ul><li>箱子的中间一条线，是数据的中位数，代表了样本数据的平均水平</li><li>箱子的上下限，分别是数据的上四分位数和下四分位数。这意味着箱子包含了50%的数据。因此，箱子的宽度在一定程度上反映了数据的波动程度</li><li>在箱子的上方和下方的一条线，代表着最大最小值</li></ul></li></ul><figure><img src="/2022/03/01/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-0x21/1646269897134.png" alt="1646269897134"><figcaption aria-hidden="true">1646269897134</figcaption></figure><h2 id="herosnet-hyperspectral-explicable-reconstruction-and-optimal-sampling-deep-network-for-snapshot-compressive-imaging">HerosNet:Hyperspectral Explicable Reconstruction and Optimal Sampling DeepNetwork for Snapshot Compressive Imaging</h2><p>文章链接：https://arxiv.org/abs/2112.06238</p><p>代码链接：https://github.com/jianzhangcs/HerosNet</p><h3 id="abstract-1">Abstract</h3><ul><li><strong>压缩感知 compressive sensing, CI</strong><ul><li>压缩感知被应用于电子工程尤其是信号处理中，用于<strong>获取和重构稀疏或可压缩的信号</strong><ul><li>信号处理领域中的一个常见问题就是从一系列的采样中重建原本的信号。一般而言，未被采样的部分信号，是不可能重建出来的。然而通过借助对于信号（性质）的预先了解或合理假设，完美地通过一系列采样重建原信号就成为了可能。</li></ul></li><li>奈奎斯特采样定理证明了若<strong>信号的最高频率小于采样频率的一半</strong>，便可完美地从采样结果中恢复原本信号，因此定义了采样定理取样频率的下限</li><li>详细的可以看这个<a href="https://bbs.huaweicloud.com/blogs/282786">解释</a></li></ul></li></ul><figure><img src="/2022/03/01/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-0x21/0.gif" alt="img"><figcaption aria-hidden="true">img</figcaption></figure><ul><li><strong>光谱快照压缩感知 spectral snapshot compressive sensing,SCI</strong><ul><li>旨在通过2D探测器来获取3D场景信息</li><li>具有低带宽、低成本和高数据吞吐量的优点，在遥感、目标检测、超分辨率和医疗诊断等广泛应用中发挥着越来越关键的作用</li></ul></li><li>高光谱图像 Hyperspectral Image, HSI<ul><li>有着更多的波段（即通道数更多）来更加准确全面的描述被捕获场景的特性</li><li>传统的成像设备采用光谱仪对成像场景进行空间域通道维度的扫描，费时费力，不适用于运动场景</li><li>因此设计出了上述的光谱快照压缩感知</li></ul></li><li>在本文中，作者重点介绍了编码孔径快照光谱成像仪（coded aperturesnapshot spectral imager, CASSI）<ul><li>该系统通过编码孔径（即物理掩模）调制光谱帧，并通过分散器在光谱维度上移动光谱帧<ul><li>CASSI系统首先通过一个编码孔径掩膜对成像场景的各光谱通道进行调制，然后通过一个三棱镜进行色散后在相机上生成一个二维的快照估计图（compressivemeasurement）</li></ul></li><li>CASSI的设备很贵，价格在 10,000 到 100,000美金。不过它有一个优势就是，存储的时候只需要存储二维的measurement，这可以极大降低存储与传输数据的成本</li></ul></li></ul><figure><img src="https://inews.gtimg.com/newsapp_bt/0/14888139640/1000" alt="img"><figcaption aria-hidden="true">img</figcaption></figure><ul><li>因此就可以想到通过深度学习来完成这项工作</li><li>利用深度学习方法，将数值优化算法映射到深度展开网络（deep unfoldingnetworks, DUNs）在光谱快照压缩感知（spectral snapshot compressivesensing, SCI）中取得了较大成果<ul><li>DUN：将求解一个给定连续模型的迭代优化看成是一个动态系统，进而通过<strong>若干可学习模块来离散化</strong>这一系统，得到数据驱动的演化过程（Data-dependentPropagation）的方法<ul><li>详细的可以参考<a href="https://www.jiqizhixin.com/articles/2018-09-07">链接</a></li></ul></li><li>但这些方法缺乏跨阶段的特征交互和自适应参数调整</li></ul></li><li>本文提出了一种全新的高光谱可解释性重建和最佳采样深度网络—— HerosNet<ul><li>在梯度下降步骤中，每个阶段都可以灵活地模拟感知矩阵，并根据上下文调整步长</li><li>在近端映射步骤中分层融合和交互先前阶段的隐藏状态，以有效恢复当前 HSI帧</li><li>端到端学习硬件友好的最佳二进制掩码，以进一步提高重建性能</li></ul></li><li>在过去的几年中，已经有许多 HSI重建方法，包括基于模型和基于深度学习的方法<ul><li>基于模型的方法<ul><li>往往是迭代搜索最优解，并通过图像先验将结果细化到期望的信号域</li><li>虽然这些方法具有高度的可解释性，但它们受到手工制作的先验知识和缓慢重建速度的限制</li></ul></li><li>基于深度学习的方法<ul><li>部分方法直接学习从2D测量到3D高光谱立方体的端到端逆映射</li><li>倾向于大幅降低时间复杂度并获得更好的性能</li><li>然而没有明确体现系统成像模型，只是被训练成一个黑箱</li></ul></li></ul></li><li>最近 DUN 引入HSI重建任务，它融合了基于模型和基于学习的方法的优点<ul><li>通过梯度下降模块执行迭代过程，并通过深度先验模块优化中间结果</li><li>问题一：如何有<strong>效地交互和融合相位间的特征</strong>是提高重建质量的关键<ul><li>大多数现有DUN没有在程序和后续阶段之间建立联系</li><li>随着阶段数量的增加，在信息传输过程中，有益的信息往往会丢失<ul><li>记忆机制的启发：先前相位的隐藏状态可以为当前相位的计算提供补充信息</li></ul></li><li>因此有必要在不同阶段之间引入特征交互机制，以获得增强的特征表示</li></ul></li><li>问题二：如何<strong>动态学习梯度下降模块中的参数</strong><ul><li>以前通常使用固定参数来压缩 HSI</li><li>然而固定的参数不能在不同场景中进行自适应和上下文调整，这将导致次优重建，限制了DUNs的灵活性</li></ul></li><li>问题三：现有的DUN没有将掩码优化和图像重建<strong>结合到一个统一的框架</strong>中<ul><li>不能完全保留HSIs的结构和信息</li></ul></li></ul></li><li>作者说本文是受 ISTA 启发，提出了本文的 HerosNet，联合掩码优化和 HSI重建<ul><li>迭代收缩阈值算法（Iterative Shrinkage Thresholding Algorithm）</li><li>大致思路就是在每一次迭代中通过一个收缩/软阈值操作进行更新</li><li>详细的分析和解释可以参考<a href="https://www.cnblogs.com/louisanu/p/12045861.html">链接</a></li></ul></li><li>该网络由采样子网、初始化子网和恢复子网组成<ul><li>每个恢复阶段对应一个ISTA迭代，并且所有参数都是端到端学习的<ul><li>因此该网络具有高质量重建和强解释性的优点</li></ul></li></ul></li><li>本文重要贡献：<ul><li>ISTA 启发的深度展开网络 HerosNet，用于联合学习二进制最优掩码和恢复高质量HSI</li><li>dynamic gradient descent module(DGDM)：在梯度下降步长中，灵活模拟传感矩阵并根据上下文调整步长</li><li>hierarchical feature interaction module(HFIM)：融合并交互先前相位的隐藏状态，在近端映射中恢复当前相位的HSI帧</li><li>在模拟和真实数据集上的性能大大优于最先进的方法</li></ul></li></ul><h3 id="method-1">Method</h3><h4 id="problem-formulation">Problem Formulation</h4><ul><li>在 CASSI体系中，三维的高光谱立方体数据首先会通过编码孔径（物理掩码）进行调制，然后通过棱镜进行色散<ul><li>考虑 <span class="math inline">\(C\)</span> 通道数的序列 <span class="math inline">\(\{ X_i \} ^{C} _{i = 1} \in \mathbb R ^ {H \timesW}\)</span> ，该序列输入通过掩码进行调制</li><li><span class="math inline">\(\mathbf{X}_{i}^{\prime}=\mathbf{M} \odot\mathbf{X}_{i}\)</span></li><li><span class="math inline">\(\mathbf{X}_{i}\)</span> 是调制后的 HSI帧，<span class="math inline">\(\odot\)</span> 是哈达玛积（ 逐项乘积）</li></ul></li><li>之后不同波长的 HSI 帧在空间位置进行位移并求和，因此调制后的 HSI 帧<span class="math inline">\(\{ X_i^{\prime} \} ^{C} _{i = 1} \in \mathbbR ^ {H \times W}\)</span> 压缩为编码测量值<ul><li><span class="math inline">\(\mathbf{Y}(m, n)=\sum_{i=1}^{C}\mathbf{X}_{i}^{\prime}\left(m, n+d_{i}\right)+\mathbf{N}\)</span><ul><li>其中m，n表示空间坐标， <span class="math inline">\(d_i\)</span> 表示<span class="math inline">\(i\)</span> 通道的移动距离</li><li><span class="math inline">\(N \in \mathbb R ^ {H \times(W+C-1)}\)</span> 和 <span class="math inline">\(Y \in \mathbb R ^ {H\times (W+C-1)}\)</span> 分别表示噪声和压缩测量结果</li></ul></li></ul></li><li>综上，SCI 可以矢量表示为<ul><li><span class="math inline">\(\mathbf{y}=\boldsymbol{\Phi}\mathbf{x}+\mathbf{n}\)</span><ul><li>其中 <span class="math inline">\(\Phi \in \mathbb R ^ {H (W+C-1)\times HWC}\)</span> 表示感知矩阵</li></ul></li></ul></li></ul><figure><img src="/2022/03/01/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-0x21/1655201595202.png" alt="1655201595202"><figcaption aria-hidden="true">1655201595202</figcaption></figure><figure><img src="/2022/03/01/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-0x21/The-process-of-CASSI-imaging-is-depicted-A-N-N-L-spectral-data-cube-is-spatially.png" alt="The process of CASSI imaging is depicted. A N × N × L spectral data... | Download Scientific Diagram"><figcaption aria-hidden="true">The process of CASSI imaging is depicted.A N × N × L spectral data... | Download Scientific Diagram</figcaption></figure><h4 id="herosnet">HerosNet</h4><p>该网络由采样子网、初始化子网和恢复子网组成</p><figure><img src="/2022/03/01/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-0x21/1655297639978.png" alt="1655297639978"><figcaption aria-hidden="true">1655297639978</figcaption></figure><h5 id="采样子网">采样子网</h5><ul><li>旨在学习 HSI压缩感知的最佳二进制掩码，从而保留足够的光谱空间信息并消除冗余</li><li>训练过程分为三个阶段：随机化、二值化和压缩</li><li>为了学习二进制掩码 <span class="math inline">\(\mathbf{M}\)</span><ul><li>首先采用随机高斯初始化（均值为 <span class="math inline">\(\mu_b\)</span> ，方差为 <span class="math inline">\(\sigma_b\)</span> ）来生成连续矩阵 <span class="math inline">\(\tilde{\mathbf{M}}\)</span></li><li><span class="math inline">\(\mathbf{M}=\operatorname{BinarySign}(\tilde{\mathbf{M}})\)</span></li><li>其中 <span class="math inline">\(\operatorname{BinarySign}(\cdot)\)</span>用来将连续矩阵转换为二进制掩码</li><li><span class="math inline">\(\operatorname{BinarySign}(z)=1\)</span>if <span class="math inline">\(z \geq \mu_{b}\)</span> or 0 else</li></ul></li><li>此外，根据上面的成像函数表示，利用转换函数将二进制掩码转换为压缩传感矩阵<ul><li><span class="math inline">\(\Phi=\operatorname{Mask2Mat}(\mathbf{M})\)</span></li><li><span class="math inline">\(\Phi\)</span> 被视为可学习参数</li><li>将二值化函数的导数设置为常数 1， 用于反向传播</li></ul></li><li>通过上面两个变换，将三维高光谱立方体数据 <span class="math inline">\(\mathbf{x}\)</span> 压缩为快照测量值 <span class="math inline">\(\mathbf{y}\)</span></li></ul><h5 id="初始化子网">初始化子网</h5><ul><li>初始化子网旨在将此二维测量 <span class="math inline">\(\mathbf{y}\in \mathbb R ^ {H \times (W+C-1)}\)</span> 拆分为三维高光谱立方体数据<span class="math inline">\(\mathbf{x}  \in \mathbb R ^ {H \times W\times C}\)</span></li><li>具体而言，通过以步长 <span class="math inline">\(d\)</span>滑动来得到 <span class="math inline">\(C\)</span> 个通道</li><li>然后再串联起来，组成一个三维高光谱立方体 <span class="math inline">\(\mathbf{x}^{(0)}  \in \mathbb R ^ {H \times W\times C}\)</span></li></ul><h5 id="恢复子网">恢复子网</h5><ul><li><p>旨在从压缩测量 <span class="math inline">\(\mathbf{y}\)</span>中重建高质量的 HSI</p></li><li><p>受 ISTA 启发，图像重建被视为优化问题</p><ul><li><span class="math inline">\(\mathbf{x}=\arg \min _{\mathbf{x}}\frac{1}{2}\|\mathbf{y}-\mathbf{\Phi} \mathbf{x}\|_{2}^{2}+\lambda\psi(\mathbf{x})\)</span><ul><li>第一项为数据保真度，第二项 <span class="math inline">\(\psi(\cdot)\)</span> 为先验正则化，其中 <span class="math inline">\(\lambda\)</span> 表示正则化参数</li></ul></li></ul></li><li><p>为了优化上面的公式，作者根据 ISTA来设计深度网络，以实现其简单性和可解释性</p></li><li><p>传统的ISTA通过两个步骤更新结果：梯度下降和近端映射</p><ul><li><span class="math inline">\(\mathbf{r}^{(k)}=\mathbf{x}^{(k-1)}-\rho\boldsymbol{\Phi}^{\top}\left(\mathbf{\Phi}\mathbf{x}^{(k-1)}-\mathbf{y}\right)\)</span></li><li><span class="math inline">\(\mathbf{x}^{(k)}=\underset{\mathbf{x}}{\arg \min }\frac{1}{2}\left\|\mathbf{x}-\mathbf{r}^{(k)}\right\|_{2}^{2}+\lambda\psi(\mathbf{x})\)</span><ul><li>其中 <span class="math inline">\(k\)</span> 表示 ISTA 的迭代次数，<span class="math inline">\(\rho\)</span> 表示步长</li></ul></li><li>通过引入近端映射操作 <span class="math inline">\(\operatorname{prox}_{\lambda\psi}(\mathbf{r})=\arg \min _{\mathbf{x}}\frac{1}{2}\|\mathbf{x}-\mathbf{r}\|_{2}^{2}+\lambda\psi(\mathbf{x})\)</span> ，可以得到</li><li><span class="math inline">\(\mathbf{x}^{(k)}=\operatorname{prox}_{\lambda\psi}(\mathbf{r}^{(k)})\)</span></li></ul></li><li><p>作者通过修改这两个步骤，设计了动态梯度下降模块（dynamic gradientdescent module, DGDM）和层次特征交互模块（hierarchical featureinteraction module, HFIM）</p></li><li><p><strong>动态梯度下降模块（dynamic gradient descent module,DGDM）</strong></p><figure><img src="/2022/03/01/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-0x21/1655281842824.png" alt="1655281842824"><figcaption aria-hidden="true">1655281842824</figcaption></figure><ul><li><span class="math inline">\(\mathbf{r}^{(k)}=\mathbf{x}^{(k-1)}-\rho\boldsymbol{\Phi}^{\top}\left(\mathbf{\Phi}\mathbf{x}^{(k-1)}-\mathbf{y}\right)\)</span></li><li>为了实现上述公式，作者设计 DGDM 来动态生成重建结果 <span class="math inline">\(\mathbf r ^ k\)</span></li><li>大多数现有的 DUN 处理 <span class="math inline">\(\Phi\)</span>时，<span class="math inline">\({\Phi}^{\top}\)</span> 和 <span class="math inline">\(\rho\)</span>作为常数，但这限制了网络的灵活性和重建效果</li><li>为了解决上述问题：</li><li>作者设计了模块 <span class="math inline">\(\mathcal{H}_\Phi(\cdot)\)</span> 和 <span class="math inline">\(\mathcal{H}_{\Phi^{\top}}(\cdot)\)</span>用来从上一个阶段 <span class="math inline">\(\mathbf{x}^{(k-1)}\)</span>中模拟 <span class="math inline">\(\Phi\)</span> 和 <span class="math inline">\({\Phi}^{\top}\)</span></li><li>对于内容感知的参数调整，在梯度下降过程中引入动态步长算子 <span class="math inline">\(\mathcal{H}_{d y \rho}(\cdot)\)</span>来增强网络泛化能力</li><li>其中步长 <span class="math inline">\(\tilde{\rho}^{(k)}\)</span>也是直接从上一个阶段 <span class="math inline">\(\mathbf{x}^{(k-1)}\)</span> 中得到<ul><li>具体来说， <span class="math inline">\(\tilde{\rho}^{(k)}\)</span>分解为静态和动态分量<ul><li>静态分量可学习，光谱通道共享</li><li>动态分量是从上一阶段中学习得到，作为通道注意力</li></ul></li></ul></li><li>基于上述分析和设计，公式修改为：<ul><li><span class="math inline">\(\begin{aligned} \mathbf{r}^{(k)}&amp;=\operatorname{DGDM}\left(\mathbf{x}^{(k-1)}, \mathbf{y}\right) \\&amp;=\mathbf{x}^{(k-1)}-\tilde{\boldsymbol{\rho}}^{(k)}\mathcal{H}_{\mathbf{\Phi}^{\top}}\left(\mathcal{H}_{\mathbf{\Phi}}\left(\mathbf{x}^{(k-1)}\right)-\mathbf{y}\right)\end{aligned}\)</span></li><li>with <span class="math inline">\(\quad\tilde{\boldsymbol{\rho}}^{(k)}=\mathcal{H}_{\mathrm{dy}\rho}\left(\mathbf{x}^{(k-1)}\right)=\boldsymbol{\rho}^{(k)}+\theta\boldsymbol{\Lambda}^{(\boldsymbol{k})}\)</span><ul><li>其中 <span class="math inline">\(\boldsymbol{\rho}^{(k)} \in\mathbb{R}^{1 \times C}\)</span> 和 <span class="math inline">\(\boldsymbol{\Lambda}^{(\boldsymbol{k})} \in\mathbb{R}^{1 \times C}\)</span> 表示静态和动态分量</li><li><span class="math inline">\(\theta\)</span> 是一个常数</li></ul></li></ul></li></ul></li><li><p><strong>层次特征交互模块（hierarchical feature interactionmodule, HFIM）</strong></p><figure><img src="/2022/03/01/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-0x21/1655282425808.png" alt="1655282425808"><figcaption aria-hidden="true">1655282425808</figcaption></figure><ul><li><span class="math inline">\(\mathbf{x}^{(k)}=\underset{\mathbf{x}}{\arg \min }\frac{1}{2}\left\|\mathbf{x}-\mathbf{r}^{(k)}\right\|_{2}^{2}+\lambda\psi(\mathbf{x})\)</span></li><li>HFIM 设计用于细化 DGDM 的重建结果 <span class="math inline">\(\mathbf r ^ k\)</span></li><li>应对的两个问题：<ul><li>梯度下降模块在图像域中起作用，而深度近端映射模块在特征域中起作用，因此在这两个模块之间传输光谱信息时，光谱信息会丢失</li><li>随着恢复阶段数量的增加，先前阶段中的有用信息无法传播到后续阶段</li></ul></li><li>从上图中可以看出，除了 DGDM 的输入之外，还有级联层次特征 <span class="math inline">\(\mathbf{H}^{k-1} =[\mathbf{h}^{k-1},\mathbf{h}^{k-2},\dots,\mathbf{h}^{0}]\)</span><ul><li><span class="math inline">\(\mathbf{h}^{i}\)</span> 表示第 <span class="math inline">\(i\)</span> 个阶段的隐藏状态</li><li>作者认为隐藏状态保留了在当前阶段从重构图像中提取的有益信息</li></ul></li><li>具体实现很简单：<ul><li><span class="math inline">\(\mathbf r ^ k\)</span> 通过卷积 Conv1转换到特征域，并与隐藏状态融合，再通过密集连接 Conv2</li><li>之后会输入到 <span class="math inline">\(\mathcal{H}_{\mathrm{EM}}\)</span>中来提取光谱和空间特征<ul><li><span class="math inline">\(\mathcal{H}_{EM}\)</span>包含四个编码块和解码块，以及中间的16个标准残差块</li></ul></li></ul></li><li><span class="math inline">\(\mathbf{F}_{\mathrm{EM}}=\mathcal{H}_{\mathrm{EM}}\left(\operatorname{Conv}_{2}\left(\left[\mathbf{H}^{(k-1)},\operatorname{Conv}_{1}\left(\mathbf{r}^{(k)}\right)\right]\right)\right)\)</span></li><li>对于获得增强的特征表示 <span class="math inline">\(\mathbf{F}_{\mathrm{EM}} \in \mathbb{R}^{H \timesW \times N}\)</span> 使用特征交互机制来生成重构的 HSI 数据 <span class="math inline">\(\mathbf{x}^{(k)} \in \mathbb{R}^{H \times W \timesC}\)</span> 和隐藏状态 <span class="math inline">\(\mathbf{h}^{(k)} \in\mathbb{R}^{H \times W \times N}\)</span><ul><li><figure><img src="/2022/03/01/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-0x21/1655293289245.png" alt="1655293289245"><figcaption aria-hidden="true">1655293289245</figcaption></figure></li><li><span class="math inline">\(\mathbf{x}^{(k)}=\operatorname{Conv}_{4}\left(\mathbf{F}_{\mathrm{EM}}\right)+\mathbf{x}^{(0)}\)</span></li><li><span class="math inline">\(\mathbf{h}^{(k)}=\operatorname{Conv}_{3}\left(\mathbf{F}_{\mathrm{EM}}\right)\otimes\operatorname{Sigmoid}\left(\operatorname{Conv}_{5}\left(\mathbf{x}^{(k)}\right)\right)+\mathbf{F}_{\mathrm{EM}}\)</span></li></ul></li><li>该阶段的 HSI 数据 <span class="math inline">\(\mathbf{x}^{(k)}\)</span> 和隐藏状态 <span class="math inline">\(\mathbf{h}^{(k)}\)</span> 将用于下一阶段的重建过程<ul><li><span class="math inline">\(\mathbf{x}^{(k)},\mathbf{h}^{(k)}=\operatorname{HFIM}\left(\mathbf{r}^{(k)},\mathbf{x}^{(0)}, \mathbf{H}^{(k-1)}\right)\)</span></li></ul></li></ul></li></ul><h5 id="实现细节">实现细节</h5><ul><li>本文中光谱通道 <span class="math inline">\(C\)</span> 取28，特征通道<span class="math inline">\(N\)</span> 取32</li><li>在使用掩码优化的采样子网中， <span class="math inline">\(\mu_b\)</span> 取0， <span class="math inline">\(\sigma_b\)</span> 取0.1</li><li>在恢复子网中，恢复的阶段数 <span class="math inline">\(K\)</span>为8，恢复子网等式中的 <span class="math inline">\(\theta\)</span>取0.5</li><li>所有可学习参数集合为 <span class="math inline">\(\Theta=\left\{\mathbf{M}, \mathbb{D}^{(k)},\mathbb{H}^{(k)}\right\}\)</span><ul><li><span class="math inline">\(\mathbf{M}\)</span>表示二进制掩码的，<span class="math inline">\(\mathbb{D}^{(k)}\)</span>表示 DGDM 的，<span class="math inline">\(\mathbb{H}^{(k)}\)</span> 表示HFIM 的</li></ul></li><li>作者最后使用了最终阶段和中间阶段的重建结果来计算损失函数<ul><li>对于给定训练数据 <span class="math inline">\(\{\mathbf{x}\}_{i=1}^{N_d}\)</span> ，损失函数定义为：</li><li><span class="math inline">\(\mathcal{L}_f=\frac{1}{N_d}\sum_{i=1}^{N_d} \| \mathbf{x}_{i}^{(K)}-\mathbf{x}_i\|_{2}^{2}\)</span></li><li><span class="math inline">\(\mathcal{L}_p = \frac{\beta}{N_d}\sum_{i=1}^{N_d} \| \mathbf{x}_{i}^{(K-1)}-\mathbf{x}_i \|_{2}^{2} + \|\mathbf{x}_{i}^{(K-2)}-\mathbf{x}_i \|_{2}^{2}\)</span></li><li><span class="math inline">\(\mathcal{L} =\mathcal{L}_f+\mathcal{L}_p\)</span><ul><li>其中 <span class="math inline">\(K\)</span>表示恢复阶段的数量，<span class="math inline">\(\mathcal{L}_p\)</span>表示中间过程的损失函数计算</li><li><span class="math inline">\(N_d\)</span> 表示样本数</li><li>平衡参数 <span class="math inline">\(\beta\)</span> 设置为0.5</li></ul></li></ul></li><li>4块 NVIDIA Tesla V100GPU（一搜发现7w一块，算力是几乎是2080两倍，好狠）</li><li>Adam优化器，100 epochs</li><li>learning rate 初始化 <span class="math inline">\(1 \times10^{-4}\)</span> ，每10个epoch衰减为原来的0.9</li></ul><h3 id="experiments">Experiments</h3><ul><li>模拟实验：公共HSI数据集Cave和KAIST（256×256×28）</li><li>真实场景：使用真实SCI系统捕获的5个空间尺寸为640×694的压缩测量值进行测试</li></ul><figure><img src="/2022/03/01/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-0x21/1655298152464.png" alt="1655298152464"><figcaption aria-hidden="true">1655298152464</figcaption></figure><figure><img src="/2022/03/01/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-0x21/1655298199321.png" alt="1655298199321"><figcaption aria-hidden="true">1655298199321</figcaption></figure><ul><li>为了客观地评估所提出的掩码优化策略的有效性，作者在不同类型的掩码上进行了对比<ul><li>下图给出了实验中四种不同掩码的中心区域，空间尺寸为64×64</li><li>当在固定掩码上进行训练时，会删除掩码优化策略</li><li>通过结果对比，作者认为优化后的二值掩模保留了完整的图像结构和足够的细节信息，以实现最佳采样</li></ul></li></ul><figure><img src="/2022/03/01/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-0x21/1655299073144.png" alt="1655299073144"><figcaption aria-hidden="true">1655299073144</figcaption></figure><ul><li>在真实数据测试中，所有方法都是使用固定掩码进行训练，并注入部分噪声</li></ul><figure><img src="/2022/03/01/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-0x21/1655299512690.png" alt="1655299512690"><figcaption aria-hidden="true">1655299512690</figcaption></figure><figure><img src="/2022/03/01/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-0x21/1655299544432.png" alt="1655299544432"><figcaption aria-hidden="true">1655299544432</figcaption></figure><h3 id="影响和限制">影响和限制</h3><ul><li>有助于光谱SCI的工业应用，并启发了其他图像逆问题中深度展开网络的设计</li><li>对于不同的成像系统和物理掩码，如果不进行再训练或微调，就无法获得令人满意的结果</li><li>基于学习的方法不可避免地会反映训练数据中的偏差</li></ul>]]></content>
    
    
    <categories>
      
      <category>study</category>
      
    </categories>
    
    
    <tags>
      
      <tag>组会分享</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>2022新年期盼</title>
    <link href="/2022/01/03/2022%E6%96%B0%E5%B9%B4%E6%9C%9F%E7%9B%BC/"/>
    <url>/2022/01/03/2022%E6%96%B0%E5%B9%B4%E6%9C%9F%E7%9B%BC/</url>
    
    <content type="html"><![CDATA[<p>时间总是过得很快，想起来要计划计划2022新的一年的时候，就已经过去两天了。回想2021年，感觉总是过得匆匆忙忙的，被deadline推着往前赶，也因为各种事情的乱入把自己的生活搞得一团糟，渐渐的吧，就开始刻意地麻痹自己，这样投入到工作里面，也就不会想太多，有活干活，少说多做的感觉。但是也是在这一年工作里面吧，收获了许多，自己也成长了许多（不过也是欢乐少了许多，冷漠了许多），也得到了一些荣誉和认可。</p><p>现在自己对2022年的期盼更希望是让自己能快乐一些，让自己静静心，找机会去做一些自己想了很久都没有去尝试的事情，去看一看自己未曾看过的风景，再去多走走路。毕竟从今年开始就算是要开始面临毕业和找工作一系列事情了，可能毕业的这么一次选择就决定了自己未来的归宿和发展。这样一想，2022年对自己来说可能注定要成为不平凡的一年，所以还是要谨慎地做出每一个决定吧，要有敬畏之心，也要敢于尝试，总之希望今年一起都能顺顺利利吧。</p><span id="more"></span><p>可能临时这么一想，难免会有遗漏或者没有想的比较恰当的方面吧，不过怎么说也都是年初的一次展望吧~</p><ul><li>多读书。看小说，读书都行吧，还是要多去看书，不然有时候真的会感觉自己被陷入在了繁杂的工作生活中，找不到自己的向往所在，过得迷迷糊糊的，很颓废。<ul><li>《大奉打更人》“今日无事，勾栏听曲~”</li></ul></li><li>多运动。强身健体，多去跑跑步、打羽毛球，找机会去尝试游泳？参加一些线上的跑步活动，这样也有动力。<ul><li>确实有段时间坚持跑步锻炼了，不过大部分时间还是过于颓废，自从入秋之后就更没有运动的想法了</li></ul></li><li>想攒钱。哎，好难，感觉自己有实话花钱总是大手大脚的，按理来说怎么会攒不下来钱呢？这个学期开始的时候就已经攒了一些，结果也都慢慢花完了，还不知道自己花在了哪里，一定要去攒钱了。<ul><li>说来可笑，不说也罢</li></ul></li><li>做好自己的工作。虽然感觉毕业越来越近，但是那也是明年2023年毕业，所以担任的班长、党支书和兼辅的工作还是要做好，不过等到了毕业年纪的时候，一些工作也就没必要非要开展了，保证一些必要工作做好就行了。<ul><li>做好自己的本职工作，既然做了就努力去做好吧，可能会有不足和问题，但问心无愧就好</li></ul></li><li>看电影。之前看论坛，说今年好像会上两个哆啦A梦的大电影，一定要去看。<ul><li>每年的一点期盼了，已看</li></ul></li><li>小论文。距离投稿已经块一个月的时间了，不过也一直没有返回来什么消息，不知道是好是坏，还是耐心等等吧，如果有修改意见就好好改，那份代码也要好好整理，然后放到github上。<ul><li>现在是2022年11月21日11:11:28，大修完投回去还没动静</li></ul></li><li>毕业论文。明年这个时候，应该已经正式开始写毕业论文了，所以希望开题等流程都能顺利吧，然后顺利写完毕业论文，等待毕业。</li><li>爬泰山。2022年还是想找机会去爬个泰山。<ul><li>因为疫情，现在出市都费劲，不过爬了崂山，嘻嘻</li></ul></li><li>LeetCode。怎么说呢，毕竟也是要尝试去找工作或者找实习，所以还是得多去看这样一些算法题，多做多练，肯定会有提升的。<ul><li>发现山东找工作基本不需要这个，，</li></ul></li><li>实习/准备。因为自己也没有想好考公考选调、考国家电网还是进互联网公司，所以对于实习也没有特别想好，但也是尽早准备的好，希望论文顺利，然后再好好规划。<ul><li>找到了中兴的实习，不过还是没去，山东找工作也不是很看实习这部分，有最好了</li></ul></li></ul><p>暂时想不到别的了，就先这样吧，其实能做到上面这些就很知足了，希望来年看这篇博客的时候，能够看到都是对号，噶油~</p>]]></content>
    
    
    <categories>
      
      <category>life</category>
      
    </categories>
    
    
    <tags>
      
      <tag>life</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>论文阅读-0x20</title>
    <link href="/2021/08/31/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-0x20/"/>
    <url>/2021/08/31/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-0x20/</url>
    
    <content type="html"><![CDATA[<p>论文的阅读笔记：</p><p>《 Contextual Transformer Networks for Visual Recognition 》, <a href="https://github.com/JDAI-CV/CoTNet">[code]</a>, CVPR 2021</p><p>《 Restormer: Efficient Transformer for High-Resolution ImageRestoration 》， <a href="https://github.com/swz30/Restormer">[code]</a>,</p><span id="more"></span><h2 id="contextual-transformer-networks-for-visual-recognition">ContextualTransformer Networks for Visual Recognition</h2><h3 id="abstract">Abstract</h3><ul><li>具有 self-attention 的 Transformer 在 NLP领域带来了技术性革命，并在最近激发了 Transformer风格的架构设计的出现，在众多计算机视觉任务中取得了有竞争力的结果</li><li>然而，大多数现有设计直接利用二维 feature map 上的 self-attention来获得基于每个空间位置处的成对孤立的 queries 和 keys的注意力矩阵，但是<strong>没有充分利用相邻 keys之间的丰富上下文信息</strong></li><li>在本文中，作者设计了一个全新的 Transformer-style 模块——ContextualTransformer (CoT) block<ul><li>充分利用输入的上下文信息来指导动态注意力矩阵的学习，从而增强了视觉表征能力</li><li>CoT 首先通过3×3卷积对输入 keys进行上下文编码，从而得到输入的<strong>静态上下文表示</strong></li><li>进一步通过两个连续的1×1卷积将编码的 keys 与输入 queries连接起来，以学习<strong>动态多头注意力矩阵</strong></li><li>将学习的注意力矩阵乘以输入values，以实现输入的<strong>动态上下文表示</strong></li><li>该模块将静态和动态上下文表示的融合结果最终作为输出</li></ul></li><li>CoT 可以很容易地替换ResNet架构中的每个3 ×3卷积，从而得到一个Transformer-style backbone——CoTNet</li><li>CNN<ul><li>CNN架构设计是基于离散卷积算子（3×3或5×5卷积），有效地施加空间局部性和平移等变性</li><li>有限的卷积感受野对全局/长程依赖性的建模产生了不利影响</li></ul></li><li>自然语言处理（NLP）领域见证了 Transformer在强大的语言建模架构中的崛起<ul><li>自注意机制是为了在序列建模中捕捉长期依赖性而设计的</li><li>以可伸缩的方式触发远程交互</li><li>自注意机制从NLP到CV的简单迁移是在图像中不同空间位置的特征向量上直接执行自注意</li></ul></li><li><strong>与其在整个特征图上使用全局自注意，还不如在局部块中使用自注意（3×3网格）</strong><ul><li>这种局部自注意力的设计有效地限制了网络消耗的参数和计算，因此可以<strong>完全取代整个深层架构中的卷积</strong></li></ul></li><li>通过将基于CNN的架构与 Transformer 模块相结合，来更好地完成 CV任务</li><li><img src="/2021/08/31/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-0x20/1.png"><ul><li>传统的自注意力<strong>仅利用孤立的 query-key对来度量注意力矩阵，但未充分利用 key 之间的丰富上下文</strong></li></ul></li><li><img src="/2021/08/31/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-0x20/2.png"><ul><li>作者将 keys之间的上下文挖掘和二维特征图上的自注意学习统一在一个体系结构中，从而避免了为上下文挖掘引入额外的分支</li><li>首先通过对3×3网格内的所有相邻 keys 执行3×3卷积，将 keys 表示上下文化<ul><li>这样得到的 keys特征视为输入的静态表示，<strong>反映了局部相邻位置的静态上下文信息</strong></li></ul></li><li>然后将 keys 和输入 query<strong>串联输入到两个连续的1×1卷积中，得到注意力矩阵</strong></li><li>进一步利用学习到的注意力矩阵聚合values，从而<strong>实现输入的动态上下文表示</strong></li><li>最后将静态和动态上下文表示相结合作为CoT块的最终输出<ul><li>作者的出发点是在输入 key 之间同时捕获上述两种空间上下文：通过 3×3卷积的静态上下文和基于上下文自注意力的动态上下文，以促进视觉表示学习</li></ul></li></ul></li></ul><h3 id="cot">CoT</h3><h4 id="multi-head-self-attention-in-vision-backbones">Multi-headSelf-attention in Vision Backbones</h4><ul><li>Local Relation Networks for Image Recognition CVPR 2019<ul><li>用局部关系层建立的网络——局部关系网络 the Local Relation Network(LR-NET)</li><li><figure><img src="https://img-blog.csdnimg.cn/2021040716064736.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxNjg0MjQ5,size_16,color_FFFFFF,t_70" alt="img"><figcaption aria-hidden="true">img</figcaption></figure></li><li>输入Input Feature，分别通过一个1×1大小的卷积核，得到Key Map和QueryMap，通道数为C/m</li><li>对于Query Map中一个像素，在KeyMap中提取出一个邻域k×k大小的区域，然后对应像素点相乘，得到AppearanceComposability</li><li>文中并没有给出如何构造2×k×k的Position。这里可以认为其中一层表示x坐标，另一层表示y坐标。然后将这个2×k×k的Position经过两个1×1大小的卷积层，得到GeometryPrior，其大小为C/m×k×k。中间使用ReLU激活函数</li><li>将Geometry Prior和AppearanceComposability相加，然后经过Softmax，得到该层的参数AggregationWeights，其大小为C/m×k×k。</li><li>对Input Feature提取出对应的一个k×k的区域，然后与AggregationWeights进行参数聚合，即每一个k×k大小进行加权平均操作，最终得到该点的结果，大小为C×k×k。</li><li>通过上述求参方法，得到输出Aggregation Feature，其大小为C×W×H。</li><li>最后将其经过一个1×1的卷积层，得到OutputFeature，其大小为C×W×H。</li></ul></li><li><img src="/2021/08/31/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-0x20/3.png"><ul><li>给定一个二维输入 <span class="math inline">\(H \times W \timesC\)</span> ，将 <span class="math inline">\(X\)</span> 转换为 queries<span class="math inline">\(Q=XW_q\)</span> , keys <span class="math inline">\(K=XW_k\)</span> , values <span class="math inline">\(V=XW_v\)</span><ul><li>其中 <span class="math inline">\(W_q, W_k, W_v\)</span> 是三个 1×1的嵌入卷积</li></ul></li><li>然后可以由 keys 和 queries 得到局部关系矩阵 <span class="math inline">\(R \in \mathbb{R}^{H \times W \times (k \times k\times C_h)}\)</span><ul><li><span class="math inline">\(R = K \circledast Q\)</span></li><li>其中 <span class="math inline">\(C_h\)</span>表示多头自注意力的头数， <span class="math inline">\(\circledast\)</span>表示局部矩阵乘法运算，用于度量空间中局部 <span class="math inline">\(k×k\)</span> 网格中每个 query 与相应 key之间的成对关系</li></ul></li><li>因此，在 <span class="math inline">\(R\)</span> 的第 <span class="math inline">\(i\)</span> 个空间位置的每个特征 <span class="math inline">\(R^{(i)}\)</span> 是一个 <span class="math inline">\(k \times k \times C_h\)</span>维向量，它由所有头部的局部 query-key 关系映射（尺寸为 <span class="math inline">\(k \times k\)</span> ）组成</li><li>局部关系矩阵 <span class="math inline">\(R\)</span> 进一步丰富了每个<span class="math inline">\(k \times k\)</span> 网格的位置信息：<ul><li><span class="math inline">\(\hat R = R + P \circledastQ\)</span></li><li><strong>其中 <span class="math inline">\(P \in \mathbb{R}^{k \timesk \times C_k}\)</span> 表示每个 <span class="math inline">\(k \timesk\)</span> 网格内的二维位置信息，并在所有 <span class="math inline">\(C_h\)</span> 头上共享</strong></li></ul></li><li>接下来，通过沿每个头部的通道维度对增强的空间感知局部关系矩阵 <span class="math inline">\(\hat R\)</span> 进行 Softmax操作来归一化，从而获得注意力矩阵 <span class="math inline">\(A =Softmax(\hat R)\)</span></li><li>将 <span class="math inline">\(A\)</span>的每个空间位置处的特征向量重塑为局部注意力矩阵（尺寸为 <span class="math inline">\(k \times k\)</span>）后，<strong>最终输出的特征图计算为每个 <span class="math inline">\(k\times k\)</span> 网格内 values 与局部注意矩阵的聚合</strong>：<ul><li><span class="math inline">\(Y = V \circledast A\)</span></li><li>每个头部的局部注意矩阵仅用于沿通道维度聚合 <span class="math inline">\(V\)</span> 的均匀分割特征映射，最终输出 <span class="math inline">\(Y\)</span> 是所有头部聚合特征映射的串联</li></ul></li></ul></li></ul><h4 id="contextual-transformer-block">Contextual Transformer Block</h4><ul><li><img src="/2021/08/31/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-0x20/4.png"></li><li>假设我们有相同的输入2D特征映射 <span class="math inline">\(X \in\mathbb{R}^{H \times W \times C}\)</span> ，对应的 key，query， value 为<span class="math inline">\(K = X, Q = K, V = XW_v\)</span></li><li>对于 key ，没有采用传统的 1×1 卷积，而是首先在空间上对 <span class="math inline">\(k \times k\)</span> 网格内的所有相邻 key 采用<span class="math inline">\(k \times k\)</span> 组卷积，以上下文化每个key 表示</li><li>所得到的上下文 key <span class="math inline">\(K^1 \in \mathbb{R}^{H\times W \times C}\)</span>自然地<strong>反映了局部相邻位置的静态上下文信息</strong>，并将 <span class="math inline">\(K^1\)</span> 作为输入 <span class="math inline">\(X\)</span> 的静态上下文表示</li><li>然后在 key <span class="math inline">\(K^1\)</span> 和 query <span class="math inline">\(Q\)</span>串联的条件下，通过两个连续的1×1卷积（带ReLU激活函数的 <span class="math inline">\(W_\theta\)</span> 和不带激活函数的 <span class="math inline">\(W_\delta\)</span> ）获得注意力矩阵<ul><li><span class="math inline">\(A=\left[K^{1}, Q\right] W_{\theta}W_{\delta}\)</span></li></ul></li><li>因此对于每个头部，<span class="math inline">\(A\)</span><strong>的每个空间位置的局部注意力矩阵是基于 query 特征和上下文的 key特征</strong>，而不是孤立的 query-key 对来学习的<ul><li>这种方式通过静态上下文 <span class="math inline">\(K^1\)</span>的额外指导增强了自注意力学习</li></ul></li><li>此外根据上下文注意力矩阵 A，通过聚合 V 来计算特征图 <span class="math inline">\(K^2\)</span><ul><li><span class="math inline">\(K^2 = V \circledast A\)</span></li><li>鉴于特征映射 <span class="math inline">\(K^2\)</span><strong>捕捉了输入之间的动态特征交互</strong>，因此将 <span class="math inline">\(K^2\)</span> 命名为输入的动态上下文表示</li></ul></li><li>最终CoT块（Y）的输出通过注意机制被计算为为静态上下文 <span class="math inline">\(K^ 1\)</span> 和动态上下文 <span class="math inline">\(K^2\)</span> 的融合</li></ul><h4 id="contextual-transformer-networks">Contextual TransformerNetworks</h4><ul><li>作者的CoT设计是一个统一的 self-attention块，它可以替代ConvNet中的标准卷积（直接用CoT块替换所有3×3卷积）</li><li>在本文中，作者将CoT块集成到现有最先进的ResNet体系结构中，例如ResNet和ResNeXt，且不会显著增加参数预算</li><li><img src="/2021/08/31/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-0x20/5.png"><ul><li>对于ResNet-50和CoTNet50：CoTNet50的参数量和FLOPs（ 每秒浮点运算次数）都略少于ResNet-50</li><li>而与ResNeXt-50相比，CoTNeXt-50的参数量稍多，但FLOPs类似</li></ul></li></ul><h4 id="connections-with-previous-vision-backbones">Connections withPrevious Vision Backbones</h4><ul><li>“蓝图”卷积<ul><li>Rethinking Depthwise Separable Convolutions: How Intra-KernelCorrelations Lead to Improved MobileNets——CVPR 2020</li><li><img src="/2021/08/31/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-0x20/6.png"></li><li>使用1×1 pointwise 卷积加上k×k depthwise卷积来近似传统卷积，旨在减少沿通道深度方向上的冗余</li><li>这种设计与transformer样式块（例如典型的self-attention和CoT块）有一些共同点<ul><li>这是因为 Transformer-style 块也利用 1×1 pointwise卷积将输入转换为值，并且以类似的depthwise 卷积方式执行具有 k×k局部注意力矩阵的后续聚合计算</li></ul></li><li>此外，对于每个头，transformer-style block中的聚合计算采用通道共享策略以高效实现，而不会出现任何显着的精度下降。这里使用的通道共享策略也可以解释为tied block convolution，它在相等的通道块上共享相同的过滤器</li><li><img src="/2021/08/31/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-0x20/7.png"></li></ul></li><li>Dynamic Region-Aware Convolution<ul><li><img src="/2021/08/31/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-0x20/8.png"></li><li>引入了一个滤波器生成器（由两个连续的1×1组成），用于学习不同空间位置区域特征的专用滤波器</li><li>因此，它与CoT块中的注意力矩阵生成器具有相似的思想，该生成器为每个空间位置实现动态局部注意力矩阵</li><li>然而该滤波器生成器模块是根据主输入特征图生成专用滤波器。相比之下，CoT注意力矩阵生成器充分利用了上下文key 和 query 之间复杂的特征交互来进行自注意力学习</li></ul></li><li>Bottleneck Transformer<ul><li><img src="/2021/08/31/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-0x20/9.png"></li><li>旨在通过用 transformer模块取代3×3卷积，用自我注意机制来增强ConvNet</li><li>采用了全局多头自注意力层 multi-headself-attention，这在计算上比CoT块中的局部自注意力更昂贵</li><li>因此，对于同一个ResNet主干网，BoT50仅用 transformer模块替换了最后三个3×3卷积，而本文的CoT块可以完全替换整个深层架构中的3×3卷积，从而利用输入之间丰富的上下文来加强自注意力学习</li></ul></li></ul><h3 id="code">Code</h3><p><img src="/2021/08/31/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-0x20/4.png"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">CotLayer</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, dim, kernel_size</span>):<br>        <span class="hljs-comment"># 调用 CotLayer(width, kernel_size=3)</span><br>        <span class="hljs-built_in">super</span>(CotLayer, self).__init__()<br><br>        self.dim = dim<br>        self.kernel_size = kernel_size<br><br>        self.key_embed = nn.Sequential(<br>            <span class="hljs-comment"># 分组卷积</span><br>            <span class="hljs-comment"># Torch.nn.Conv2d(in_channels,out_channels,kernel_size,</span><br>            <span class="hljs-comment"># stride,padding,dilation,groups,bias)</span><br>            nn.Conv2d(dim, dim, self.kernel_size, stride=<span class="hljs-number">1</span>, padding=self.kernel_size//<span class="hljs-number">2</span>, groups=<span class="hljs-number">4</span>, bias=<span class="hljs-literal">False</span>),<br>            nn.BatchNorm2d(dim),<br>            nn.ReLU(inplace=<span class="hljs-literal">True</span>)<br>        )<br><br>        <span class="hljs-comment"># 共享通道，感觉和缩放一样，，</span><br>        share_planes = <span class="hljs-number">8</span><br>        <span class="hljs-comment"># 缩放因子</span><br>        factor = <span class="hljs-number">2</span><br>        self.embed = nn.Sequential(<br>            <span class="hljs-comment"># w_\theta, 存在concat操作</span><br>            nn.Conv2d(<span class="hljs-number">2</span>*dim, dim//factor, <span class="hljs-number">1</span>, bias=<span class="hljs-literal">False</span>), <br>            nn.BatchNorm2d(dim//factor),<br>            nn.ReLU(inplace=<span class="hljs-literal">True</span>),<br>            <span class="hljs-comment"># w_\delta, 没有激活函数</span><br>            nn.Conv2d(dim//factor, <span class="hljs-built_in">pow</span>(kernel_size, <span class="hljs-number">2</span>) * dim // share_planes, kernel_size=<span class="hljs-number">1</span>),<br>            <span class="hljs-comment"># 将channel方向分group，然后每个group内做归一化</span><br>            nn.GroupNorm(num_groups=dim // share_planes, num_channels=<span class="hljs-built_in">pow</span>(kernel_size, <span class="hljs-number">2</span>) * dim // share_planes)<br>        )<br><br>        self.conv1x1 = nn.Sequential(<br>            nn.Conv2d(dim, dim, kernel_size=<span class="hljs-number">1</span>, stride=<span class="hljs-number">1</span>, padding=<span class="hljs-number">0</span>, dilation=<span class="hljs-number">1</span>, bias=<span class="hljs-literal">False</span>),<br>            nn.BatchNorm2d(dim)<br>        )<br><br>        self.local_conv = LocalConvolution(dim, dim, kernel_size=self.kernel_size, stride=<span class="hljs-number">1</span>, padding=(self.kernel_size - <span class="hljs-number">1</span>) // <span class="hljs-number">2</span>, dilation=<span class="hljs-number">1</span>)<br>        self.bn = nn.BatchNorm2d(dim)<br>        act = get_act_layer(<span class="hljs-string">&#x27;swish&#x27;</span>)<br>        self.act = act(inplace=<span class="hljs-literal">True</span>)<br><br>        reduction_factor = <span class="hljs-number">4</span><br>        self.radix = <span class="hljs-number">2</span><br>        <br>        attn_chs = <span class="hljs-built_in">max</span>(dim * self.radix // reduction_factor, <span class="hljs-number">32</span>)<br>        self.se = nn.Sequential(<br>            nn.Conv2d(dim, attn_chs, <span class="hljs-number">1</span>),<br>            nn.BatchNorm2d(attn_chs),<br>            nn.ReLU(inplace=<span class="hljs-literal">True</span>),<br>            nn.Conv2d(attn_chs, self.radix*dim, <span class="hljs-number">1</span>)<br>        )<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x</span>):<br>        <span class="hljs-comment"># 用一个3x3卷积得到 key, 静态上下文信息 K^1</span><br>        k = self.key_embed(x)<br>        <span class="hljs-comment"># concat =&gt; 2C x H x W</span><br>        qk = torch.cat([x, k], dim=<span class="hljs-number">1</span>)<br>        b, c, qk_hh, qk_ww = qk.size()<br>        <br><span class="hljs-comment"># 两个1x1卷积操作，得到注意力图</span><br>        w = self.embed(qk)<br>        <span class="hljs-comment"># 转换成一维向量，自动补齐，k x k，H，W</span><br>        w = w.view(b, <span class="hljs-number">1</span>, -<span class="hljs-number">1</span>, self.kernel_size*self.kernel_size, qk_hh, qk_ww)<br>        <br>        <span class="hljs-comment"># 计算 value</span><br>        x = self.conv1x1(x)<br>        <br>        <span class="hljs-comment"># 使用注意力图（每个key对应一个k x k注意力矩阵）计算 value，得到动态上下文信息 K^2</span><br>        x = self.local_conv(x, w)<br>        x = self.bn(x)<br>        x = self.act(x)<br><br>        <span class="hljs-comment"># 拼接上面得到的静态和动态上下文信息</span><br>        B, C, H, W = x.shape<br>        x = x.view(B, C, <span class="hljs-number">1</span>, H, W)<br>        k = k.view(B, C, <span class="hljs-number">1</span>, H, W)<br>        x = torch.cat([x, k], dim=<span class="hljs-number">2</span>)<br>        <br><span class="hljs-comment"># 使用一个SE注意模块进行融合</span><br>        x_gap = x.<span class="hljs-built_in">sum</span>(dim=<span class="hljs-number">2</span>) <span class="hljs-comment"># 求和合并(B, C, H, W)</span><br>        x_gap = x_gap.mean((<span class="hljs-number">2</span>, <span class="hljs-number">3</span>), keepdim=<span class="hljs-literal">True</span>) <span class="hljs-comment"># (B, C, 1, 1)</span><br>        x_attn = self.se(x_gap) <span class="hljs-comment"># (B, 2C, 1, 1)</span><br>        x_attn = x_attn.view(B, C, self.radix) <span class="hljs-comment"># (B, C, 2)</span><br>        x_attn = F.softmax(x_attn, dim=<span class="hljs-number">2</span>)<br>        <span class="hljs-comment"># (B, C, 2, H, W) * (B, C, 2, 1, 1) , (B, C, H, W)</span><br>        out = (x * x_attn.reshape((B, C, self.radix, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>))).<span class="hljs-built_in">sum</span>(dim=<span class="hljs-number">2</span>)<br>        <br>        <span class="hljs-keyword">return</span> out.contiguous()<br></code></pre></td></tr></table></figure><h3 id="experiments">Experiments</h3><ul><li>ImageNet Classification<ul><li><img src="/2021/08/31/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-0x20/10.png"></li></ul></li><li>Object Detection<ul><li><img src="/2021/08/31/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-0x20/11.png"></li></ul></li><li>Instance Segmentation<ul><li><img src="/2021/08/31/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-0x20/12.png"></li></ul></li><li>Ablation Study<ul><li><img src="/2021/08/31/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-0x20/13.png"></li><li><img src="/2021/08/31/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-0x20/14.png"><ul><li>上表对比了不同阶段替换CoT（√）以及SE注意力（⭐）等改进的性能影响</li><li>提升替换CoT模块的数量可以有效提升模型的性能，而参数量与FLOPs不会显著变化</li><li>相比SE-ResNetD-50，所提SE-CoTNetD-50取得了更佳的性能</li></ul></li></ul></li></ul><h2 id="restormer-efficient-transformer-for-high-resolution-image-restoration">Restormer:Efficient Transformer for High-Resolution Image Restoration</h2><h3 id="abstract-1">Abstract</h3><ul><li><p>在这项工作中，作者提出了一个高效的转换器模型——RestorationTransformer（Restormer），通过在构建模块（multi-head attention andfeed-forwardnetwork）中进行几个关键设计，使其能够捕获长距离像素交互的同时，仍然适用于大型图像</p></li><li><p>CNN</p><ul><li>卷积神经网络（CNN）在从大规模数据中学习广义图像先验知识方面表现良好，这些模型已被广泛应用于图像恢复和相关任务</li><li>CNN中的基本操作是“卷积”，它提供了 local connectivity 局部连通性和translation equivariance翻译等价性，这些特性为CNN带来了效率和通用性</li><li>两个主要问题<ul><li><strong>感受野有限</strong>，因此无法对长距离像素相关性进行建模</li><li>卷积滤波器<strong>在推理时具有静态权重</strong>，因此不能灵活地适应输入内容</li></ul></li><li>为了解决上述缺点，一种更强大、更动态的替代方法是<strong>自注意（SA）机制</strong>，该机制通过所有其他位置的加权和计算给定像素处的响应</li></ul></li><li><p>Transformers：Self-attention</p><ul><li>Transformer模型缓解了CNN的缺点（即，有限的感受野和对输入内容的不适应性）</li><li>SA在捕捉远距离像素交互方面非常有效，但其<strong>计算复杂度</strong>随着空间分辨率的增加而呈二次方增长，因此无法应用于高分辨率图像<ul><li><img src="/2021/08/31/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-0x20/15.jpg"></li></ul></li><li>将图像分解为一系列patch（局部窗口），并了解它们之间的相互关系。这些模型的显著特点是具有强大的能力，能够学习到图像patch 之间的长距离依赖关系以及对给定输入内容的适应性<ul><li></li></ul></li><li>减少计算量：<ul><li>在每个像素周围大小为8×8的<strong>小空间窗口上应用SA</strong></li><li>将输入图像分割为大小为48×48的非重叠patch，并<strong>独立计算每个patch上的SA</strong></li></ul></li><li>但这种限制 SA的空间范围与捕获真正的远程空间像素关系的目标自相矛盾，尤其是在高分辨率图像上</li></ul></li><li><p>本文作者提出了一种高效的图像恢复 transformer，它能够建模全局连通性，并且仍然适用于高分图像</p><ul><li>引入了一个 multi-Dconv head ‘transposed’ attention (MDTA) block来代替具有线性复杂性的普通多头SA<ul><li>将SA应用于特征维度而不是空间维度</li><li>MDTA不是显式地对成对像素交互进行建模，而是计算特征通道之间的交叉协方差，以从（关键点和查询投影）输入特征中获得注意图，以从（key and queryprojected）输入特征获得注意力图</li><li>MDTA块的一个重要特征是在特征协方差计算之前进行局部上下文混合。这是通过使用1×1卷积的跨通道上下文的像素级聚合和使用有效的深度级卷积的本地上下文的通道级聚合来实现的<ul><li>强调空间局部上下文，并在通道中引入卷积运算的互补优势</li><li>其次，它确保在计算基于协方差的注意图时隐式地建模像素之间的上下文化全局关系</li></ul></li></ul></li><li>feed-forward network (FN)<ul><li>由两个全连接层组成，层间具有非线性</li><li>本文作者使用门控机制重新制定常规FN的第一个线性变换层，以改善通过网络的信息流</li><li>该门控层被设计为两个线性投影层的元素乘积，其中之一被 GELU非线性激活</li><li>gated-Dconv FN (GDFN)也基于本地内容混合，类似于MDTA模块，以同样强调空间上下文</li><li>GDFN中的门控机制控制哪些互补特征应该向前流动，并允许网络层次结构中的后续层专门关注更精细的图像属性，从而获得高质量的输出</li></ul></li><li>除了上述体系结构的新颖之处外，作者还展示了Restormer渐进式学习策略的有效性<ul><li>在这个过程中，网络在早期的小patch和大batch上进行训练，在后期逐渐在大patch和小batch上进行训练</li><li>这种训练策略有助于Restormer从大型图像中学习上下文，并在测试时提供质量性能改进</li></ul></li></ul></li><li><p>三个主要贡献：</p><ul><li>提出Restormer，一种编码器-解码器转换器，用于高分辨率图像的多尺度局部-全局表示学习，而无需将其分解为局部窗口，从而利用远距离图像上下文</li><li>提出了一种 multi-Dconv head transposed attention (MDTA) module，该模块能够聚合局部和非局部像素交互，并且足够有效地处理高分辨率图像</li><li>一种新的 gated-Dconv feed-forward network(GDFN)，用于执行受控特征转换，抑制信息量较小的特征，只允许有用的信息进一步通过网络层次结构</li></ul></li><li><p>模型在多个图像恢复任务上实现了最优结果，包括图像去噪、单图像运动去模糊、散焦去模糊（单图像和双像素数据）和图像去噪（高斯灰度/颜色去噪和真实图像去噪）</p></li></ul><h3 id="method">Method</h3><figure><img src="/2021/08/31/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-0x20/1638960902013.png" alt="1638960902013"><figcaption aria-hidden="true">1638960902013</figcaption></figure><ul><li>主要目标是开发一个高效的 transformer模型，该模型可以处理用于恢复任务的高分辨率图像<ul><li>为了缓解计算瓶颈，引入了multi-head SA layer and a multi-scalehierarchical module 的关键设计<ul><li>与单尺度网络相比，多尺度分层模块的计算需求更少</li></ul></li></ul></li><li>输入退化图像 <span class="math inline">\(\mathbf{I} \in\mathbb{R}^{H \times W \times 3}\)</span> ，Restormer首先卷积得到低级特征嵌入 <span class="math inline">\(\mathbf{F_0} \in\mathbb{R}^{H \times W \times C}\)</span></li><li>之后这些浅层特征，通过4个层对称的 encoder-decoder 来转化为深层特征<span class="math inline">\(\mathbf{F_d} \in \mathbb{R}^{H \times W\times 2C}\)</span><ul><li>从高分辨率输入开始，编码器分层减少空间大小，同时扩展通道数</li><li>解码器采用低分辨率潜在特征 <span class="math inline">\(\mathbf{F}_l\in \mathbb{R}^{H \times W \times 2C}\)</span>作为输入，并逐层恢复高分辨率表示</li><li>对于特征上下采样，作者分别应用了 pixel-unshuffle and pixel-shuffle操作<ul><li><a href="https://pytorch.org/docs/stable/generated/torch.nn.PixelShuffle.html#torch.nn.PixelShuffle"><code>PixelShuffle</code></a>: Rearranges elements in a tensor of shape <span class="math inline">\((*, C \times r^2, H, W)\)</span> to a tensor ofshape <span class="math inline">\((*, C, H \times r, W \timesr)\)</span> , where r is an upscale factor.</li><li><a href="https://pytorch.org/docs/stable/generated/torch.nn.PixelUnshuffle.html"><code>PixelUnshuffle</code></a>: Reverses the <a href="https://pytorch.org/docs/stable/generated/torch.nn.PixelShuffle.html#torch.nn.PixelShuffle"><code>PixelShuffle</code></a>operation by rearranging elements in a tensor of shape <span class="math inline">\((*, C, H \times r, W \times r)\)</span> to atensor of shape <span class="math inline">\((*, C \times r^2, H,W)\)</span> , where r is a downscale factor.</li></ul></li><li>通过跳跃连接来拼接编码器和解码器输出，拼接后使用 1x1卷积来减少通道数（减半）</li><li>其实感觉就是个U-Net结构</li></ul></li></ul><h4 id="multi-dconv-head-transposed-attentionmdta">Multi-Dconv HeadTransposed Attention(MDTA)</h4><ul><li><p>Transformers 中的主要计算开销来自 <a href="https://zhuanlan.zhihu.com/p/365386753"><code>SA</code></a></p><ul><li>SA中的时间和内存复杂度主要是 key-query对的计算会随着空间分辨率增长呈二次方增长，即对于 W×H像素的图像，复杂度为 <span class="math inline">\(O(W^2H^2)\)</span></li><li>因此，在高分辨率图像上使用SA是不可行的</li></ul><figure><img src="/2021/08/31/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-0x20/1638963023390.png" alt="1638963023390"><figcaption aria-hidden="true">1638963023390</figcaption></figure></li><li><p>为此，作者提出了MDTA，其跨通道应用SA，而不是空间维度</p><ul><li><strong>计算通道间的交叉协方差</strong>，从而生成隐式编码全局上下文的attention map</li><li>作为MDTA的另一个重要组成部分，在计算特征协方差生成全局 attention map之前，引入 depth-wise 卷积来强调上下文信息</li></ul></li><li><p>对于输入 <span class="math inline">\(\mathbf{X} \in\mathbb{R}^{\hat H \times \hat W \times \hat C}\)</span> ，归一化得到<span class="math inline">\(\mathbf{Y} \in \mathbb{R}^{\hat H \times\hat W \times \hat C}\)</span> ，然后生成 $query (Q), key (K), value (V)$</p><ul><li><a href="https://zhuanlan.zhihu.com/p/80041030">应用1×1 point-wise卷积来聚合像素级跨通道上下文，然后应用 3×3 depth-wise卷积来编码通道级空间上下文</a><ul><li><span class="math inline">\(\mathbf{Q}=W_p^Q W_d^Q \mathbf{Y},\mathbf{K}=W_p^K W_d^K \mathbf{Y}, \mathbf{V}=W_p^V W_d^V\mathbf{Y}\)</span></li><li>其中 <span class="math inline">\(W_p^{(\sdot)}\)</span> 是 1x1point-wise 卷积， <span class="math inline">\(W_d^{(\sdot)}\)</span> 是3x3 depth-wise 卷积</li></ul></li><li>之后对 query 和 key 做一下reshape操作，来 dot-product 计算生成transposed-attention map <span class="math inline">\(\mathbf{A} \in\mathbb{R}^{\hat C \times \hat C}\)</span><ul><li>而不是传统方法生成的注意力图 <span class="math inline">\(\mathbb{R}^{\hat H \hat W \times \hat H \hatW}\)</span></li></ul></li></ul></li><li><p><span class="math inline">\(\hat{\mathbf{X}}=W_{p}\)</span>Attention <span class="math inline">\((\hat{\mathbf{Q}},\hat{\mathbf{K}}, \hat{\mathbf{V}})+\mathbf{X}\)</span> Attention <span class="math inline">\((\hat{\mathbf{Q}}, \hat{\mathbf{K}},\hat{\mathbf{V}})=\hat{\mathbf{V}} \cdot\operatorname{Softmax}(\hat{\mathbf{K}} \cdot \hat{\mathbf{Q}} /\alpha)\)</span></p><ul><li>其中， <span class="math inline">\(\hat{\mathbf{Q}} \in\mathbb{R}^{\hat H \hat W \times \hat C}, \hat{\mathbf{K}} \in\mathbb{R}^{\hat C \times \hat H \hat W}, \hat{\mathbf{V} \in\mathbb{R}^{\hat H \hat W \times \hat C}}\)</span> 由原始尺度 <span class="math inline">\(\mathbb{R}^{\hat H \times \hat W \times \hatC}\)</span> reshape得到</li><li><span class="math inline">\(\alpha\)</span>是一个可学习的缩放参数，用来控制 <span class="math inline">\(\hat{\mathbf{Q}}, \hat{\mathbf{K}}\)</span>的点积大小</li></ul></li><li><p>与传统的SA一样，也使用了多头方法，分组并行就行进行学习</p></li></ul><h4 id="gated-dconv-feed-forward-networkgdfn">Gated-Dconv Feed-ForwardNetwork(GDFN)</h4><figure><img src="/2021/08/31/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-0x20/1638966876739.png" alt="1638966876739"><figcaption aria-hidden="true">1638966876739</figcaption></figure><ul><li>对于 transformer 特征，feed-forward network (FN)分别对每个像素位置进行相同的操作<ul><li>它使用两个1×1卷积，一个用于扩展特征通道（膨胀比 <span class="math inline">\(\gamma\)</span>），另一个用于将通道缩减回原始输入维度</li></ul></li><li>在本文中，作者进行了两个改进：<ul><li>gating mechanism——门控机制<ul><li>被表示为线性变换层的两条平行路径的元素级乘积，其中一条通过 <a href="https://zhuanlan.zhihu.com/p/349492378">GELU非线性激活</a></li><li><figure><img src="/2021/08/31/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-0x20/16.jpg" alt="1638966876739"><figcaption aria-hidden="true">1638966876739</figcaption></figure></li></ul></li><li>depth-wise convolutions<ul><li>在这里依然使用了 depth-wiseconvolution，以对空间相邻像素位置的信息进行编码，这有助于学习局部图像结构以进行有效恢复</li></ul></li></ul></li><li>对于输入 <span class="math inline">\(\mathbf{X} \in \mathbb{R}^{\hatH \times \hat W \times \hat C}\)</span> ，GDFN公式化：<ul><li><span class="math inline">\(\begin{aligned} \hat{\mathbf{X}}&amp;=W_{p}^{0} \text { Gating }(\mathbf{X})+\mathbf{X} \\ \text {Gating }(\mathbf{X}) &amp;=\phi\left(W_{d}^{1}W_{p}^{1}(\mathrm{LN}(\mathbf{X}))\right) \odot W_{d}^{2}W_{p}^{2}(\mathrm{LN}(\mathbf{X})) \end{aligned}\)</span><ul><li>其中 <span class="math inline">\(\odot\)</span> 表示元素相乘，<span class="math inline">\(\phi\)</span> 表示 GELU 非线性激活， LN是归一化层</li></ul></li></ul></li><li>GDFN控制各层中的通道中的信息流，从而使得每层都专注于与其他层之间互补的精细细节。既与MDTA相比，GDFN更专注于使用上下文信息丰富特性<ul><li>由于GDFN比常规的FN执行更多操作，所以减低的膨胀比 <span class="math inline">\(\gamma\)</span> ，以便具有相似的参数和计算量</li></ul></li></ul><h4 id="progressive-learning">Progressive Learning</h4><ul><li>基于CNN的恢复模型通常在固定大小的图像 patch 上进行训练<ul><li>然而，在裁剪的小 patch 上训练 transformer可能不会对全局图像统计进行编码，从而在测试时在全分辨率图像上得到次优性能</li></ul></li><li>为此，作者执行渐进式学习，其中网络在早期阶段在较小的图像 patch上进行训练，在后期训练阶段在逐渐增大的图像 patch 上进行训练</li><li>通过渐进学习在混合大小的 patch上训练的模型在测试时显示出更好的性能，其中图像可以具有不同的分辨率，这也是图像恢复中的常见情况</li><li>随着 patch 大小的增加，也随之减少了 batch 大小，以保持与固定 patch训练相同的每个优化步骤的时间</li></ul><h3 id="experiments-and-analysis">Experiments and Analysis</h3><ul><li>作者在四个图像处理任务的基准数据集和实验设置上进行了实验评估<ul><li>图像去雨</li><li>单图像运动去模糊</li><li>散焦去模糊（在单图像和双像素数据上）</li><li>图像去噪（在合成和真实数据上）</li></ul></li><li>从第一层到第四层，Transformer blocks数量设置为[4，6，6，8]，MDTA中的注意头为[1，2，4，8]，通道的数量为[48，96，192，384]</li><li>patch size 和 batch size <span class="math inline">\([(128^2,64),(160^2,40), (192^2,32), (256^2,16), (320^2,8), (384^2,8)]\)</span></li></ul><figure><img src="/2021/08/31/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-0x20/1638972160218.png" alt="1638972160218"><figcaption aria-hidden="true">1638972160218</figcaption></figure><figure><img src="/2021/08/31/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-0x20/1638972235377.png" alt="1638972235377"><figcaption aria-hidden="true">1638972235377</figcaption></figure><figure><img src="/2021/08/31/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-0x20/1638972255581.png" alt="1638972255581"><figcaption aria-hidden="true">1638972255581</figcaption></figure><figure><img src="/2021/08/31/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-0x20/1639037957648.png" alt="1639037957648"><figcaption aria-hidden="true">1639037957648</figcaption></figure><h4 id="ablation-studies">Ablation Studies</h4><figure><img src="/2021/08/31/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-0x20/1638972409807.png" alt="1638972409807"><figcaption aria-hidden="true">1638972409807</figcaption></figure><figure><img src="/2021/08/31/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-0x20/1638972547853.png" alt="1638972547853"><figcaption aria-hidden="true">1638972547853</figcaption></figure><ul><li>表8中，展示了第一层的设计选择：在 concat 操作之后，是否使用 1x1卷积（将通道数减半）<ul><li>有 1x1 卷积；无 1x1 卷积，直接concat；在 cancat 之后加一个transformer block</li></ul></li><li>表10中，展示了选择更深还是更浅的网络的选择<ul><li>通过调整 transformer block 来保持包含的计算量和参数量</li><li>深窄模型比宽浅模型执行得更准确；然而，由于并行化，更广泛的模型运行得更快。在本文中使用了深窄恢复器</li></ul></li></ul>]]></content>
    
    
    <categories>
      
      <category>study</category>
      
    </categories>
    
    
    <tags>
      
      <tag>组会分享</tag>
      
      <tag>即插即用模块</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>论文阅读-0x19</title>
    <link href="/2021/06/05/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-0x19/"/>
    <url>/2021/06/05/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-0x19/</url>
    
    <content type="html"><![CDATA[<p>论文的阅读笔记：</p><p>《 End-to-End Object Detection with Fully Convolutional Network 》,<a href="https://github.com/Megvii-BaseDetection/DeFCN">[code]</a>, CVPR2021</p><p>《 Removing Diffraction Image Artifacts in Under-Display Camera viaDynamic Skip Connection Networks 》, <a href="https://github.com/jnjaby/DISCNet">[code]</a>, CVPR 2021</p><span id="more"></span><h2 id="end-to-end-object-detection-with-fully-convolutional-network">End-to-EndObject Detection with Fully Convolutional Network</h2><h3 id="abstract">Abstract</h3><ul><li><p>基于全卷积网络的主流目标检测器已经取得了令人瞩目的性能。而其中大多数仍然需要<strong>手工设计的非最大值抑制(NMS)</strong>后期处理，这阻碍了端到端训练</p><ul><li><blockquote><p>NMS思想，步骤如下： （1）按照分类概率排序，概率最高的框作为候选框（2）其它所有与候选框的IOU高于一个阈值（这是人工指定的超参）的框其概率被置为0（3）然后在剩余的框里寻找概率第二大的框，其它所有与第二大的框的IOU高于一个阈值的框其概率被置为0（4）依次类推（5）最终所有的框相互之间的IOU都是小于超参阈值的，或者概率被置为0了（6）剩下的所有概率非0的框就是最终的检测框</p></blockquote></li></ul></li><li><p>本文作者对丢弃NMS的情况进行了分析，结果表明一个适当的标签分配起着关键性作用</p></li><li><p>对于全卷积检测器，作者提出了一个<strong>Prediction-awareOneTo-One(POTO)的标签分配用于分类</strong>，从而实现端到端检测，性能可以和NMS匹配</p></li><li><p>此外，提出了一种简单的<strong>3D Max Filtering(3DMF)，利用多尺度特征，提高卷积在局部区域的可鉴别性</strong></p></li><li><blockquote><p>目标检测是计算机视觉中的一个基本课题，它为每幅图像预测一组带有预定义类别标签的边界框</p><ul><li>大多数主流检测器利用了一些手工设计，如基于锚定anchor-based的标签赋值和非最大抑制(NMS)</li><li>最近，有相当多的方法被提出，通过使用<strong>距离感知和基于分布的标签分配</strong>来消除预定义的锚框集合</li><li>虽然取得了显著的进步和卓越的表现，但仍然面临着抛弃NMS后期处理的挑战，虽然很多方法被提出用来改进重复删除这一问题，不过仍然没有提供有效的端到端训练策略</li><li>同时，许多基于递归神经网络的方法被引入使用自回归解码器来预测每个实例的边界框</li><li>这些方法为边界框的预测提供了自然的顺序建模。但是，它们只在一些小的数据集上进行评估而且没有使用最新的检测器，而且迭代的方式使推理过程效率低下</li><li>DETR引入了一种基于二部匹配的训练策略和带有并行解码器的transformers来实现端到端检测。它实现了与许多最先进的探测器竞争的性能。然而DETR目前的训练时间要长得多，在小物体上的表现相对较差</li></ul></blockquote></li><li><p><img src="/2021/06/05/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-0x19/1.png"></p></li><li><p>大部分全卷积网络采用一对多的标签分配规则，即为一个ground-truth实例分配多个预测作为前景样本（），然后采用NMS后期处理（上图虚线部分）</p><ul><li>通过这种方法，可以提供足够的前景样本，以获得<strong>强大和鲁棒的特征表示</strong></li><li>但大量的前景样本会导致<strong>单个实例的预测框重复</strong>，不利于端到端检测</li><li>为了证明这一点，作者首先对不同的现有手工设计标签分配进行实证比较，然后发现一对一标签分配在后处理重复去除中起着至关重要的作用</li><li>不过手工设计的一对一作业仍然有一个缺点：固定的分配可能会导致歧义问题，降低特征的可辨别性，因为<strong>预定义的实例区域可能不是训练的最佳选择</strong></li></ul></li><li><p>为了解决这一问题，作者提出了一种预测感知的一对一(POTO)标签分配方法，该方法<strong>根据分类质量和回归质量同时动态分配前景样本</strong></p></li><li><p>对于基于FPN的现代检测器，大量的实验表明，重复的边界盒主要来自<strong>相邻尺度上最可信预测的邻近区域</strong>，为此作者设计了一个<strong>三维最大滤波(3dmaxFiltering, 3DMF)</strong>，它可以作为一个可微模块嵌入到FPN头部</p><ul><li>该模块通过使用一个简单的跨相邻尺度的三维最大滤波算子来提高卷积局部区域的可判别性</li></ul></li><li><p>现在单阶段或者双阶段的检测器都严重依赖于锚定或基于锚定的方案，在这些检测器中，<strong>锚框由预定义的滑动窗口组成，这些滑动窗口被分配为具有边界框偏移量的前景或背景样本</strong></p></li><li><p>由于NMS是一种启发式方法，并<strong>对所有实例采用恒定的阈值</strong>，因此需要谨慎地进行调优，而且可能不够健壮，特别是在拥挤的场景中</p></li><li><p>基于无锚框架，本文提出了一种预测感知的一对一分类分配规则，以抛弃不可训练的NMS</p></li><li><p>看到这里，感觉作者主要就是针对NMS</p><ul><li>首先，因为这是个端到端网络，之前的方法都是输出多个预测作为前景样本，然后使用NMS进行后期处理，但作者发现<strong>一对一标签分配更好</strong>，而且在去除重复这方面有着重要作用，但是直接使用一对一作业的话，可能出现歧义，不是最佳的训练选择====为此，作者提出了POTO，<strong>根据分类质量和回归质量同时动态分配前景样本</strong>，大概理解起来好像是通过学习来对这个前景样本进行一定处理，然后再让网络去学习</li><li>同时也是因为这些前景样本都是人工选择，而且NMS也存在一定问题，作者认为目前的很多方法说到底都没有实现基于完全卷积网络的端到端目标检测，不然就是存在网络很大，计算昂贵等各种问题</li><li>在这就是那个3DMF，也是为了去除重复，在多尺度之间进行一个最大滤波，感觉就是直接处理掉一些不重要信息，从而减少最后面出现的重复框，也就是作者说的利用多尺度特征，提高局部区域卷积的可分辨性</li><li>还是再看看后面的具体实现部分吧</li></ul></li></ul><h3 id="methodology">Methodology</h3><h4 id="analysis-on-label-assignment">Analysis on Label Assignment</h4><ul><li>为了揭示标签分配对端到端目标检测的影响，作者在COCO数据集上构建了几个常规标签分配的消融研究，结果显示了<strong>一对多分配在特征表示上的优越性</strong>，以及<strong>一对一分配在抛弃NMS方面的潜力</strong></li></ul><p><img src="/2021/06/05/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-0x19/2.png"></p><ul><li><strong>One-to-many Label Assignment</strong><ul><li>由于NMS后期处理在密集的预测框架中被广泛采用，一对多标签分配成为一种传统的训练目标分配方式</li><li>充分的前景样本导致一个<strong>强大和鲁棒的特征表示</strong><ul><li>然而当丢弃NMS时，由于一对多的标签赋值的冗余前台样本，重复的误报预测可能会导致性能的显著下降，如上表FCOS再基线上的mAP下降了28.4%</li></ul></li><li>另外，上表中报告的mAR表示了前100分的预测的召回率<ul><li>在没有NMS的情况下，<strong>一对多的分配规则会导致大量重复的高分预测</strong>，从而降低召回率</li><li>因此，仅依靠一对多的分配很难实现具有竞争力的端到端检测</li></ul></li></ul></li><li><strong>Hand-designed One-to-one Label Assignment</strong><ul><li>作者在这里评估了两个一对一标签分配规则（Anchor规则和Center规则），解释了丢弃NMS后的潜在连接<ul><li>锚定规则基于视网膜网络RetinaNet，每个ground-truth实例只给分配具有最大IoU的锚点</li><li>中心规则基于FCOS，每个groundtruth实例只分配到预定义特征层中离实例中心最近的像素</li></ul></li><li>从上表也可以看出，这种一对一标签分配大大减少了有无NMS的差距，也能达到一个较好的性能</li><li>不过在应用一对一标签分配时，有无NMS的检测器之间的<strong>性能差距</strong>仍然不可忽视。其次，由于对每个实例的监督较少，一对一标签分配的<strong>性能仍然低于FCOS基线</strong></li></ul></li></ul><h4 id="methods">Methods</h4><p><img src="/2021/06/05/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-0x19/4.png"></p><ul><li><p>本文中，作者为了能够去竞争端到端目标检测性能，提出了一种<strong>混合标签分配</strong>和一种新的<strong>3D最大滤波</strong>(3DMF)</p></li><li><p>混合标签分配由<strong>预测感知的一对一(POTO)标签分配</strong>和<strong>改进的一对多标签分配(辅助损失)</strong>组成</p></li><li><p><strong>Prediction-aware One-to-one Label Assignment</strong></p><ul><li><p>手工设计的一对一标签分配遵循一个固定的规则。然而，对于复杂场景中的各种实例来说，这个规则可能不是最优的，例如，中心规则对于一个偏心eccentric物体</p></li><li><p>因此，如果<strong>分配过程被迫将次优预测分配为唯一的前景样本，网络收敛的难度将显著增加</strong>，导致更多的假阳性预测</p></li><li><p>假设 <span class="math inline">\(\Psi\)</span>为所有预测的索引集， <span class="math inline">\(G\)</span> 和 <span class="math inline">\(N\)</span>分别对应于ground-truth实例和预测的数量，其中在密集预测探测器中 <span class="math inline">\(G \ll N\)</span> ， <span class="math inline">\(\hat{\pi} \in \Pi_{G}^{N}\)</span>表示N个预测的G排列方式</p></li><li><p>POTO的目的就是想要生成一个合适的排列预测 <span class="math inline">\(\hat{\pi}\)</span> 用来作为前景样本</p></li><li><p>其中训练损失如下，包含一个前景损失 <span class="math inline">\(\mathcal{L}_{f g}\)</span> 和背景损失 <span class="math inline">\(\mathcal{L}_{b g}\)</span></p><ul><li><span class="math inline">\(\mathcal{L}=\sum_{i}^{G} \mathcal{L}_{fg}\left(\hat{p}_{\hat{\pi}(i)}, \hat{b}_{\hat{\pi}(i)} \mid c_{i},b_{i}\right)+\sum_{j \in \Psi \backslash \mathcal{R}(\hat{\pi})}\mathcal{L}_{b g}\left(\hat{p}_{j}\right)\)</span></li><li>其中， <span class="math inline">\(\mathcal{R}(\hat{\pi})\)</span>表示指定前景样本对应的索引集</li><li>对于第 <span class="math inline">\(i\)</span> 个ground-truth，<span class="math inline">\(c_i,b_i\)</span>分别表示其类别标签和边界框坐标</li><li>对应的第 <span class="math inline">\(\hat{\pi}(i)\)</span> 个预测，<span class="math inline">\(\hat{p}_{\hat{\pi}(i)},\hat{b}_{\hat{\pi}(i)}\)</span>对应其预测的分类分数和预测的方框坐标</li></ul></li><li><p>为了实现端到端检测，就需要找到一个合适的标签分配 <span class="math inline">\(\hat{\pi}\)</span>，之前的工作就是将其看作一个二部图匹配问题，利用前景损失作为匹配代价，可以用匈牙利算法快速求解</p><ul><li><blockquote><p>借用SSD来理解二部图匹配策略与匈牙利算法：</p><p>1）建图。对于某个预测框，遍历所有的gt_bbox，如果它们的交集大于0，那么就用一条边把gt_bbox与预测框连接起来；这样对所有的预测框都进行同样操作。如果把所有gt_bbox放入集合A，所有预测框放入集合B，那么这一步就是建立A与B之间的二部图，二部图中边的权值为预测框与gt_bbox的iou分数。（这里的权值就是上文的匹配代价）</p><p>2）匹配。匹配分为2个阶段：</p><p>第一个阶段：找出二部图中的边权值最大的边，并该边对应的gt_bbox与预测框从顶点集中删除；反复进行这个过程，直到所有的gt_bbox都找到匹配的预测框；</p><p>第二个阶段：如果匹配的类型是BIPARTITE，那么匹配过程已结束；如果匹配类型是PER_PREDICTION，表示对于每个预测框，都要找到一个gt_bbox与之匹配，那么对于第一阶段未匹配上的预测框，从gt_bboxes集合中找到与它所连边中权值最大的gt_bbox作为它的匹配。</p><p>第一个阶段的匹配保证每个gt_bbox都至少有一个预测框与之匹配，这是因为预测框足够多，必定有许多预测框与gt_bbox建立边连接，且每个预测框都有0个或1个gt_bbox与之匹配。第二个阶段的匹配如果采用PER_PREDICTION方法，那么每个预测框都有且只有1个gt_bbox与之匹配了，但是gt_bbox可能匹配上多个预测框。</p></blockquote></li><li><p><span class="math inline">\(\hat{\pi}=\underset{\pi \in\Pi_{G}^{N}}{\arg \min } \sum_{i}^{G} \mathcal{L}_{fg}\left(\hat{p}_{\hat{\pi}(i)}, \hat{b}_{\hat{\pi}(i)} \mid c_{i},b_{i}\right)\)</span></p></li><li><p>但是，前景损失通常需要额外的权重来缓解优化问题，如训练样本不平衡、多任务联合训练等，这种特性使得训练损失不是匹配代价的最优选择</p></li></ul></li><li><p>为此作者提出了一个简单有效的POTO来寻找一个更好地分配方式：</p><ul><li><img src="/2021/06/05/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-0x19/5.png"></li><li><span class="math inline">\(\begin{aligned} \hat{\pi}&amp;=\underset{\pi \in \Pi_{G}^{N}}{\arg \max } \sum_{i}^{G} Q_{i,\pi(i)}, \\ \text { where } Q_{i, \pi(i)}=&amp;\underbrace{\mathbb{1}\left[\pi(i) \in \Omega_{i}\right]}_{\text{spatial prior }} \cdot\underbrace{\left(\hat{p}_{\pi(i)}\left(c_{i}\right)\right)^{1-\alpha}}_{\text{classification }} . \\ &amp;\underbrace{\left(\operatorname{IoU}\left(b_{i},\hat{b}_{\pi(i)}\right)\right)^{\alpha}}_{\text {regression }} .\end{aligned}\)</span></li><li>这里的 <span class="math inline">\(Q_{i, \pi(i)} \in [0,1]\)</span>表示第 <span class="math inline">\(i\)</span> 个ground-truth和 <span class="math inline">\(\hat{\pi}(i)\)</span>个预测之间的一个<strong>匹配质量</strong>（同时考虑了空间先验、分类的置信度和回归质量）</li><li><span class="math inline">\(\Omega_i\)</span> 表示第 <span class="math inline">\(i\)</span>个ground-truth的候选预测集，也就是空间先验；</li><li>FCOS采用中心采样策略，只将ground-truth<strong>实例中心部分的预测作为前景样本</strong>，作者也将其应用在POTO中，以获得更高的性能，但并不需要抛弃NMS</li><li>在上式中，使用分类得分 <span class="math inline">\(\hat{p}_{\pi(i)}\left(c_{i}\right)\)</span>和回归质量 <span class="math inline">\(\operatorname{IoU}(b_{i},\hat{b}_{\pi(i)})\)</span> 的加权几何平均值来定义质量</li><li>超参数 <span class="math inline">\(\alpha \in [0,1]\)</span>调整分类与回归之间的比值，其中默认采用 <span class="math inline">\(\alpha =0.8\)</span></li></ul></li></ul></li><li><p><strong>3D Max Filtering</strong></p><ul><li>作者发现重复预测主要来自于<strong>最可信预测的邻近空间区域</strong>。因此提出了一个新的模块，称为3DMax Filtering (3DMF)，以<strong>抑制重复预测</strong></li><li>卷积是一种具有平移等方差的线性运算，<strong>对不同位置的相似模式产生相似的输出</strong>。但是，由于<strong>相同实例的不同预测</strong>对于密集预测检测器来说通常<strong>具有相似的特征</strong>，所以这个属性对于消除重复预测有很大的障碍</li><li>Max滤波器是一种基于秩的非线性滤波器，用于补偿局部区域内卷积的判别能力，也可以被作为一个新的后期处理步骤来取代非最大抑制<ul><li>它展示了一些执行重复去除的潜力，但非可训练的方式妨碍了有效性和端到端的训练。同时，最大滤波器只考虑了单尺度特征，不适用于广泛应用的基于FPN的检测器</li></ul></li><li><img src="/2021/06/05/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-0x19/3.png"></li><li>因此作者在这里就是扩展成多尺度的版本——3D MaxFiltering，在FPN的每个尺度上进行特征转换， 在featuremap的每个通道上分别采用3D Max滤波<ul><li><span class="math inline">\(\tilde{x}^{s}=\left\{\tilde{x}^{s,k}:=\operatorname{Bilinear}_{x^{s}}\left(x^{k}\right) \mid \forall k\in\left[s-\frac{\tau}{2}, s+\frac{\tau}{2}\right]\right\}\)</span></li><li>给定FPN的尺度 <span class="math inline">\(s\)</span>中的一个输入特征 <span class="math inline">\(x^s\)</span>，首先采用双线性算子对 <span class="math inline">\(\tau\)</span>相邻尺度的<strong>相邻特征插值到与输入 <span class="math inline">\(x^s\)</span> 相同的尺度</strong></li><li><span class="math inline">\(y_{i}^{s}=\max _{k\in\left[s-\frac{\tau}{2}, s+\frac{\tau}{2}\right]} \max _{j \in\mathcal{N}_{i}^{\phi \times \phi}} \tilde{x}_{j}^{s, k}\)</span></li><li>对于 <span class="math inline">\(s\)</span> 尺度下的空间位置 <span class="math inline">\(i\)</span> ，在预先定义的三维中，根据比例 <span class="math inline">\(\tau\)</span> 尺度和 <span class="math inline">\(\phi \times \phi\)</span> 的空间距离，得到最大值<span class="math inline">\(y_i^s\)</span>。这个操作可以通过高效的<strong>3D max-pooling运算</strong>轻松实现</li></ul></li><li>此外，为了将3D MaxFiltering嵌入到现有框架中，实现端到端训练，作者提出了一个新的模块，如上图</li><li>该模块利用最大滤波来选择<strong>局部区域中激活值最高的预测</strong>，可以增强与其他预测的区别，这在后文中得到了进一步的验证。由于这一特性，作者<strong>采用3DMF对粗密预测进行细化，抑制重复预测</strong>。此外，所有的模块都是由简单的可微算子构造的，计算开销很小</li></ul></li><li><p><strong>Auxiliary Loss</strong></p><ul><li><img src="/2021/06/05/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-0x19/2.png"></li><li>在使用NMS时，如上表所示，POTO和3DMF的性能仍然低于FCOS基线</li><li>作者认为这种现象可能是由于<strong>一对一的标签分配提供较少的监督</strong>，使得网络很难学习强大和鲁棒的特征表示</li><li>为此引入了一个基于one-to-many的标签分配的辅助损耗，以提供足够的监督</li><li><img src="/2021/06/05/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-0x19/6.png"></li><li>与ATSS相似，辅助损失采用了改进的one-to-many标签分配的focal loss</li><li>one-to-many标签分配首先根据上面提出的匹配质量，在每个FPN阶段将<strong>前9个预测作为候选</strong>。然后将匹配质量超过<strong>统计阈值</strong>（统计阈值由所有候选匹配质量的均值和标准差的总和计算）的候选样本作为前景样本分配</li></ul></li></ul><h3 id="experiments">Experiments</h3><ul><li><img src="/2021/06/05/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-0x19/7.png"></li><li>上图展示了从FCOS基线和提出的框架得到的分类分数的可视化<ul><li>例如，一对多任务的FCOS基线输出了<strong>大量重复的预测</strong>，这些预测是高度激活的，与最自信的预测具有可比性的激活分数，这些重复的预测被评估为假阳性样本，并极大地影响性能</li><li>与此相反，通过使用所提出的POTO规则，重复样本的得分被显著地抑制。该特性对于探测器在没有NMS的情况下实现直接边界框预测至关重要</li><li>此外，通过提出的3DMF模块，进一步增强了这一特性，特别是在最自信的预测附近区域，同时由于3DMF模块引入了多尺度竞争机制，检测器可以很好地在FPN的不同阶段进行独特的预测，如上图中的一个实例<strong>在各个阶段有一个高度激活的分数</strong></li></ul></li><li>做的一些消融实验，直接看实验结果表格吧，不解释了，，，</li><li><img src="/2021/06/05/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-0x19/8.png"></li><li>为了弥补全卷积网络与端到端目标检测之间的差距，本文提出了一种基于predict-awareone-to-one标签分配策略和3D Max Filtering方法。在引入辅助损失的情况下，本文端到端框架在COCO和CrowdHuman数据集上使用NMS实现了比许多先进检测器更优越的性能。在复杂拥挤的场景中也显示了巨大的潜力，这可能有利于许多其他实例级任务。附录的检测对比图，如下图所示，在有目标重叠情况下，带有NMS的FCOS确实是会去掉一些预测正确的框，而留下一些虚警，而论文提出的方法在此方面有较好的效果</li><li><img src="/2021/06/05/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-0x19/9.png"></li></ul><h3 id="section">😝😜😋</h3><p>目标检测的论文确实要比图像增强去噪这方面复杂不少，这篇论文里面，作者确实是提出了很多想法，也许是我没读过太多这方面的论文吧，不过确实还是感觉很强的</p><p>说到底，作者就是针对NMS这一算法，想要给替换掉，然后想着使用一对一标签进行端到端学习，于是提出了使用predict-awareone-to-one标签分配策略和3D Max Filtering方法，同时引入了一个辅助损失来提供更多的监督信息，网络结构部分就是3DM那个信息的流动方向没太看懂，再研究研究，然后别的部分都还好，刚开始读这种目标检测的论文，确实有点吃力，里面说的很多名词都没怎么接触过，不太好理解，，，</p><h2 id="removing-diffraction-image-artifacts-in-under-display-camera-via-dynamic-skip-connection-networks">RemovingDiffraction Image Artifacts in Under-Display Camera via Dynamic SkipConnection Networks</h2><p>通过动态跳跃连接网络消除屏下相机图像中的由于衍射产生的痕迹</p><h3 id="abstract-1">Abstract</h3><ul><li><p>屏下相机 (Under-Display Camera, UDC)</p><ul><li>消费者对无边框、无凹槽显示屏智能手机的需求引发了手机制造商对新定义的成像系统UDC的兴趣<ul><li>手机全面屏发展：刘海屏-水滴屏-挖孔屏-<strong>屏下摄像头</strong></li></ul></li><li>此外在视频会议中，使用UDC将相机放置在显示器的中心时，能够实现更自然的凝视聚焦</li><li><figure><img src="/2021/06/05/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-0x19/1626085977109.png" alt="1626085977109"><figcaption aria-hidden="true">1626085977109</figcaption></figure></li></ul></li><li><p>在典型的UDC系统中，半透明有机发光二极管(organic light-emittingdiode,OLED)像素阵列的微结构会<strong>衰减并衍射相机上的入射光</strong>，导致图像质量显著下降</p><ul><li><figure><img src="/2021/06/05/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-0x19/1626086265915.png" alt="1626086265915"><figcaption aria-hidden="true">1626086265915</figcaption></figure></li><li><figure><img src="/2021/06/05/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-0x19/1626086454586.png" alt="1626086454586"><figcaption aria-hidden="true">1626086454586</figcaption></figure><ul><li><strong>中兴Axon 20手机</strong>上UDCOLED的特写——<strong>降低了摄像头上方区域的像素密度，从而提高了透明度</strong></li></ul></li><li>典型的UDC系统将相机模块放置在半透明有机发光二极管(OLED)显示器的下方并与之紧密相连<ul><li><strong>OLED屏幕本身就是透光的，不过透光不是很多</strong>（5%）</li></ul></li><li>尽管显示器看起来是部分透明的，但是光可以通过的区域，即显示像素之间的间隙，通常是微米级的，这基本上<strong>衍射了从场景到传感器的入射光</strong></li><li><figure><img src="https://img-blog.csdnimg.cn/img_convert/90ebd1497b981eedf8f1f7c1c50c2931.png" alt="光栅衍射主极大个数_大学物理——光的干涉和衍射（二）_weixin_39633452的博客-CSDN博客"><figcaption aria-hidden="true">光栅衍射主极大个数_大学物理——光的干涉和衍射（二）_weixin_39633452的博客-CSDN博客</figcaption></figure></li><li>市面上主流的手机屏幕的像素密度大概为 <span class="math inline">\(300-400,500-600 ppi, 400 ppi \approx 63 \mu m /pixel\)</span>，像素之间的空隙则要更小，已经满足了相关条件，所以更<strong>会发生光的衍射，不可忽略</strong></li></ul></li><li><p>在这项工作中，作者旨在分析和解决上述退化问题，定义了一个<strong>基于物理的图像形成模型</strong>，以更好地理解退化</p><ul><li>作者利用世界上最早的商品UDC智能手机原型之一来测量UDC系统的真实世界<strong>点扩散函数(Point Spread Function,PSF)</strong>，并提供<strong>基于模型的数据合成流程来生成真实的降级图像</strong><ul><li>这里的点扩散函数，就是说给一个成像系统一个点冲击的时候，它感受器产生的结果</li><li><figure><img src="/2021/06/05/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-0x19/1626092238569.png" alt="1626092238569"><figcaption aria-hidden="true">1626092238569</figcaption></figure></li></ul></li></ul></li><li><p>作者专门设计了一个新的领域知识支持的<strong>动态跳跃连接网络DynamicSkip Connection Network, DISCNet)</strong>来恢复UDC图像</p><ul><li>将图像形成模型的领域知识结合到网络设计中</li><li><strong>将PSF作为一个附加条件</strong>，将其输入到一个条件编码器中，以便于<strong>动态滤波器的生成</strong></li><li>在这项工作中，作者构建了<strong>多尺度滤波器生成器</strong>，并<strong>在特征域中采用动态卷积来处理大支持度和长尾PSF的退化</strong></li></ul></li><li><p>基于物理的图像形成模型和提出的DISCNet可以为UDC图像恢复的进一步探索提供基础，甚至为更广泛意义上的一般衍射伪影去除提供基础</p></li><li><p>在解决UDC图像恢复问题的第一次尝试（2020年的一篇论文）中，作者提出了一种<strong>监控摄像机成像系统(MonitorCamera Imaging System,MCIS)来捕获成对数据</strong>，并使用图像构造来生成两种类型的OLED显示器的点扩散函数(PSF)，使用UNet的一个变体将UDC图像恢复问题解决为一个盲去卷积问题</p><ul><li><figure><img src="/2021/06/05/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-0x19/1626172472059.png" alt="1626172472059"><figcaption aria-hidden="true">1626172472059</figcaption></figure><ul><li>使用一块屏幕进行显示，然后将摄像机放在OLED屏幕后面采集数据，这块OLED屏幕会通过拿下来再放置上去来得到图像对，这难免会造成很多影响（<strong>偏移、倾斜等</strong>）</li></ul></li><li>这项开创性工作有几个缺点:<ul><li>由于实际和合成PSF之间的不匹配，PSF不准确</li><li>在MCIS捕获的数据中<strong>缺乏适当的高动态范围(high dynamic range,HDR)，缺少真实的UDC退化</strong><ul><li>虽然MCIS通常用于计算成像领域以捕获系统PSF或获取成对图像数据，但大多数商品监视器缺乏高动态范围，而<strong>高动态范围是模拟UDC系统中真实衍射伪影所必需的</strong></li><li>因此<strong>他们使用的PSF具有不完整的旁瓣，并且图像产生的伪影（例如模糊、阴影和眩光）比较浅</strong><ul><li><figure><img src="/2021/06/05/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-0x19/1626092195900.png" alt="1626092195900"><figcaption aria-hidden="true">1626092195900</figcaption></figure></li><li><figure><img src="/2021/06/05/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-0x19/Users\pan\Desktop\论文分享\1626092238569.png" alt="1626092238569"><figcaption aria-hidden="true">1626092238569</figcaption></figure></li><li>在本文的工作中，作者<strong>在数据生成和PSF测量中考虑了HDR，从而能够正确处理真实场景</strong></li></ul></li></ul></li><li>原型UDC与实际生产的UDC显著不同</li><li>缺少对非MCIS数据的真实世界评估<ul><li>作者在他们的设置中使用常规的OLED手动覆盖相机，而不是实际的刚性UDC组件，对准真实数据进行实验和评估</li><li>因此，显示器相对于传感器平面的任何轻微移动、旋转或倾斜都会导致PSF的变化</li></ul></li><li>提出的网络没有充分利用领域知识<ul><li>尽管作者在数据合成中捕获并使用了PSF，但只是<strong>通过一个简单的UNet将UDC图像恢复公式化为一个盲去噪卷积问题</strong>，而没有明确地利用PSF作为有用的领域知识</li><li>相比之下，本文利用PSF作为DISCNet中的重要支持信息</li></ul></li></ul></li></ul></li><li><p>在这项工作中，作者旨在解决上述问题</p><ul><li>首先提出了一个真实的图像形成模型和测量协议，考虑了<strong>场景和相机传感器的适当动态范围</strong>，并恢复了真实世界中退化的UDC图像</li><li>为此<strong>试验了最早生产的UDC设备之一——中兴Axon20，它在自拍相机中加入了UDC系统</strong><ul><li>这里作者的目的是分析和研究衍射效应造成的伪像，而不是为中兴手机摄像头提出一个产品就绪的解决方案。该方法是通用的，并且可应用于其他UDC设备，或者<strong>更广泛的其他衍射受限成像系统</strong>，例如显微镜成像、针孔照相机</li></ul></li><li><figure><img src="/2021/06/05/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-0x19/1626093707637.png" alt="1626093707637"><figcaption aria-hidden="true">1626093707637</figcaption></figure></li><li>作者设计了一个成像系统，用点光源直接测量UDC设备的PSF。如图所示，由于显示器的衍射，产生的PSF具有一些特殊的特性:<strong>空间支撑大，中心响应强，长尾低能旁瓣</strong></li><li>利用测量的PSF，重新构建了图像形成模型，以<strong>解决由于场景的有限动态范围而缺失的真实耀斑、雾霾和模糊</strong></li><li>然后作者开发了一个基于图像形成模型的数据模拟流程，使用HDR图像来逼近真实场景</li><li>此外使用UDC手机的自拍相机捕捉<strong>真实图像</strong>，以验证模拟数据并评估恢复网络的性能</li><li><figure><img src="/2021/06/05/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-0x19/1626093919998.png" alt="1626093919998"><figcaption aria-hidden="true">1626093919998</figcaption></figure></li><li>如图所示，模拟和真实数据显示了类似的退化，尤其是在那些高强度区域。具体来说，耀斑可以在强光源附近观察到，在强光源处，<strong>高光以结构化衍射图案扩散到相邻的低强度区域</strong></li></ul></li><li><p>为了恢复UDC图像，作者提出了一种动态跳跃连接网络(DISCNet)，它将图像形成模型的领域知识结合到网络设计中</p><ul><li>传感器饱和破坏了基于PSF的卷积的移位不变性，导致空间变化的退化。这促使我们设计一个动态滤波器网络来动态预测每个像素的滤波器</li><li>此外，由于PSF的大规模空间支持，提出了多尺度架构，并在特征域进行动态卷积，以获得更大的感受野。此外，还引入了一个条件编码器来利用PSF的信息</li></ul></li><li><p>综上所述，本文贡献如下</p><ul><li>通过考虑动态范围和饱和度重新构建了UDC系统的成像模型，其中考虑了UDC图像中常见的衍射光斑</li><li>利用第一批UDC智能手机原型来测量真实世界的PSF<ul><li>PSF用作基于模型的数据合成流程的一部分，以生成逼真的降级图像</li></ul></li><li>设计了一个动态跳跃连接网络(DISCNet)，它结合了UDC图像形成模型的领域知识<ul><li>实验结果表明，这种方法能有效地消除衍射图像伪影</li></ul></li></ul></li></ul><h3 id="image-formation-model-and-dataset">Image Formation Model andDataset</h3><ul><li><strong>UDC的真实图像形成模型遭受几种类型的退化，包括衍射效应、饱和度和相机噪声，从而构建了退化模型</strong>：<ul><li><span class="math inline">\(\hat{y}=\phi[C(x * k+n)]\)</span><ul><li>其中 <span class="math inline">\(x\)</span>代表<strong>具有高动态范围(HDR)的真实场景的光照度</strong></li><li><span class="math inline">\(k\)</span>是已知的卷积核，即点扩散函数(PSF)，然后和 <span class="math inline">\(x\)</span> 进行二维卷积操作</li><li><span class="math inline">\(n\)</span> 模拟相机噪声</li><li>为了对从<strong>数字传感器的有限动态范围</strong>中导出的饱和度进行建模，我们应用了由<span class="math inline">\(C(x) = min(x,x_{max})\)</span>表示的限幅操作 <span class="math inline">\(C(\cdot)\)</span> ，其中<span class="math inline">\(x_{max}\)</span> 是范围阈值</li><li>非线性增强映射函数（non-linear tone mapping function） <span class="math inline">\(\phi(\cdot)\)</span>用于匹配人类对场景的感知，还有一些相机的响应函数等操作</li></ul></li></ul></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">_tonemap</span>(<span class="hljs-params">self, x, <span class="hljs-built_in">type</span>=<span class="hljs-string">&#x27;simple&#x27;</span></span>):<br>    <span class="hljs-keyword">if</span> <span class="hljs-built_in">type</span> == <span class="hljs-string">&#x27;mu_law&#x27;</span>:<br>        norm_x = x / x.<span class="hljs-built_in">max</span>()<br>        mapped_x = np.log(<span class="hljs-number">1</span> + <span class="hljs-number">10000</span> * norm_x) / np.log(<span class="hljs-number">1</span> + <span class="hljs-number">10000</span>)<br>    <span class="hljs-keyword">elif</span> <span class="hljs-built_in">type</span> == <span class="hljs-string">&#x27;simple&#x27;</span>:<br>        mapped_x = x / (x + <span class="hljs-number">0.25</span>)<br>    <span class="hljs-keyword">elif</span> <span class="hljs-built_in">type</span> == <span class="hljs-string">&#x27;same&#x27;</span>:<br>        mapped_x = x<br>    <span class="hljs-keyword">else</span>:<br>        <span class="hljs-keyword">raise</span> NotImplementedError(<span class="hljs-string">&#x27;tone mapping type [&#123;:s&#125;] is not recognized.&#x27;</span>.<span class="hljs-built_in">format</span>(<span class="hljs-built_in">type</span>))<br>    <span class="hljs-keyword">return</span> mapped_x<br></code></pre></td></tr></table></figure><h4 id="psf-measurement">PSF Measurement</h4><ul><li><figure><img src="/2021/06/05/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-0x19/Users\pan\Desktop\论文分享\1626092238569.png" alt="1626092238569"><figcaption aria-hidden="true">1626092238569</figcaption></figure></li><li>给定单位幅度点源输入，传感器捕获的光场 <span class="math inline">\(U_S(p,q)\)</span> 可以表示为<ul><li><span class="math inline">\(\begin{aligned} U_{S}(p,q)=&amp;\left\{\left[\exp \left(\frac{i \pi r^{2}}{\lambda z_{1}}\right)\cdot t(p, q)\right] * \exp \left(\frac{i \pi r^{2}}{\lambdad}\right)\right.\\ &amp; \left.\cdot \exp \left(\frac{-i \pir^{2}}{\lambda f}\right)\right\} * \exp \left(\frac{i \pi r^{2}}{\lambdaz_{2}}\right) \end{aligned}\)</span><ul><li>这里的 <span class="math inline">\((p,q)\)</span> 表示为二维坐标，<span class="math inline">\(r^2 = p^2 + q^2\)</span></li><li><span class="math inline">\(\lambda\)</span> 为波长， <span class="math inline">\(f\)</span> 是镜头的焦距， <span class="math inline">\(t(p,q)\)</span> 是显示器的传输函数</li><li><span class="math inline">\(z_1,d,z_2\)</span>分别表示光源和显示器之间的距离、显示器和透镜之间的距离以及透镜和传感器之间的距离</li><li><span class="math inline">\(*\)</span> 表示卷积， <span class="math inline">\(\cdot\)</span> 表示乘积</li></ul></li><li>最后，成像系统的PSF由 <span class="math inline">\(k=\left|U_{S}\right|^{2}\)</span> 给出</li><li>ps：这一部分都是光学上的东西，不过可以在相关论文上找到公式推导和实验结果等<ul><li>顾梦涛, 宋祥磊, 张彪, 等. 基于波动光学的显微光场成像点扩散函数.北京航空航天大学学报, 2019, 45(8): 1552-1559.</li></ul></li></ul></li><li>有了某个显示器的精确像素布局，那么就可以<strong>从理论上模拟在显示器条件下的光学系统的PSF</strong><ul><li>然而尽管模拟和真实测量的PSF具有相似的形状，但由于<strong>模型近似和制造缺陷</strong>，它们在颜色和对比度上略有不同</li><li>此外，我们无法访问在这项工作中使用的OLED的传输函数 <span class="math inline">\(t( p, q)\)</span>，由于其自身原因，像素结构未知</li></ul></li><li>因此，作者设计了一个成像系统，通过将白点光源放置在离OLED显示器1米远的地方来直接测量峰值功率因数。在这个距离上，点光源的大小相当于传感器的一个像素，因此该光源可被视为脉冲输入</li><li>为了捕获整个PSF，包括强主峰和弱旁瓣，在不同的曝光下（1，1/32，1/768）连续拍摄三幅图像，然后将其归一化为相同的亮度水平</li><li>UDC系统的捕获PSF显示了结构化模式<ul><li><strong>中心的响应(表示为主峰)非常强</strong>，并且具有更大能量</li><li>与普通相机的PSF相比，它<strong>具有更大的空间支撑(超过800 ×800)和能量指数下降的尖峰形长尾旁瓣</strong></li><li>在旁瓣的尾部区域，可以观察到<strong>明显的颜色偏移</strong></li></ul></li><li>如果将同一场景<strong>从高动态范围剪辑到低动态范围，这些由衍射引起的耀斑将变得不可见</strong></li><li>由于输入场景的高动态范围，数字传感器(通常为10位)在实际应用中不可避免地会饱和，导致信息丢失<ul><li>在成像模型中也应该考虑这个因素</li></ul></li><li><figure><img src="/2021/06/05/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-0x19/1626140101133.png" alt="1626140101133"><figcaption aria-hidden="true">1626140101133</figcaption></figure><ul><li>UDC和普通摄像机的PSF能量比较。PSF被增亮以可视化结构化旁瓣模式</li><li>由于有限的光圈大小和制造缺陷，<strong>普通相机的PSF将是某种大小的模糊内核，而不是完美的点</strong></li></ul></li></ul><h4 id="data-collection-and-simulation">Data Collection andSimulation</h4><ul><li>为了生成合成数据，作者从HDRIhaven数据集收集了132幅动态范围大的HDR图像（分辨率为8192×4096的360度全景图像）<ul><li>首先将这些全景图像重新投影回透视图，然后将它们裁剪成800×800的patches块</li><li>这样总共获得了2016张用于训练的子图像和360张用于测试的子图像</li><li>对于每种裁剪出来的图像，使用上面提到的方程模拟了相应的退化图像（校准的PSF用作内核k）</li></ul></li><li>对于每个真实场景，使用中兴 Axon20手机拍摄了三张不同曝光（1，1/4，1/16）的图像，然后将它们组合成一张HDR图像<ul><li>为了保证数据的线性，作者直接使用了HDR融合后的原始数据，没有进行任何非线性处理</li></ul></li></ul><h4 id="dynamic-skip-connection-network">Dynamic Skip ConnectionNetwork</h4><ul><li><p>动机</p><ul><li>将UDC图像恢复视为一个非盲图像恢复问题<ul><li>给定退化图像 <span class="math inline">\(\{ \hat y_i \}\)</span>和GT退化信息（PSF） <span class="math inline">\(\{ k_i \}\)</span>，来去恢复得到清晰图像 <span class="math inline">\(\{ x_i\}\)</span></li><li><strong>在已知卷积核的情况下</strong>，非盲恢复建立了盲恢复的上限</li><li>可以通过结合任何 PSF 估计算法用于盲 UDC 图像恢复</li></ul></li><li>PSF的形状和强度会<strong>根据输入像素及其在相应位置的邻域而变化</strong><ul><li>例如 OLED将饱和高光衍射到相邻的不饱和区域，促使从附近区域截取信息的适应性恢复</li></ul></li><li>受最近成功的卷积核预测网络(Kernel Prediction Network, KPN)的启发，作者提出了动态跳跃连接网络(DISCNet)<ul><li>它在每个像素动态生成卷积核，并将其应用于具有跳跃连接的不同网络层的不同特征空间</li><li>该网络以两个输入为条件以促进恢复空间变化的退化图像<ul><li>提供关于图像形成模型的领域知识的PSF</li><li>提供光强度和邻域上下文信息</li></ul></li><li>对于动态卷积，像大多数现有的基于KPN的方法一样，在图像域中直接应用预测滤波器并不最适合UDC图像恢复，因为UDC中的PSF具有大的支持度和长尾旁瓣<ul><li>这种具有大PSF的逆卷积过程只能在具有足够大的核(大于100)的图像域中很好地近似，而动态滤波器的尺寸通常要小得多(例如5或7)</li></ul></li><li>因此，作者建议<strong>在特征域中应用动态卷积</strong>。除此之外作者还构建了一个多尺度架构，其中每个尺度上的滤波器生成器分别预测动态滤波器，以进一步扩大学习滤波器的空间支持</li></ul></li></ul></li><li><p>网络结构</p><ul><li><figure><img src="/2021/06/05/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-0x19/1626148304608.png" alt="1626148304608"><figcaption aria-hidden="true">1626148304608</figcaption></figure></li><li><p>网络包括一个<strong>恢复分支</strong>和一个<strong>动态跳跃连接网络</strong></p><ul><li>恢复分支学习提取特征并恢复最终的干净图像</li><li>DISCNet用于处理各种退化，并转换和细化从恢复分支提取的特征</li></ul></li><li><p>Restoration Branch</p><ul><li><figure><img src="/2021/06/05/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-0x19/1626148321728.png" alt="1626148321728"><figcaption aria-hidden="true">1626148321728</figcaption></figure></li><li>an encoder-decoder architecture with skip connections</li><li>编码器包含三个卷积块，每个卷积块都有一个步长为2的3 ×3卷积层、一个LeakyReLU层和两个残差块</li><li>通过编码器得到三种不同尺度的特征 <span class="math inline">\(E_1,E_2, E_3\)</span> ，提取的特征被送入DISCNet，并分别转换成 <span class="math inline">\(R_1,R_2,R_3\)</span></li><li>解码器由两个卷积块组成，包括一个上采样卷积层和两个残差块<ul><li>每个卷积块以其相应尺度的变换特征作为输入，并重构最终增强映射的清晰图像</li></ul></li></ul></li><li><p>Dynamic Skip Connection Network</p><ul><li><figure><img src="/2021/06/05/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-0x19/1626166163139.png" alt="1626166163139"><figcaption aria-hidden="true">1626166163139</figcaption></figure></li><li><p>假设退化图像 <span class="math inline">\(\hat y_i\)</span>的尺寸为 <span class="math inline">\(H \times W \times C\)</span>，通过Principal Component Analysis (PCA) 将PSF投影到一个 b维的向量上，称为kernel code，大小为 <span class="math inline">\(H \timesW \times b\)</span> ，降低计算复杂度</p><ul><li>在本文中，根据经验设置 <span class="math inline">\(b =5\)</span></li></ul></li><li><p>之后将这部分和退化图像直接拼接，得到大小为 <span class="math inline">\(H \times W \times (C+b)\)</span>的条件图，然后将其输入到DISCNet</p></li><li><p>提出的DISCNet主要包括三种设计:条件编码器、多尺度滤波器生成器、动态卷积</p></li><li><p>条件编码器使用<strong>类似于恢复分支的编码器</strong>的3个网络块来获取特定比例的特征<span class="math inline">\(H_1,H_2,H_3\)</span></p></li><li><p>尽管kernel code映射是全局统一的，但条件编码器仍然可以从具有空间可变性的退化图像中捕获丰富的信息，并设法从附近的低光区域恢复饱和信息</p></li><li><p>然后提取得到的不同尺度的特征被输入到它们相应的滤波器生成器</p><ul><li>其中每个滤波器生成器包括3 × 3卷积层、两个残差块和1 ×1卷积层以扩展特征维度</li></ul></li><li><p>给定动态滤波器 <span class="math inline">\(s\)</span>的大小，滤波器生成器 <span class="math inline">\(G_n\)</span>以特定的比例提取的特征 <span class="math inline">\(H_n \in \mathbb{R}^{h\times w \times c}\)</span> 作为输入，输出预测滤波器 <span class="math inline">\(F_n = G_n(H_n)\)</span> ，其中 <span class="math inline">\(F_n \in \mathbb{R}^{h \times w \timescs^2}\)</span> 。然后动态卷积使用这些滤波器来细化特征 <span class="math inline">\(E_n\)</span></p></li><li><p>对于特征 <span class="math inline">\(E_n \in {h \times w \timesc}\)</span> 的每一个像素点 <span class="math inline">\((i,j,c_m)\)</span> ，其输出 <span class="math inline">\(R_n\)</span> 为</p><ul><li><span class="math inline">\(R_{n}\left(i, j,c_{m}\right)=\left\langle K_{n}\left(i, j, c_{m}\right),\varphi\left(E_{n}\left(i, j,c_{m}\right)\right)\right\rangle\)</span></li><li>其中 <span class="math inline">\(K_{n}\left(i, j,c_{m}\right)\)</span> 是一个从 <span class="math inline">\(F_n(i,j,c_m)\in \mathbb{R}^{1 \times 1 \times s^2}\)</span> reshape 得到的 <span class="math inline">\(s \times s\)</span> 的卷积核</li><li><span class="math inline">\(\varphi(\cdot)\)</span> 表示以 <span class="math inline">\((i,j,c_m)\)</span> 为中心 <span class="math inline">\(s \times s\)</span> 大小的patch</li><li><span class="math inline">\(\langle \rangle\)</span>表示内积运算，然后将 <span class="math inline">\(R_n\)</span>投射到修复分支中</li></ul></li></ul></li></ul></li></ul><h3 id="experiments-1">Experiments</h3><ul><li>为了评估DISCNet对于非盲降级的有效性，作者旋转了PSF<ul><li>类似于在成像系统中围绕光轴旋转显示器</li><li>为了考虑旋转角度的变化，构建了一个内核集，其中角度在(-12，12)范围内变化，其中0弧度指的是原始PSF</li></ul></li><li>这种设置下，每个降级的图像都用方程模拟得到，在训练过程中，子图像被随机裁剪成256× 256大小的patch</li><li><figure><img src="/2021/06/05/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-0x19/1626227193659.png" alt="1626227193659"><figcaption aria-hidden="true">1626227193659</figcaption></figure><ul><li>在只有1个内核的数据集上训练的baseline很容易过度拟合到单个降级数据集，无法推广到其他降级类型<ul><li>由于假设的PSF和真实的PSF之间的差异，在其他数据集上的性能严重下降</li></ul></li><li>即使是最简单的单尺度动态卷积设计也有利于特征细化</li></ul></li><li><figure><img src="/2021/06/05/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-0x19/1626228205814.png" alt="1626228205814"><figcaption aria-hidden="true">1626228205814</figcaption></figure></li><li><figure><img src="/2021/06/05/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-0x19/1626228369489.png" alt="1626228369489"><figcaption aria-hidden="true">1626228369489</figcaption></figure></li><li><figure><img src="/2021/06/05/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-0x19/1626228394714.png" alt="1626228394714"><figcaption aria-hidden="true">1626228394714</figcaption></figure></li><li><figure><img src="/2021/06/05/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-0x19/1626228513343.png" alt="1626228513343"><figcaption aria-hidden="true">1626228513343</figcaption></figure></li></ul><h3 id="discussion">Discussion</h3><ul><li>本文的工作只是消除UDC系统中衍射图像伪影的第一步，而其他的复杂性，例如空间变化的功率谱、弱光下的噪声和散焦，需要更多的研究</li><li>所提出的DISCNet有时会由于模拟数据和真实数据之间的域差距而失败，例如相机噪声、运动模糊、场景变化等</li><li>定义了一个基于物理的图像形成模型，测量了UDC系统的真实世界PSF，并提供了一个基于模型的数据合成流程来生成真实的图像</li><li>作者还提出了一种新的基于领域知识的动态跳跃连接网络来恢复UDC图像。为进一步探索UDC图像复原提供了基础</li><li>本文对UDC的观点有潜力能够激发更多衍射受限的图像恢复工作</li></ul><h3 id="code">Code</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment">#############################</span><br><span class="hljs-comment"># Kernel Prediction Branch</span><br><span class="hljs-comment">#############################</span><br><span class="hljs-keyword">if</span> self.kernel_cond:<br>    <span class="hljs-keyword">if</span> self.kernel_cond == <span class="hljs-string">&#x27;img&#x27;</span>:<br>        cond_nc = in_nc<br>    <span class="hljs-keyword">elif</span> self.kernel_cond == <span class="hljs-string">&#x27;psf&#x27;</span>:<br>        cond_nc = psf_nc<br>    <span class="hljs-keyword">elif</span> self.kernel_cond == <span class="hljs-string">&#x27;img-psf&#x27;</span>:<br>        cond_nc = in_nc + psf_nc<br><br>    self.kconv_11 = conv_block(cond_nc, nf, kernel_size=kernel_size, act_type=act_type)<br>    self.kconv_12 = ResBlock(nf, res_scale=res_scale, act_type=act_type)<br>    self.kconv_13 = ResBlock(nf, res_scale=res_scale, act_type=act_type)<br><br>    <span class="hljs-keyword">if</span> self.multi_scale:<br>        self.dynamic_kernel1 = nn.Sequential(<br>            conv_block(nf, nf, kernel_size=kernel_size),<br>            ResBlock(nf, res_scale=res_scale, act_type=act_type),<br>            ResBlock(nf, res_scale=res_scale, act_type=act_type),<br>            conv_block(nf, nf * (kpn_sz ** <span class="hljs-number">2</span>), kernel_size=<span class="hljs-number">1</span>))<br><br>    self.kconv_21 = conv_block(nf, <span class="hljs-number">2</span>*nf, stride=<span class="hljs-number">2</span>, kernel_size=kernel_size, act_type=act_type)<br>    self.kconv_22 = ResBlock(<span class="hljs-number">2</span>*nf, res_scale=res_scale, act_type=act_type)<br>    self.kconv_23 = ResBlock(<span class="hljs-number">2</span>*nf, res_scale=res_scale, act_type=act_type)<br><br>    <span class="hljs-keyword">if</span> self.multi_scale:<br>        self.dynamic_kernel2 = nn.Sequential(<br>            conv_block(<span class="hljs-number">2</span>*nf, <span class="hljs-number">2</span>*nf, kernel_size=kernel_size),<br>            ResBlock(<span class="hljs-number">2</span>*nf, res_scale=res_scale, act_type=act_type),<br>            ResBlock(<span class="hljs-number">2</span>*nf, res_scale=res_scale, act_type=act_type),<br>            conv_block(<span class="hljs-number">2</span>*nf, <span class="hljs-number">2</span>*nf * (kpn_sz ** <span class="hljs-number">2</span>), kernel_size=<span class="hljs-number">1</span>))<br><br>    self.kconv_31 = conv_block(<span class="hljs-number">2</span>*nf, <span class="hljs-number">4</span>*nf, stride=<span class="hljs-number">2</span>, kernel_size=kernel_size, act_type=act_type)<br>    self.kconv_32 = ResBlock(<span class="hljs-number">4</span>*nf, res_scale=res_scale, act_type=act_type)<br>    self.kconv_33 = ResBlock(<span class="hljs-number">4</span>*nf, res_scale=res_scale, act_type=act_type)<br><br>    self.dynamic_kernel = nn.Sequential(<br>        conv_block(<span class="hljs-number">4</span>*nf, <span class="hljs-number">4</span>*nf, kernel_size=kernel_size),<br>        ResBlock(<span class="hljs-number">4</span>*nf, res_scale=res_scale, act_type=act_type),<br>        ResBlock(<span class="hljs-number">4</span>*nf, res_scale=res_scale, act_type=act_type),<br>        conv_block(<span class="hljs-number">4</span>*nf, <span class="hljs-number">4</span>*nf * (kpn_sz ** <span class="hljs-number">2</span>), kernel_size=<span class="hljs-number">1</span>))<br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment">#############################</span><br><span class="hljs-comment"># Kernel Prediction Branch</span><br><span class="hljs-comment">#############################</span><br><span class="hljs-comment"># kernel network</span><br><span class="hljs-keyword">if</span> self.kernel_cond:<br>    <span class="hljs-keyword">if</span> self.kernel_cond == <span class="hljs-string">&#x27;img&#x27;</span>:<br>        cond_x = x<br>    <span class="hljs-keyword">elif</span> self.kernel_cond == <span class="hljs-string">&#x27;psf&#x27;</span>:<br>        cond_x = psf.expand(-<span class="hljs-number">1</span>, -<span class="hljs-number">1</span>, x.shape[<span class="hljs-number">2</span>], x.shape[<span class="hljs-number">3</span>])<br>    <span class="hljs-keyword">elif</span> self.kernel_cond == <span class="hljs-string">&#x27;img-psf&#x27;</span>:<br>        cond_x = psf.expand(-<span class="hljs-number">1</span>, -<span class="hljs-number">1</span>, x.shape[<span class="hljs-number">2</span>], x.shape[<span class="hljs-number">3</span>])<br>        cond_x = torch.cat((cond_x, x), dim=<span class="hljs-number">1</span>)<br><span class="hljs-comment"># condition encoder</span><br>    kfea1 = self.kconv_13(self.kconv_12(self.kconv_11(cond_x)))<br>    kfea2 = self.kconv_23(self.kconv_22(self.kconv_21(kfea1)))<br>    kfea3 = self.kconv_33(self.kconv_32(self.kconv_31(kfea2)))<br>    <br>    <span class="hljs-keyword">if</span> self.multi_scale:<br>        dynamic_kernel1 = self.dynamic_kernel1(kfea1)<br>        dynamic_kernel2 = self.dynamic_kernel2(kfea2)<br>    dynamic_kernel = self.dynamic_kernel(kfea3)<br>    <br><br>    <br><span class="hljs-comment"># Dynamic convolution</span><br><span class="hljs-keyword">if</span> self.kernel_cond:<br>    fea3 = kernel2d_conv(fea3, dynamic_kernel, self.kpn_sz)<br><br><span class="hljs-comment"># Decoder</span><br><span class="hljs-keyword">if</span> self.multi_scale:<br>    fea2 = kernel2d_conv(fea2, dynamic_kernel2, self.kpn_sz)<br>    upfea2 = self.upconv_23(self.upconv_22(self.upconv_21(fea3) + fea2))<br><span class="hljs-keyword">else</span>:<br>    upfea2 = self.upconv_23(self.upconv_22(self.upconv_21(fea3) + fea2))<br><br><span class="hljs-keyword">if</span> self.multi_scale:<br>    fea1 = kernel2d_conv(fea1, dynamic_kernel1, self.kpn_sz)<br>    upfea1 = self.upconv_13(self.upconv_12(self.upconv_11(upfea2) + fea1))<br><span class="hljs-keyword">else</span>:<br>    upfea1 = self.upconv_13(self.upconv_12(self.upconv_11(upfea2) + fea1))<br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">kernel2d_conv</span>(<span class="hljs-params">feat_in, kernel, ksize</span>):<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    If you have some problems in installing the CUDA FAC layer, </span><br><span class="hljs-string">    you can consider replacing it with this Python implementation.</span><br><span class="hljs-string">    Thanks @AIWalker-Happy for his implementation.</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    channels = feat_in.size(<span class="hljs-number">1</span>)<br>    N, kernels, H, W = kernel.size()<br>    pad_sz = (ksize - <span class="hljs-number">1</span>) // <span class="hljs-number">2</span><br><br>    <span class="hljs-comment"># 边缘pad</span><br>    feat_in = F.pad(feat_in, (pad_sz, pad_sz, pad_sz, pad_sz), mode=<span class="hljs-string">&quot;replicate&quot;</span>)<br>    <span class="hljs-comment"># 将输入按照ksize进行切割</span><br>    feat_in = feat_in.unfold(<span class="hljs-number">2</span>, ksize, <span class="hljs-number">1</span>).unfold(<span class="hljs-number">3</span>, ksize, <span class="hljs-number">1</span>)<br>    <span class="hljs-comment"># 做一些变换</span><br>    feat_in = feat_in.permute(<span class="hljs-number">0</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">1</span>, <span class="hljs-number">5</span>, <span class="hljs-number">4</span>).contiguous()<br>    feat_in = feat_in.reshape(N, H, W, channels, -<span class="hljs-number">1</span>)<br><br>    <span class="hljs-comment"># 尺度变换</span><br>    kernel = kernel.permute(<span class="hljs-number">0</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">1</span>).reshape(N, H, W, channels, ksize, ksize)<br>    kernel = kernel.permute(<span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">5</span>, <span class="hljs-number">4</span>).reshape(N, H, W, channels, -<span class="hljs-number">1</span>)<br>    <span class="hljs-comment"># 内积</span><br>    feat_out = torch.<span class="hljs-built_in">sum</span>(feat_in * kernel, axis=-<span class="hljs-number">1</span>)<br>    feat_out = feat_out.permute(<span class="hljs-number">0</span>, <span class="hljs-number">3</span>, <span class="hljs-number">1</span>, <span class="hljs-number">2</span>).contiguous()<br>    <span class="hljs-keyword">return</span> feat_out<br></code></pre></td></tr></table></figure>]]></content>
    
    
    <categories>
      
      <category>study</category>
      
    </categories>
    
    
    <tags>
      
      <tag>图像增强与图像恢复</tag>
      
      <tag>高光谱图像去噪</tag>
      
      <tag>组会分享</tag>
      
      <tag>目标检测</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>论文阅读-0x18</title>
    <link href="/2021/05/22/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-0x18/"/>
    <url>/2021/05/22/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-0x18/</url>
    
    <content type="html"><![CDATA[<p>论文的阅读笔记：</p><p>《 NBNet: Noise Basis Learning for Image Denoising with SubspaceProjection 》, <a href="https://github.com/megvii-research/NBNet">[code]</a> , CVPR2021</p><p>《 Invertible Denoising Network: A Light Solution for Real NoiseRemoval 》, <a href="https://github.com/Yang-Liu1082/InvDN">[code]</a>,CVPR 2021</p><span id="more"></span><h2 id="nbnet-noise-basis-learning-for-image-denoising-with-subspace-projection">NBNet:Noise Basis Learning for Image Denoising with Subspace Projection</h2><h3 id="abstract">Abstract</h3><ul><li><p>一种新的图像去噪框架——NBNet</p></li><li><p>不同于以往的工作，作者提出从一个新的角度解决这一具有挑战性的问题:<strong>通过图像自适应投影来降噪</strong></p><ul><li>具体来说，我们提出通过<strong>在特征空间中学习一组重构基</strong>，训练出一个能分离信号和噪声的网络</li><li>然后，通过选择信号子空间对应的基，将输入投影到该空间，实现图像去噪</li></ul></li><li><blockquote><p>子空间方法用于图像分析是相当经典的，经常见到的是傅立叶变换、小波去噪、SVD。基本原理就是把图像拆分为一组基，只要找到噪声所处的基，把这部分去掉就可以了。</p></blockquote></li><li><p>作者认为投影可以自然地保持输入信号的局部结构，特别是在光线较暗或纹理较弱的区域。为此，作者提出了SSA，这是一个非局部子空间注意力模块，明确设计用于学习基的生成和子空间投影</p></li><li><p>进一步将SSA与NBNet结合，NBNet是一种用于端到端图像去噪的UNet结构网络</p></li><li><p>作者对基准进行了评估，包括SIDD和DND,NBNet在PSNR和SSIM上实现了最先进的性能，且计算成本显著降低</p></li><li><p>图像去噪主要是从有噪声污染的 <span class="math inline">\(y\)</span> 中恢复出 干净信号 <span class="math inline">\(x\)</span> ，去除噪声信息 <span class="math inline">\(n\)</span></p><ul><li><span class="math inline">\(y = x + n\)</span></li></ul></li><li><p>但如果<strong>干净信号 <span class="math inline">\(x\)</span>，去除噪声信息 <span class="math inline">\(n\)</span>两者都未知或者难以分离</strong>，那么这种问题是不适用上述方法的</p><ul><li>为此，许多去噪方法利用图像先验和噪声模型从噪声观测中估计图像或噪声</li><li>传统的NLM和BM3D方法利用了图像的局部相似性和噪声的独立性</li><li>小波去噪方法利用了图像在变换域的稀疏性</li><li>最近的基于深度神经网络(DNN)的去噪方法通常隐式地利用从大量成对训练数据中学习到的图像先验和噪声分布<ul><li>但在诸如<strong>弱纹理或高频细节等困难场景</strong>中恢复高质量图像仍然具有挑战性</li><li>作者观察到，卷积网络通常依赖于局部滤波器对噪声和信号分离的响应。而在低信噪比(SNR)的情况下，如果没有额外的全局结构信息，局部响应很容易被混淆</li></ul></li></ul></li></ul><p><img src="/2021/05/22/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-0x18/1.png"></p><ul><li>在本文中，作者通过投影来利用非局部的图像信息，投影方式如上图所示<ul><li>由输入图像生成一组图像基向量，然后在这些基向量张成的子空间中重构图像</li><li><strong>由于自然图像通常处于低秩信号子空间中，通过适当地学习和生成基向量，重建图像可以保留大部分原始信息，抑制与生成基集无关的噪声</strong></li></ul></li><li>基于这种思想，作者提出了NBNet，NBNet的总体架构是一个常用的UNet，除了关键成分子空间注意力(SSA)模块，该模块以端到端方式学习子空间基和图像投影</li><li>总结如下：<ul><li>从<strong>子空间投影</strong>的新角度分析了图像去噪问题，设计了一个简单有效的SSA模块来学习子空间投影，该模块可以插入到普通的CNN中</li><li>提出NBNet，一个<strong>带有SSA模块的UNet</strong>，用于基于投影的图像去噪</li><li>在许多流行的基准上，NBNet在PSNR和SSIM方面都实现了最先进的性能</li><li>对基于投影的图像去噪进行了深入的分析，证明了这是一个有前途的探索方向</li></ul></li><li>网络架构设计和噪声建模是基于CNN方法的两个主要方向<ul><li>DnCNN演示了残差学习和批归一化对深度cnn去噪网络的有效性</li><li>后来，为了扩大接收场或平衡效率，又提出了更多的网络结构，如扩张卷积、跳跃连接的自动编码器、ResNet、递归分支反卷积网络(RBDN)</li></ul></li><li>在这项工作中，作者采用了一个<strong>UNet风格的体系结构与一个新的子空间注意模块</strong>。不同于使用注意模块进行区域或特征选择的方法，<strong>SSA的设计目的是学习子空间基和图像投影</strong></li><li>真实图像噪声的合成问题也得到了广泛的研究<ul><li>为了接近真实噪声，之前的工作中已经探讨了多种合成噪声，如高斯-泊松、摄像机内过程仿真、高斯混合模型(GMM)和gan产生的噪声等</li><li>已经证明，从合成数据中适当训练的网络可以很好地概括到真实数据</li><li>不同于以往的噪声建模，该方法研究了<strong>子空间基的生成，并通过投影来提高降噪效果</strong></li></ul></li></ul><h3 id="method">Method</h3><h4 id="subspace-projection-with-neural-network">Subspace Projectionwith Neural Network</h4><p><img src="/2021/05/22/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-0x18/1.png"></p><ul><li>如上图，投影方法主要有两个步骤：<ul><li><strong>Basis generation：从图像特征生成子空间的基向量</strong></li><li><strong>Projection：将特征映射变换到信号子空间</strong></li></ul></li><li>假设 <span class="math inline">\(\boldsymbol{X}_{1},\boldsymbol{X}_{2} \in \mathbb{R}^{H \times W \times C}\)</span>是从一幅图像中得到的两个特征向量，它们是一个CNN的中间激活，可以在不同的层，但具有相同的大小<ul><li>首先基于 <span class="math inline">\(\boldsymbol{X}_{1},\boldsymbol{X}_{2}\)</span> 估计 <span class="math inline">\(K\)</span>个基向量 <span class="math inline">\([v_1,v_2,\dots,v_K]\)</span> ，向量<span class="math inline">\(v_i \in \mathbb{R}^{N}\)</span>是信号子空间的基向量，其中 <span class="math inline">\(N=HW\)</span></li><li>然后将 <span class="math inline">\(\boldsymbol{X}_{1}\)</span>变换到子空间 <span class="math inline">\(\{ v \}\)</span></li></ul></li><li>设θ参数化的函数 <span class="math inline">\(f_{\theta}:\left(\mathbb{R}^{H \times W \times C},\mathbb{R}^{H \times W \times C}\right) \rightarrow \mathbb{R}^{N \timesK}\)</span> ，基向量的生成可以表示成：<ul><li><span class="math inline">\(V = f_\theta (X_1,X_2 )\)</span></li><li>这里作者用一个小型卷积网络来实现该函数</li><li>这里的实现就是沿着通道轴将 <span class="math inline">\(\boldsymbol{X}_{1}, \boldsymbol{X}_{2}\)</span>拼接，然后送到一个浅层残差卷积块中，输出通道为K</li><li>在训练过程中以端到端方式更新基向量生成块的权重和偏差</li><li><img src="/2021/05/22/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-0x18/2.png"></li></ul></li><li>已知上述的 其列是K维信号子空间 <span class="math inline">\(\mathcal{V} \subset \mathbb{R}^{N}\)</span>的基向量 的矩阵 <span class="math inline">\(\boldsymbol{V} \subset\mathbb{R}^{N}\)</span><ul><li>可以通过正交线性投影将图像特征向量 <span class="math inline">\(\boldsymbol{X}_1\)</span> 投影到 <span class="math inline">\(\mathcal{V}\)</span> 上</li><li>设 <span class="math inline">\(\boldsymbol{P}: \mathbb{R}^{N}\rightarrow \mathcal{V}\)</span>是变换到信号子空间的正交投影矩阵，可以通过 <span class="math inline">\(\boldsymbol{V}\)</span> 计算得到<ul><li><span class="math inline">\(\boldsymbol{P}=\boldsymbol{V}\left(\boldsymbol{V}^{\boldsymbol{\top}}\boldsymbol{V}\right)^{-1}\boldsymbol{V}^{\boldsymbol{\top}}\)</span></li><li>这里为了是基向量比此正交，所以需要正规化项 <span class="math inline">\(\left(\boldsymbol{V}^{\boldsymbol{\top}}\boldsymbol{V}\right)^{-1}\)</span></li></ul></li><li>最后，图像特征表示 <span class="math inline">\(\boldsymbol{X}_1\)</span>可以在信号子空间中被重构为 <span class="math inline">\(\boldsymbol{Y}\)</span><ul><li><span class="math inline">\(\boldsymbol{Y} =\boldsymbol{P}\boldsymbol{X}_1\)</span></li></ul></li><li>投影中的操作是带有某些适当重塑的纯线性矩阵操作，这是完全可微的，并且可以在现代神经网络框架中轻松实现</li></ul></li><li>结合基生成和子空间投影，作者构造了所提出的SSA模块的结构</li><li><img src="/2021/05/22/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-0x18/3.png"></li><li>这样回过头来看看这部分的实现，感觉还是蛮简单的，感觉就是加了几层卷积层，然后来学习一个所谓的基向量，然后基向量来对特征向量进行重构，确实很像是个注意力模块，但和注意力模块的功能也不太一样，它主要是为了进行重构嘛，，确实和作者说的一样</li></ul><h4 id="nbnet-architecture-and-loss-function">NBNet Architecture andLoss Function</h4><p><img src="/2021/05/22/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-0x18/4.png"></p><ul><li>提出的SSA模块被放置在每个跳跃连接中<ul><li>由于来自底层的特征映射包含更详细的原始图像信息，将底层特征映射作为<span class="math inline">\(X_1\)</span> ，高级特征作为 <span class="math inline">\(X_2\)</span> ，并将它们输入到SSA模块中</li><li>换句话说，<strong>在上采样的高级特征的引导下，将跳跃连接的低级特征映射到信号子空间</strong></li><li>在输出到下一个解码器阶段之前，投影的特征与原始的高级特征融合</li></ul></li><li>与传统的类unet体系结构在每个译码阶段直接融合低层和高层特征图相比，NBNet的主要区别在于融合前通过SSA模块对低层特征进行投影</li><li>然后网络结构中的具体实现细节就不写了，直接看上面的图吧</li><li>损失函数也很简单<ul><li><span class="math inline">\(\mathcal{L}(G, \boldsymbol{x},\boldsymbol{y})=\|\boldsymbol{x}-G(\boldsymbol{y})\|_{1}\)</span></li></ul></li></ul><h3 id="experiment">Experiment</h3><p>主要写一下消融实验部分</p><ul><li><img src="/2021/05/22/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-0x18/5.png"></li><li>跳跃连接中的Blocks和SSA都对网络性能有提升，不过单加SSA和单加Blocks性能十分相近，但计算成本要低很多</li><li><img src="/2021/05/22/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-0x18/6.png"></li><li>UNet+Blocks+SSA作为基线，然后用其他替代方案替换SSA模块，如non-local, Attention-UNet和SSA（点积，其中投影被点积操作替代）</li><li><img src="/2021/05/22/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-0x18/7.png"></li><li>K值的影响<ul><li>当基向量的个数K为32时，模型不收敛<ul><li>在这个设置中，由于第一阶段的通道数也是32个，SSA模块不能有效地作为子空间投影，因为K等于全维大小</li><li>另一方面，子空间的高维可能会增加模型拟合的难度，从而造成训练的不稳定性</li></ul></li><li>如果K =1，则子空间中保留的信息不足，在跳连过程中会造成严重的信息丢失</li><li>将K设置为8和16可以获得类似的性能，并且SSA模块可能创建低维、紧凑或可分类的子空间</li><li>由此可见，子空间维数K是一个合理范围内的鲁棒超参数</li></ul></li><li><img src="/2021/05/22/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-0x18/8.png"></li><li>关于投影的不同选项:如何生成基向量以及如何为投影选择特征映射</li><li><img src="/2021/05/22/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-0x18/9.png"></li><li>选取一个样本图像并检查由SSA模块生成的子空间，可以看到细微的纹理在基底上较好的学了出来，NBNet在弱纹理区域表现优于其他方法，能够更好的实现图像细节的修复<ul><li>上图左侧所示，16个通道中有许多包含均匀地跨越整个图像patch的点模式</li><li>这种改进应归因于SSA模块创建的非局部相关性：<strong>上部的弱纹理由图像其他部分中类似的出现所支持</strong>，并且投影通过组合图像来重构纹理全局确定的系数的基础</li><li>与此相反，传统的卷积神经网络依赖于固定值局部滤波器的响应和来自降采样特征的粗糙信息。当过滤响应不显著且粗糙信息模糊时，例如在弱纹理区域，非局部信息几乎不能改善局部响应</li></ul></li></ul><h3 id="section">😝😜😋</h3><p>这样整篇读完，还是感觉比较简单的，但是这种想法或者说是说法吧，确实挺有道理的，去子空间做去噪任务，而且看作者的效果确实很棒</p><h2 id="invertible-denoising-network-a-light-solution-for-real-noise-removal">InvertibleDenoising Network: A Light Solution for Real Noise Removal</h2><h3 id="abstract-1">Abstract</h3><ul><li>可逆网络（Invertible networks）在图像去噪方面有各种各样的好处，因为它们是轻量级的，信息无损的，并且在反向传播过程中节省内存</li><li>然而，应用可逆模型去噪具有挑战性，因为输入是有噪声的，而可逆输出是干净的，遵循两种不同的分布</li></ul>]]></content>
    
    
    <categories>
      
      <category>study</category>
      
    </categories>
    
    
    <tags>
      
      <tag>图像增强与图像恢复</tag>
      
      <tag>高光谱图像去噪</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>论文阅读-0x17</title>
    <link href="/2021/05/10/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-0x17/"/>
    <url>/2021/05/10/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-0x17/</url>
    
    <content type="html"><![CDATA[<p>论文的阅读笔记：</p><p>《 Localization Distillation for Object Detection 》, <a href="https://github.com/HikariTJU/LD">[code]</a>, CVPR 2021</p><span id="more"></span><h2 id="localization-distillation-for-object-detection">LocalizationDistillation for Object Detection</h2><h3 id="abstract">Abstract</h3><ul><li>知识蒸馏(Knowledge distillation,KD)在深度学习领域对学习<strong>紧凑模型</strong>具有强大的能力，但在提取目标定位信息用于目标检测方面仍存在局限性</li><li>现有的目标检测KD方法主要是<strong>模仿教师模型和学生模型之间的深度特征</strong>，不仅受具体模型架构的限制，而且无法提取定位模糊度localizationambiguity</li><li>本文首先提出了基于定位蒸馏 localization distillation (LD)的目标检测方法<ul><li>LD可以通过<strong>采用边界框的通用定位表示</strong>来表示为标准KD</li><li>LD非常灵活，适用于提取任意架构的教师模型和学生模型的定位歧义</li><li>此外，有趣的是，Self-LD，即提取教师模型本身，可以进一步提高当前模型水平</li></ul></li><li>其次提出一种<strong>教师助理(TA)策略来填补教师模型和学生模型之间可能存在的差距</strong>，这样即使所选的教师模型不是最优的，也能保证蒸馏效果</li><li>包围盒回归Bounding box regression是目标检测中最常用的定位方式，其中Dirac delta分布表示直观<ul><li>然而定位歧义是非常普遍的，<strong>对象不能确定定位边缘</strong></li><li>为了对歧义进行建模，定位质量可以通过预测质量得分来衡量，这通常与分类或后处理NMS相结合<ul><li>这里NMS就是非极大值抑制（Non-Maximum Suppression，NMS）</li></ul></li></ul></li><li>在定位表示方面，建议用边界盒的高斯分布来模拟每个边的不确定性<ul><li>最近进一步提出了边界盒的一般分布，即边缘用离散的概率分布表示，没有任何先验约束。尽管有明显的提高，但仍有一定的余地来提高模糊边缘的定位质量</li></ul></li><li>本文提出知识精馏是提高目标检测定位质量的一种潜在方法，它可以<strong>通过优秀的教师模型捕获定位歧义，然后提炼出来训练学生模型</strong></li><li>现有的KD方法主要是加强教师模型和学生模型之间深度特征的一致性<ul><li>不过很多方法都是针对于专门设计的体系结构，不能明确地提取定位歧义</li><li>有一个工作是Teacher Bounded Regression (TBR) loss ，当学生的预测比教师的预测差一个阈值ε时，会受到额外的回归惩罚</li><li><span class="math inline">\(\mathcal{L}_{\mathrm{add}}=\lambda\mathcal{L}_{\mathrm{reg}}\left(\boldsymbol{b}_{s}, \boldsymbol{b}^{gt}\right)\)</span>, if <span class="math inline">\(\ell_{2}\left(\boldsymbol{b}_{s},\boldsymbol{b}^{g t}\right)+\varepsilon \leqslant\ell_{2}\left(\boldsymbol{b}_{t}, \boldsymbol{b}^{g t}\right)\)</span><ul><li>这里的 <span class="math inline">\(\mathcal{L}_{\mathrm{reg}}\)</span>是包围盒回归损失， <span class="math inline">\(\lambda\)</span>是平衡参数</li></ul></li><li>虽然不局限于特定的体系结构，但TBR损失不能提取定位歧义</li></ul></li><li>本文首先提出了基于定位蒸馏(LD)的目标检测方法。<strong>基于边界框的一般分布，LD可以表述为标准KD</strong>，通过该方法，<strong>教师模型捕获的定位歧义可以很好地提炼为学生模型</strong><ul><li>与单一的ground-truthbox注释相比，LD在培养学生模型时可以提供额外的本地化信息</li></ul></li><li><img src="/2021/05/10/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-0x17/1.png"><ul><li>主干ResNet-34的GFocal预测出了冗余box。由于“bird”与背景之间的相似性，出现了多个冗余框的定位歧义，这在默认的NMS中是无法抑制的</li><li>使用教师检测器GFocal以ResNet-101-DCN作为主干蒸馏ResNet-34后，预测的边界框都具有较高的定位质量，并且默认NMS可以轻松抑制多余的框以提高对象检测性能</li><li>而且，有趣的是发现Self-LD，即<strong>提炼教师模型本身，可以进一步增强最新的检测器</strong>，在该检测器中可以进一步捕获定位歧义</li></ul></li><li>LD的第二个问题是如何选择好的教师模式。对教师模型的直观选择是准确率最高的检测器<ul><li>但<strong>当学生和老师之间的差距太大时，LD可能就没有那么有效了</strong></li><li>为了解决这个问题，我们进一步为LD引入了助教（TA）策略。通过引入几种中间助理模型，可以弥补学生模型和教师模型之间可能存在的差距，因此带有TA的LD对教师模型的选择不敏感</li></ul></li><li>将本文的贡献总结如下<ul><li>定位精馏是一种提取定位歧义的方法，适用于任何结构的检测器。据我们所知，LD是第一个明确地将知识蒸馏应用于目标检测的定位分支的工作</li><li>LD不仅可以帮助轻量级学生检测器的培训，而且可以通过使用Self-LD进一步提高最先进的检测器本身</li><li>在教师模型不是最优的情况下，建议采用教师辅助策略保持LD的精馏效果</li></ul></li></ul><h3 id="related-work">Related Work</h3><ul><li>Bounding Box Regression<ul><li>最近包围盒表示从Diracdelta分布发展到高斯分布，并进一步发展到一般分布。边界盒的一般分布对于描述边界盒的不确定性更为全面。<strong>基于包围盒的一般表示，建立了一种标准的定位精馏机制，用于提取定位歧义</strong></li></ul></li><li>Localization Quality Estimation<ul><li>定位质量估计(LQE)预测一个分数，该分数衡量检测器预测的边界框的定位质量</li><li>一方面，LQE在训练过程中通常用于配合分类任务，即<strong>增强分类与定位的一致性</strong></li><li>另一方面，LQE可以应用于后处理时的联合决策，即<strong>在执行NMS时既考虑分类评分又考虑LQE</strong></li></ul></li><li>Knowledge Distillation<ul><li>知识精馏应用于越来越多的深度学习领域，其目的通常是<strong>在优秀教师模式的引导下学习紧凑高效的学生模式</strong></li><li>在定位歧义方面，所有这些KD方法都只是从<strong>特征模仿的角度</strong>提高了学生的表现</li><li>唯一的例外是TBR损失，它只在 学生的成绩在某种程度上低于老师时惩罚网络</li><li>TBR损失显然是一个初步的解决方案，它不能提取定位歧义</li><li>本文首次提出了定位精馏的概念，该方法不仅可以改进紧凑的学生模型，而且可以提高检测器的性能</li></ul></li></ul><h3 id="the-proposed-method">The Proposed Method</h3><h4 id="revisit-of-knowledge-distillation">Revisit of KnowledgeDistillation</h4><ul><li>KD最初是为了分类和识别而开发的，其中<strong>学生模型可以通过模仿更大的教师模型的软输出soft output 来改进</strong></li><li>设 <span class="math inline">\(\boldsymbol{T}\)</span> 为教师模型，<span class="math inline">\(\boldsymbol{S}\)</span> 为学生模型，<span class="math inline">\(z_T, z_S \in \mathbb {R}^n\)</span> 分别为 <span class="math inline">\(\boldsymbol{T}\)</span> 和 <span class="math inline">\(\boldsymbol{S}\)</span> 的<strong>输出对数outputlogits</strong><ul><li>对于分类任务，使用SoftMax函数将这些对数 <span class="math inline">\(z_T, z_S\)</span> 转换为它们对应的概率分布 <span class="math inline">\(p_T, p_S\)</span></li></ul></li><li>在KD中，教师模型 <span class="math inline">\(\boldsymbol{T}\)</span>是重量级的，并且通常在大规模数据集上进行训练以实现较高的准确性，然后可以使用该模型来提炼轻量级学生模型<span class="math inline">\(\boldsymbol{S}\)</span> 的训练</li><li>学生模型 <span class="math inline">\(\boldsymbol{S}\)</span>的目的是模仿两者 ground-trurh 标签 <span class="math inline">\(\boldsymbol{g}\)</span> 和 soft label 软标签 <span class="math inline">\(p_T\)</span> ，通过最小化以下损失函数<ul><li><span class="math inline">\(\mathcal{L}_{W_{S}}=\lambda_{1}\mathcal{L}_{\mathrm{CE}}\left(\boldsymbol{p}_{S},\boldsymbol{g}\right)+\lambda_{2}\mathcal{L}_{\mathrm{KL}}\left(\boldsymbol{p}_{S}^{\tau},\boldsymbol{p}_{T}^{\tau}\right)\)</span><ul><li>其中 <span class="math inline">\(\tau\)</span> 是蒸馏温度， <span class="math inline">\(\mathcal{L}_{\mathrm{CE}},\mathcal{L}_{\mathrm{KL}}\)</span>是交叉熵损失和KL散度损失</li><li>在学习学生模型的过程中，只更新 <span class="math inline">\(\boldsymbol{S}\)</span> 的参数 <span class="math inline">\(\boldsymbol{W}_S\)</span>，冻结老师模型的参数</li></ul></li></ul></li><li>一般情况下，模型压缩通常采用KD来减少推理时的计算负担，即 <span class="math inline">\(\boldsymbol{W}_S\)</span> 一般比 <span class="math inline">\(\boldsymbol{W}_T\)</span> 轻</li><li>除了使用 ground-trurh 标签，soft label 软标签 <span class="math inline">\(p_T\)</span>能够更好地转移教师模型获取的知识，从而引入 <span class="math inline">\(\tau &gt; 0\)</span> 来软化 <span class="math inline">\(\boldsymbol{p}_S\)</span> 和 <span class="math inline">\(\boldsymbol{p}_T\)</span> 的概率分布<ul><li><span class="math inline">\(\begin{aligned}\boldsymbol{p}_{T}^{\tau} &amp;=\mathcal{S}\left(\boldsymbol{z}_{T},\tau\right)=\operatorname{SoftMax}\left(\boldsymbol{z}_{T} /\tau\right), \\ \boldsymbol{p}_{S}^{\tau}&amp;=\mathcal{S}\left(\boldsymbol{z}_{S},\tau\right)=\operatorname{SoftMax}\left(\boldsymbol{z}_{S} / \tau\right)\end{aligned}\)</span><ul><li>这里注意到，如果 <span class="math inline">\(\tau = 1\)</span>，其实就是原始的SoftMax函数</li><li>当 <span class="math inline">\(\tau \rightarrow 1\)</span>时，趋于Diracdelta分布：只有一个位置概率最高为1，其余位置的概率为零</li><li>当 <span class="math inline">\(\tau \rightarrow 1\)</span>时，趋于均匀分布：所有位置的概率 <span class="math inline">\(p_{i}=\frac{1}{n}, \forall i\)</span></li><li>根据经验， <span class="math inline">\(\tau &gt; 1\)</span>被设置为5、10和20，以软化分布，使 <span class="math inline">\(p_T^{\tau}\)</span> 携带更多信息</li></ul></li><li>可以看这个图，其实就是用一个小型网络去学习大网络学习的数据分布知识</li><li><img src="/2021/05/10/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-0x17/2.png"></li></ul></li></ul><h4 id="localization-distillation">Localization Distillation</h4><ul><li>虽然KD算法在分类中取得了很大的成功，但在目标检测中提取定位信息却非常具有挑战性</li><li>对于包围盒 <span class="math inline">\(\mathcal{B}\)</span>，常规表示有两种形式:{x, y, w, h}和{t, B, l, r}(采样点到 <span class="math inline">\(\mathcal{B}\)</span>的上、下、左、右边缘的距离)</li><li>传统的两种表示形式实际上都遵循Diracdelta分布，只关注ground-truth位置，不能像图1所示的那样对边界盒的歧义进行建模</li><li>为了建模歧义，提出了高斯分布表示，通过引入额外的方差来建模每个边缘的不确定性，但是高斯分布的定位信息不能反映边界框的真实分布</li><li>我们利用 <span class="math inline">\(\mathcal{B} = {t, B, l,r}\)</span> 作为边界框的基本表示。假设 <span class="math inline">\(e \in\mathcal{B}\)</span> 是包围盒的一条边，其值一般可以表示为<ul><li><span class="math inline">\(\hat{e}=\int_{e_{\min }}^{e_{\max }} x\operatorname{Pr}(x) d x, \quad e \in \mathcal{B}\)</span>,<ul><li>其中x是在[emin, emax]范围内的回归坐标，Pr(x)是对应的概率</li></ul></li></ul></li><li>传统的Dirac delta表示是上式的一种特例，其中当 <span class="math inline">\(x = e^{gt}\)</span> 时 <span class="math inline">\(Pr(x) = 1\)</span> ，否则 <span class="math inline">\(Pr(x) = 0\)</span> ，即只关注ground-truth edge<span class="math inline">\(e^{gt}\)</span></li><li>所以与Diracdelta表示相比，一般表示非常灵活，可以<strong>用概率分布来模拟边缘的不确定性</strong></li><li>为了满足深度模型的离散输出，引入了一般分布的离散版本。将回归范围[emin,emax]划分为n个子区间<ul><li><span class="math inline">\(e=\left[e_{1}, e_{2}, \cdots,e_{n}\right]^{\mathrm{T}} \in \mathbb{R}^{n}\)</span> 其中 <span class="math inline">\(e_{1}=e_{\min },e_{n}=e_{\max }\)</span></li></ul></li><li>为简便起见，对 <span class="math inline">\(\forall i, j\)</span>采用均匀除法 <span class="math inline">\(\Delta_{i}=e_{i+1}-e_{i}\)</span> and <span class="math inline">\(\Delta_{i}=\Delta_{j}\)</span></li><li>为了表示每个位置的概率，采用了SoftMax函数。特别地，回归模型对 <span class="math inline">\(e\)</span> 边缘的所有可能位置预测n logitsz。使用SoftMax将它们转换为概率 <span class="math inline">\(p =SoftMax(z)\)</span></li><li>这样的话，预测回归值为<ul><li><span class="math inline">\(\hat{e}=\boldsymbol{e}^{\mathrm{T}}\boldsymbol{p}=\sum_{i=1}^{n} e_{i} \operatorname{Pr}\left(e_{i}\right),\quad e \in \mathcal{B}\)</span>,</li></ul></li><li>由此可计算出包围盒 <span class="math inline">\(\mathcal{B}\)</span>的四条边为 <span class="math inline">\(\hat{\boldsymbol{b}}=[\hat{t},\hat{b}, \hat{l}, \hat{r}]^{\mathrm{T}}\)</span> ，然后在预测盒 <span class="math inline">\(\hat{\boldsymbol{b}}\)</span> 和ground-truth box<span class="math inline">\(b^{gt}\)</span> 之间计算包围盒回归损失</li><li>同时 <span class="math inline">\(\mathcal{B}\)</span>的每条边都可以表示为一个概率分布，即包围盒 <span class="math inline">\(\mathcal{B}\)</span> 可以表示为矩阵 <span class="math inline">\(\boldsymbol{B}=\left[\boldsymbol{p}_{t},\boldsymbol{p}_{b}, \boldsymbol{p}_{l}, \boldsymbol{p}_{r}\right] \in\mathbb{R}^{n \times 4}\)</span></li><li>给定边 <span class="math inline">\(x\)</span> 的输出对数 <span class="math inline">\(z_T\)</span> 和 <span class="math inline">\(z_S\)</span> , <span class="math inline">\(\boldsymbol{T}\)</span> 和 <span class="math inline">\(\boldsymbol{S}\)</span> 的边可以用概率分布 <span class="math inline">\(p_T\)</span> 和 <span class="math inline">\(p_S\)</span> 表示。类似于KD，边 <span class="math inline">\(e\)</span> 的定位知识可以通过kl -散度损失来提取<ul><li><span class="math inline">\(\mathcal{L}_{\mathrm{LD}}^{e}=\mathcal{L}_{\mathrm{KL}}\left(\boldsymbol{p}_{S}^{\tau},\boldsymbol{p}_{T}^{\tau}\right)=\mathcal{L}_{\mathrm{KL}}\left(\mathcal{S}\left(\boldsymbol{z}_{S},\tau\right), \mathcal{S}\left(\boldsymbol{z}_{T},\tau\right)\right)\)</span></li></ul></li><li>最后，包围盒B的所有四条边的LD可以表示为<ul><li><span class="math inline">\(\mathcal{L}_{\mathrm{LD}}\left(\boldsymbol{B}_{S},\boldsymbol{B}_{T}\right)=\sum_{e \in \mathcal{B}}\mathcal{L}_{\mathrm{LD}}^{e}\)</span></li></ul></li><li><img src="/2021/05/10/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-0x17/2.png"></li><li>图1中的模糊边对应的是平坦的位置分布，泛化知识也可以从教师模型提炼到学生模型。因此，如果教师已经很好地掌握了定位歧义，通过定位精馏将这种能力转移到学生模型中是一种自然的方法</li><li>总损失：<ul><li><span class="math inline">\(\mathcal{L}_{W_{S}}=\lambda_{1}\mathcal{L}_{\text {reg }}\left(\hat{b}_{S}, b^{g t}\right)+\lambda_{2}\mathcal{L}_{\text {DFL }}\left(B_{S}\right)+\lambda_{3}\mathcal{L}_{\mathrm{LD}}\left(\boldsymbol{B}_{S},\boldsymbol{B}_{T}\right)\)</span><ul><li>前两项和GFocal模型完全相同</li><li><strong>所以上面说了这么多，其实就是最后这一部分，作者把边界信息用一种概率形式来表示，从而达到一种可以学习的状态，就是可能得到它中间的软输出，从而来向学生模型去传递这种学习信息，其实这样做也主要是为了针对作者提出的最初的那个定位歧义的问题</strong></li></ul></li></ul></li><li>在KD中，学生模型S一般比教师模型T轻，这在学习紧凑高效的模型中很有用。相反，一个有趣的问题是这种“潜在知识”是否有利于教师模式本身?<strong>自蒸馏在分类中一直被观察到有积极的作用</strong><ul><li>对于目标检测，也可以看到S =T的Self-LD也可以为教师模型带来绩效收益，提高最先进的绩效</li><li>由于自蒸馏可以促进模型精度的原因，[34]首先通过对希尔伯特空间中的自蒸馏进行理论分析，揭开了这一神秘面纱。证明了几轮自精馏可以减少过拟合，因为它能引起正则化。然而，持续的自我蒸馏可能会导致不适合</li></ul></li><li>Teacher Assistant Strategy<ul><li>在KD模型中，教师模型T被认为是最大的模型，并且期望能更好地提取出最小的学生模型s</li><li>然而，LD中观察到悖论的例子，其中最好的模型不如折中的模型。在最近的工作中也提到了这种现象，即学生模型和教师模型之间的差距太大，蒸馏的好处就会减少</li><li>因此，受[33]的启发，引入教师辅助策略，即使教师模型不是最优选择，也能保持LD的有效性</li><li><img src="/2021/05/10/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-0x17/3.png"></li><li>我们选择m名助教候选人，分别表示为A1,A2，···，Am，按模型容量递减</li><li>从上图可以看出，有完全2条不同的LD路径。如果不使用任何教师助手，这意味着我们要经过最上面的路径，这相当于直接执行LD从教师T到学生S(最坏的情况)</li><li>然后，可以选择使用一个或两个教师助理，或任意k &lt;m教师助理，或所有m助理(底部路径)</li><li>根据[33]中的结论设置教师助理策略<ul><li>如果没有培训时间限制，提高学生的最佳顺序是使用所有的教师助理</li><li>在限制使用k &lt;m个教师助理的情况下，学生的成绩一般会在上图中从上到下的路径范围内</li></ul></li></ul></li></ul>]]></content>
    
    
    <categories>
      
      <category>study</category>
      
    </categories>
    
    
    <tags>
      
      <tag>目标检测</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>论文阅读-0x16</title>
    <link href="/2021/04/25/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-0x16/"/>
    <url>/2021/04/25/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-0x16/</url>
    
    <content type="html"><![CDATA[<p>论文的阅读笔记：</p><p>《 Representative Batch Normalization with Feature Calibration 》, <a href="https://github.com/ShangHua-Gao/RBN">[code]</a>, CVPR 2021</p><p>（组会分享）</p><span id="more"></span><h2 id="representative-batch-normalization-with-feature-calibration">RepresentativeBatch Normalization with Feature Calibration</h2><h3 id="abstract">Abstract</h3><ul><li><p>批归一化(Batch Normalization,BatchNorm)已成为现代神经网络稳定训练的默认组件</p><ul><li>在BatchNorm中，centering and scaling operations（中心化以及缩放操作），对应的均值和方差统计信息，被用于batch寸尺上的特征标准化</li></ul></li><li><p>BatchNorm的批处理依赖性使得网络的训练更加稳定，并且可以更好的表示，但不可避免的忽略了实例之间的表示差异</p></li><li><p>作者建议在BatchNorm的中心化和缩放操作中添加一个简单而有效的<strong>特征校准方案</strong>（featurecalibrationscheme），<strong>以可忽略的计算代价增强特定实例的表示</strong></p><ul><li>中心化校准centering calibration (CC)<strong>增强了信息特征，减少了噪声特征</strong></li><li>缩放校准scale calibration (SC)限制了特征强度，<strong>使特征分布更加稳定</strong></li></ul></li><li><p>提出的BatchNorm的变体——RepresentativeBatchNorm，可以插入到现有的方法中，以提高各种任务的性能，如分类、检测和分割等</p></li><li><p>随着神经网络结构复杂度和模型参数的增长，其训练难度也越来越大。Batchnormalization(BatchNorm)通过将中间特征限制在mini-batch统计信息的归一化分布内，降低了训练难度</p></li><li><p>在BatchNorm中，对mini-batch信息的依赖建立在这样一个假设之上，即<strong>不同实例对应得到的特性都服从相同分布</strong>。然而，这一假设并不总是适用于以下两种情况：</p><ul><li>当前mini-batch的统计信息和总体训练集/测试集中的统计信息可能不一致</li><li>测试集中的实例不满足训练集的分布</li></ul></li><li><p><img src="/2021/04/25/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-0x16/3.png"></p><ul><li>在这种情况下：</li><li>如果测试实例特征<strong>均值低于训练均值(running_mean)</strong>：BN会错误地删除部分代表性特征（上图a）</li><li>如果测试实例特征<strong>均值高于训练均值</strong>：BN会保持对小值噪声的激活（误认，上图b）</li><li>如果测试实例特征<strong>方差与训练均值(running_var)存在误差</strong>：导致通道间特征分布不稳定（上图cd）<ul><li>假设在测试过程中，一个通道的比例特征比同一层中的其他通道大得多。该通道的特征将支配下一个卷积层产生的特征</li></ul></li></ul></li><li><p>为了避免这两种不一致性带来的副作用，一些工作<strong>利用特定实例的统计信息</strong>而不是小批量统计信息来规范化中间特性</p><ul><li>然而，由于缺乏批处理信息，训练的不稳定性使得它们的性能在很多情况下不如BatchNorm</li></ul></li><li><p>其他工作通过结合多种规范化技术或引入注意机制来利用mini-batch处理和实例统计信息</p><ul><li>然而，这些方法通常会引入更多的开销，使其在实际使用中不友好</li></ul></li><li><p>为此作者提出了一种简单而有效的特征标定方案——Representative BatchNormalization(RBN)，以可以忽略不计的代价标定BatchNorm的特征标准化操作，在保持BatchNorm优点的同时减少一些不适当的运行统计信息所带来的副作用，使用<strong>特定于实例的统计信息以可以忽略不计的成本来校准中心化和缩放操作</strong></p><ul><li>中心化校准，通过移除特定实例的特征统计信息，以加强代表性特征和减少噪声特征</li><li>缩放校准，用统计方法对特征强度进行了相应的尺度变换，得到了较稳定的特征分布</li><li>这两种校准在每个通道中只引入三个权重，需要可以忽略不计的计算成本</li></ul></li><li><p>这里作者思想就是将基于特定实例的特征和mini-batch的统计信息结合起来，从而达到一个稳定训练分布并且能够根据特定实例的特征来尽量避免不一致性情况的发生</p></li><li><p>Statistics for Normalization.</p><ul><li>从不同维度和区域计算的统计数据用于特征归一化</li><li>当mini-batch统计数据特别不准确，即训练批数过小时，采用mini-batch独立统计数据（就是最传统的norm吧，然后没有那俩学习参数）进行归一化可以提高模型的稳定性。然而，由于缺乏批处理信息，训练的不稳定性使得它们的性能在很多情况下不如BatchNorm</li></ul></li><li><p>Combinations of Multiple Dimensional Normalization.</p><ul><li>多重归一化组合方法通常需要额外的计算代价来归一化不同维度之间的特征</li></ul></li><li><p>Alternatives of Standardization.</p><ul><li>作者提出的校准方案也可以应用于这些标准化的替代方案</li></ul></li><li><p>Conditional Transformation.</p><ul><li>一些工作修改了仿射变换，条件为任务或实例指定的属性；引入了注意机制来生成仿射变换的权值，形成了一个更广义的变换</li></ul></li></ul><h3 id="method">Method</h3><h4 id="revisiting-batch-normalization">Revisiting BatchNormalization</h4><ul><li><p>归一化可以加快网络的训练速度，提高学习的稳定性，并且能够对数据做去相关性，突出分布的相对差</p></li><li><p>BatchNorm就是一种归一化方法，能够减小图像之间的绝对差异，突出相对差异，加快训练速度</p></li><li><p><img src="/2021/04/25/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-0x16/1.png"></p><ul><li>对于上图中的分布，如果我们将其归一化到0点附近，显然会加快训练速度，如此我们更进一步的通过变换拉大数据之间的相对差异性，那么就更容易区分</li></ul></li><li><p>BatchNorm由特征中心化、特征缩放和仿射变换（affinetransformation）操作组成</p></li><li><p>给定输入 <span class="math inline">\(\mathbf{X} \in \mathbb{R}^{N\times C \times H \times W}\)</span> ，仿射变换可写为</p><ul><li>Centering <span class="math inline">\(:\mathbf{X}_{m}=X-\mathrm{E}(\mathbf{X})\)</span></li><li>Scaling <span class="math inline">\(:\mathbf{X}_{s}=\frac{\mathbf{X}_{m}}{\sqrt{\operatorname{Var}(\mathbf{X})+\epsilon}}\)</span></li><li>Affine : <span class="math inline">\(\mathbf{Y}=\mathbf{X}_{s}\gamma+\beta\)</span><ul><li>其中 <span class="math inline">\(E(X),Var(X)\)</span>是均值和方差，用于中心化和缩放</li><li><span class="math inline">\(\gamma , \beta\)</span>是用于仿射变换的学习参数，<span class="math inline">\(\varepsilon\)</span> 用于避免零方差</li></ul></li></ul></li><li><p>在训练过程中，mini-batch 的均值和方差记为：</p><ul><li><span class="math inline">\(\begin{aligned} \mu_{B} &amp;=\frac{1}{NH W} \sum_{n=1}^{N} \sum_{h=1}^{H} \sum_{w=1}^{W} \mathbf{X}_{(n, c, h,w)} \\ \sigma_{B}^{2} &amp;=\frac{1}{N H W} \sum_{n=1}^{N}\sum_{h=1}^{H} \sum_{w=1}^{W}\left(\mathbf{X}_{(n, c, h,w)}-\mu_{B}\right)^{2} . \end{aligned}\)</span></li><li>其中 <span class="math inline">\(E(X),Var(X)\)</span>的统计量在训练过程中在数据集上会不断累积，而在测试期间保持不变</li></ul></li><li><p>运行均值和方差：</p><ul><li><span class="math inline">\(\begin{aligned} \mathrm{E}(\mathbf{X})&amp; \Leftarrow m \mathrm{E}(\mathbf{X})+(1-m) \mu_{B} \\\operatorname{Var}(\mathbf{X}) &amp; \Leftarrow m\operatorname{Var}(\mathbf{X})+(1-m) \sigma_{B}^{2}\end{aligned}\)</span></li><li><span class="math inline">\(m\)</span> 表示一个动量</li></ul></li><li><p>然后训练可学习参数 <span class="math inline">\(\gamma,\beta\)</span> 来拟合由批统计数据归一化的特征</p></li><li><p>但是，mini-batch处理和运行统计信息不能严格对齐，并且测试实例可能不总是适合在训练期间积累的运行分布。因此，训练和测试过程的不一致性削弱了BatchNorm的作用</p></li><li><p><img src="/2021/04/25/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-0x16/2.png"></p></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">MyBN</span>:<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, momentum, eps, num_features</span>):<br>        <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">        初始化参数值</span><br><span class="hljs-string">        :param momentum: 追踪样本整体均值和方差的动量</span><br><span class="hljs-string">        :param eps: 防止数值计算错误</span><br><span class="hljs-string">        :param num_features: 特征数量</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br>        <span class="hljs-comment"># 对每个batch的mean和var进行追踪统计</span><br>        self._running_mean = <span class="hljs-number">0</span><br>        self._running_var = <span class="hljs-number">1</span><br>        <span class="hljs-comment"># 更新self._running_xxx时的动量</span><br>        self._momentum = momentum<br>        <span class="hljs-comment"># 防止分母计算为0</span><br>        self._eps = eps<br>        <span class="hljs-comment"># 对应论文中需要更新的beta和gamma，采用pytorch文档中的初始化值</span><br>        self._beta = np.zeros(shape=(num_features, ))<br>        self._gamma = np.ones(shape=(num_features, ))<br>      <br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">batch_norm</span>(<span class="hljs-params">self, x</span>):<br>        <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">        BN向传播</span><br><span class="hljs-string">        :param x: 数据</span><br><span class="hljs-string">        :return: BN输出</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br>        x_mean = x.mean(axis=<span class="hljs-number">0</span>)<br>        x_var = x.var(axis=<span class="hljs-number">0</span>)<br>        <span class="hljs-comment"># 对应running_mean的更新公式</span><br>        self._running_mean = (<span class="hljs-number">1</span>-self._momentum)*x_mean + self._momentum*self._running_mean<br>        self._running_var = (<span class="hljs-number">1</span>-self._momentum)*x_var + self._momentum*self._running_var<br>        <span class="hljs-comment"># 对应论文中计算BN的公式</span><br>        x_hat = (x-x_mean)/np.sqrt(x_var+self._eps)<br>        y = self._gamma*x_hat + self._beta<br>        <span class="hljs-keyword">return</span> y<br></code></pre></td></tr></table></figure><h4 id="representative-batch-normalization">Representative BatchNormalization</h4><ul><li>目标：增强特定于实例的表示，并维护BatchNorm的优点</li><li>主要研究：由特征中心化和特征缩放组成的特征标准化操作</li><li>提出的RBN，配备了简单而有效的特征校准方案，增强了实例指定的特征，并产生了更稳定的特征分布</li><li>Statistics for Calibration（统计数据校准）<ul><li>校准BatchNorm中的运行统计信息需要特定实例特性的统计信息</li><li>本文主要研究信道维度上的统计，因为BatchNorm是用来统计信道上的统计的，特征通道的通道维数统计量：平均µc和方差σ2c分别为</li><li><span class="math inline">\(\begin{aligned} \mu_{c} &amp;=\frac{1}{HW} \sum_{h=1}^{H} \sum_{w=1}^{W} \mathbf{X}_{(n, c, h, w)} \\\sigma_{c}^{2} &amp;=\frac{1}{H W} \sum_{h=1}^{H}\sum_{w=1}^{W}\left(\mathbf{X}_{(n, c, h, w)}-\mu_{c}\right)^{2} .\end{aligned}\)</span></li></ul></li><li>Centering Calibration（中心化校准）<ul><li><img src="/2021/04/25/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-0x16/3.png"><ul><li>在图像上画一条线，沿着这条线采样特征强度</li><li>在这种情况下，直接将特征中心化作为运行的平均值可能会丢失信息表示(上图(a))或引入额外的噪声(上图2(b))</li><li>为了减少对定心操作对运行均值的依赖，作者增加了一个由实例统计数据驱动的中心化校准方案</li></ul></li><li>运行平均值计算训练数据集上通道的平均统计数据</li><li>当忽略仿射变换的影响时，在后面的激活层之后，保留大于运行均值的特征，反之亦然。但当特征变化较大时，信道的运行平均值可能不准确</li><li>给定输入特征 <span class="math inline">\(\mathbf{X}\)</span>，中心化校准如下：<ul><li><span class="math inline">\(\mathbf{X}_{c m(n, c, h,w)}=\mathbf{X}_{(n, c, h, w)}+w_{m} \odot \mathbf{K}_{m}\)</span></li><li>其中 <span class="math inline">\(w_m \in \mathbb{R}^{1 \times C\times 1 \times 1}\)</span> 是一个学习向量，<span class="math inline">\(\mathbf{K}_{m}\)</span> 是特征 <span class="math inline">\(\mathbf{X}\)</span> 的统计信息，这里作者设置 <span class="math inline">\(\mathbf{K}_{m}\)</span> 为<span class="math inline">\(\mu_{c} \in \mathbb{R}^{N \times C \times 1 \times1}\)</span> （这就是一个全局平均池化）</li><li><span class="math inline">\(\odot\)</span>是点积运算，将两个特征转换到同一形状，然后进行点积，为了简化符号，用<span class="math inline">\(\cdot\)</span> 代替 <span class="math inline">\(\odot\)</span></li><li>上式中的 <span class="math inline">\(w_{m} \cdot\mathbf{K}_{m}\)</span> 引入了特定实例信息，其中可学习的权重 <span class="math inline">\(w_m\)</span>，通过平衡mini-batch和具体实例的统计数据来校准中心化操作</li></ul></li><li>假设中心化校准前后的运行均值分别为 <span class="math inline">\(\mathrm{E}(\mathbf{X}),\mathrm{E}(\mathbf{X}_{cm})\)</span>， <span class="math inline">\(\mathbf{K}_{m}\)</span> 设置为 <span class="math inline">\(\mu_{c}\)</span> 时， <span class="math inline">\(\mathbf{K}_{m}\)</span> 的运行均值就是 <span class="math inline">\(\mathrm{E}(\mathbf{X})\)</span></li><li>所以 <span class="math inline">\(\mathrm{E}(\mathbf{X}),\mathrm{E}(\mathbf{X}_{cm})\)</span>两者的关系其实就是加了一个学习参数<ul><li><span class="math inline">\(\mathrm{E}\left(\mathbf{X}_{cm}\right)=\left(1+w_{m}\right) \cdot\mathrm{E}(\mathbf{X})\)</span></li></ul></li><li>有无中心化校准操作的区别如下：<ul><li><span class="math inline">\(\mathbf{X}_{c a l}=\mathbf{X}_{cm}-\mathrm{E}\left(\mathbf{X}_{c m}\right)\)</span></li><li><span class="math inline">\(\mathbf{X}_{no}=\mathbf{X}-\mathrm{E}(\mathbf{X})\)</span></li><li><span class="math inline">\(\mathbf{X}_{c a l}-\mathbf{X}_{no}\)</span> <span class="math inline">\(=\left(\mathbf{X}_{cm}-\mathrm{E}\left(\mathbf{X}_{cm}\right)\right)-(\mathbf{X}-\mathrm{E}(\mathbf{X}))\)</span> <span class="math inline">\(=\mathbf{X}+w_{m} \cdot\mathbf{K}_{m}-\left(1+w_{m}\right) \cdot\mathrm{E}(\mathbf{X})-(\mathbf{X}-\mathrm{E}(\mathbf{X}))\)</span><span class="math inline">\(=w_{m}\cdot\left(\mathbf{K}_{m}-\mathrm{E}(\mathbf{X})\right)\)</span></li></ul></li><li>这里的 <span class="math inline">\(\mathbf{K}_{m}\)</span> 因为就是<span class="math inline">\(\mu_{c}\)</span> ，就是对HW求均值， <span class="math inline">\(\mathrm{E}(\mathbf{K}_{m})\)</span>就是在batch上再求一次均值，所以说 <span class="math inline">\(\mathrm{E}(\mathbf{K}_{m}) =\mathrm{E}(\mathbf{X})\)</span></li><li>当学习向量 <span class="math inline">\(w_m\)</span>接近于零的时候，中心化操作仍然依赖于运行统计信息；相反，如果 <span class="math inline">\(w_m\)</span>这一项过大的话，特定实例特征的重要性就会增加</li><li>考虑 <span class="math inline">\(w_{m} \cdot \mathbf{K}_{m}\)</span>这一项，有以下情况：<ul><li>在 <span class="math inline">\(w_{m} &gt; 0\)</span> 条件下，当<span class="math inline">\(\mathbf{K}_{m} &gt;\mathrm{E}(\mathbf{X})\)</span> ：代表性特征趋于激活增强，反之亦然</li><li>在 <span class="math inline">\(w_{m} &lt; 0\)</span> 条件下，当<span class="math inline">\(\mathbf{K}_{m} &gt;\mathrm{E}(\mathbf{X})\)</span> ：噪声特征趋于激活抑制，反之亦然</li><li>当 <span class="math inline">\(w_{m} \cdot \mathbf{K}_{m} &gt;0\)</span> 时，如上图a，特征得到增强从而更好地来表示猫的部分</li><li>当 <span class="math inline">\(w_{m} \cdot \mathbf{K}_{m} &lt;0\)</span> 时，如上图b，背景特征得到抑制从而减少猫部分中的噪声</li></ul></li><li><img src="/2021/04/25/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-0x16/4.png"></li><li>此外，在上图中可以观察到，训练模型的某些层的 <span class="math inline">\(w_m\)</span>接近于零，这表明中心化校准可以通过训练同时利用批统计和实例统计</li></ul></li><li>Scaling Calibration（缩放校准）<ul><li>不同于中心化操作，其决定了激活后保留的特征，而<strong>缩放操作只改变特征的强度</strong>，忽略了仿射变换的影响</li><li>不过，如果对运行方差缩放不准确，会导致特征强度不稳定，即某些信道的特征比其他信道的特征大得多</li><li>作者对此提出了基于实例统计的特征强度标定方法：<strong>在原有缩放操作的基础上增加了缩放校准</strong></li><li>给定特征 <span class="math inline">\(\mathbf{X}_{s}\)</span>，特征校准为：<ul><li><span class="math inline">\(\mathbf{X}_{c s(n, c, h,w)}=\mathbf{X}_{s(n, c, h, w)} \cdot \mathbf{R}\left(w_{v} \odot\mathbf{K}_{s}+w_{b}\right)\)</span></li><li>其中 <span class="math inline">\(w_v,w_b \in \mathbb{R}^{1 \times C\times 1 \times 1}\)</span> 是可学习参数向量， <span class="math inline">\(\mathbf{R}()\)</span>是一个限制函数（可以使用各种形式来定义，这里作者使用了Sigmoid），与中心化校准中的<span class="math inline">\(\mathbf{K}_{m}\)</span> 相同，这里的 <span class="math inline">\(\mathbf{K}_{s}\)</span> 也是实例特征 <span class="math inline">\(\mathbf{X}_{s}\)</span>的统计，也是用全局平均池化得到</li></ul></li><li>限制函数 <span class="math inline">\(\mathbf{R}()\)</span> 与 <span class="math inline">\(w_v,w_b\)</span>的共同作用，会抑制分布特征，使特征分布更加稳定</li><li>（由于缩放操作和缩放校准都不会改变特征的符号，且负特征在激活层后会失效，所以只需要考虑特征为正的部分）</li><li>特征校准后的方差为：<ul><li><span class="math inline">\(\operatorname{Var}\left(\mathbf{X}_{cs}\right)=\operatorname{Var}\left(\mathbf{X}_{s} \cdot\mathbf{R}\left(w_{v} \cdot\mathbf{K}_{s}+w_{b}\right)\right)\)</span></li><li>由于 <span class="math inline">\(0 &lt; \mathbf{R}() &lt; 1\)</span>，必然存在一个 <span class="math inline">\(\tau\)</span> ，满足 <span class="math inline">\(\mathbf{R}() &lt; \tau &lt; 1\)</span> 。因此<span class="math inline">\(\operatorname{Var}\left(\mathbf{X}_{cs}\right)\)</span> 可以放宽为：</li><li><span class="math inline">\(\operatorname{Var}\left(\mathbf{X}_{cs}\right)&lt;\operatorname{Var}\left(\mathbf{X}_{s} \tau\right)=\tau^{2}\operatorname{Var}\left(\mathbf{X}_{s}\right)\)</span></li></ul></li><li><img src="/2021/04/25/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-0x16/5.png"></li><li>可以看出，尺度标定后会将特征方差限制在较小的范围（如上图所示），使通道之间的分布更加稳定</li><li><img src="/2021/04/25/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-0x16/6.png"></li><li><span class="math inline">\(w_v,w_b\)</span>用来控制约束的强度（避免过大的值）和位置，其中 <span class="math inline">\(w_v \leq 1\)</span> 从而使特征落入 <span class="math inline">\(\mathbf{R}()\)</span> 的非饱和区</li></ul></li><li>Implementation of Representative BatchNorm（RBN的实现）<ul><li>Given the input feature <span class="math inline">\(\mathbf{X} \in\mathbb{R}^{N \times C \times H \times W}\)</span> , the formulation ofthe Representative BatchNorm (RBN) is written as:</li><li>Centering Calibration : <span class="math inline">\(\mathbf{X}_{cm}=\mathbf{X} +w_{m} \odot\mathbf{K}_{m}\)</span></li><li>Centering : <span class="math inline">\(\mathbf{X}_{m}=\mathbf{X}_{cm} -\mathrm{E}(\mathbf{X}_{cm})\)</span></li><li>Scaling : <span class="math inline">\(\mathbf{X}_{s}=\frac{\mathbf{X}_{m}}{\sqrt{\operatorname{Var}\left(\mathbf{X}_{cm}\right)+\epsilon}}\)</span></li><li>Scaling Calibration : <span class="math inline">\(\mathbf{X}_{cs}=\mathbf{X}_{s} \cdot \mathbf{R}\left(w_{v} \odot\mathbf{K}_{s}+w_{b}\right)\)</span></li><li>Affine : <span class="math inline">\(\mathbf{Y}=\mathbf{X}_{c s}\gamma+\beta\)</span></li><li>为了在现有深度学习框架中利用BatchNorm的优化，作者分别在BatchNorm原始归一化层的开始和结束处添加了中心化和缩放比例校准</li></ul></li></ul><h4 id="code">Code</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch.nn <span class="hljs-keyword">as</span> nn<br><span class="hljs-keyword">import</span> math<br><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">import</span> torch.nn.functional <span class="hljs-keyword">as</span> F<br><span class="hljs-keyword">class</span> <span class="hljs-title class_">RepresentativeBatchNorm2d</span>(nn.BatchNorm2d):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, num_features, eps=<span class="hljs-number">1e-5</span>, momentum=<span class="hljs-number">0.1</span>, affine=<span class="hljs-literal">True</span>,</span><br><span class="hljs-params">                 track_running_stats=<span class="hljs-literal">True</span></span>):<br>        <span class="hljs-built_in">super</span>(RepresentativeBatchNorm2d, self).__init__(<br>            num_features, eps, momentum, affine, track_running_stats)<br>        self.num_features = num_features<br>        <span class="hljs-comment">### weights for affine transformation in BatchNorm ###</span><br>        <span class="hljs-keyword">if</span> self.affine:<br>            self.weight = nn.Parameter(torch.Tensor(<span class="hljs-number">1</span>, num_features, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>))<br>            self.bias = nn.Parameter(torch.Tensor(<span class="hljs-number">1</span>, num_features, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>))<br>            self.weight.data.fill_(<span class="hljs-number">1</span>)<br>            self.bias.data.fill_(<span class="hljs-number">0</span>)<br>        <span class="hljs-keyword">else</span>:<br>            self.register_parameter(<span class="hljs-string">&#x27;weight&#x27;</span>, <span class="hljs-literal">None</span>)<br>            self.register_parameter(<span class="hljs-string">&#x27;bias&#x27;</span>, <span class="hljs-literal">None</span>)<br><br>        <span class="hljs-comment"># 下面是用于中心化和缩放校准的可学习参数</span><br>        <span class="hljs-comment">### weights for centering calibration ###</span><br>        self.center_weight = nn.Parameter(torch.Tensor(<span class="hljs-number">1</span>, num_features, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>))<br>        self.center_weight.data.fill_(<span class="hljs-number">0</span>)<br>        <span class="hljs-comment">### weights for scaling calibration ###</span><br>        self.scale_weight = nn.Parameter(torch.Tensor(<span class="hljs-number">1</span>, num_features, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>))<br>        self.scale_bias = nn.Parameter(torch.Tensor(<span class="hljs-number">1</span>, num_features, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>))<br>        self.scale_weight.data.fill_(<span class="hljs-number">0</span>)<br>        self.scale_bias.data.fill_(<span class="hljs-number">1</span>)<br>        <span class="hljs-comment"># 基于实例特征的统计信息（用全局平均池化得到）</span><br>        <span class="hljs-comment">### calculate statistics ###</span><br>        self.stas = nn.AdaptiveAvgPool2d((<span class="hljs-number">1</span>,<span class="hljs-number">1</span>))<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, <span class="hljs-built_in">input</span></span>):<br>        self._check_input_dim(<span class="hljs-built_in">input</span>)<br><br>        <span class="hljs-comment">####### centering calibration begin #######</span><br>        <span class="hljs-comment"># X_cm = X + w_m \odot K_m</span><br>        <span class="hljs-built_in">input</span> += self.center_weight.view(<span class="hljs-number">1</span>,self.num_features,<span class="hljs-number">1</span>,<span class="hljs-number">1</span>)*self.stas(<span class="hljs-built_in">input</span>)<br>        <span class="hljs-comment">####### centering calibration end #######</span><br><br>        <span class="hljs-comment">####### BatchNorm begin #######</span><br>        <span class="hljs-comment"># 调用BatchNorm</span><br>        <span class="hljs-comment"># 一个动量</span><br>        <span class="hljs-keyword">if</span> self.momentum <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span>:<br>            exponential_average_factor = <span class="hljs-number">0.0</span><br>        <span class="hljs-keyword">else</span>:<br>            exponential_average_factor = self.momentum<br><br>        <span class="hljs-keyword">if</span> self.training <span class="hljs-keyword">and</span> self.track_running_stats:<br>            <span class="hljs-keyword">if</span> self.num_batches_tracked <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>                self.num_batches_tracked = self.num_batches_tracked + <span class="hljs-number">1</span><br>                <span class="hljs-keyword">if</span> self.momentum <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span>:  <span class="hljs-comment"># use cumulative moving average</span><br>                    exponential_average_factor = <span class="hljs-number">1.0</span> / <span class="hljs-built_in">float</span>(self.num_batches_tracked)<br>                <span class="hljs-keyword">else</span>: <br>                    exponential_average_factor = self.momentum<br>      <br>        output = F.batch_norm(<br>            <span class="hljs-built_in">input</span>, self.running_mean, self.running_var, <span class="hljs-literal">None</span>, <span class="hljs-literal">None</span>,<br>            self.training <span class="hljs-keyword">or</span> <span class="hljs-keyword">not</span> self.track_running_stats,<br>            exponential_average_factor, self.eps)<br>        <span class="hljs-comment">####### BatchNorm end #######</span><br><br>        <span class="hljs-comment">####### scaling calibration begin #######</span><br>        <span class="hljs-comment"># X_cs = X_s \cdot R( w_v \odot K_s + w_b)</span><br>        scale_factor = torch.sigmoid(self.scale_weight*self.stas(output)+self.scale_bias)<br>        <span class="hljs-comment">####### scaling calibration end #######</span><br>      <br>        <span class="hljs-comment"># 仿射变换</span><br>        <span class="hljs-keyword">if</span> self.affine:<br>            <span class="hljs-keyword">return</span> self.weight*scale_factor*output + self.bias<br>        <span class="hljs-keyword">else</span>:<br>            <span class="hljs-keyword">return</span> scale_factor*output<br></code></pre></td></tr></table></figure><h3 id="experiments">Experiments</h3><ul><li><p>在实验中，作者为了确保模块的作用是校准而不是初始化，所以将中心化和缩放校准在训练开始时不起作用</p><ul><li>即 <span class="math inline">\(w_m = 0; w_v = 0, w_b =1\)</span></li></ul></li><li><p><img src="/2021/04/25/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-0x16/7.png"></p></li><li><p><img src="/2021/04/25/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-0x16/8.png"></p><ul><li>因为通道平均值 <span class="math inline">\(\mu_c\)</span>的计算效率高，并且性能较好，所以作者这里就默认都是用的 <span class="math inline">\(\mu_c\)</span> 作为实例特征统计信息</li><li>其中 <span class="math inline">\(\sigma_c\)</span>表示通道的标准划分standard division ofchannels，这种统计数据的效果更好一点，不过计算量要大</li></ul></li><li><p><img src="/2021/04/25/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-0x16/9.png"></p><ul><li>RBN对网络中不同位置的影响</li><li>在早期和晚期阶段加入RBN比在中期阶段取得了更好的性能，作者假设其原因是早期阶段更依赖于输入实例，而最后阶段与实例的语义含义更相关</li></ul></li><li><p><img src="/2021/04/25/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-0x16/4.png"></p><ul><li>大多数层的 <span class="math inline">\(w_m\)</span>都接近于零，说明mini-batch统计在大多数层中仍然起着重要的作用</li><li>在每个阶段的第一层 <span class="math inline">\(w_m\)</span>绝对值通常比其他层大。作者假设原因是特征分辨率的变化使得batchdependency不稳定，需要对特征进行校准来强化特征和降低噪声</li><li>另外，最后一个阶段的 <span class="math inline">\(w_m\)</span>绝对值更大，因为这个阶段具有更多实例特定的特征</li></ul></li><li><p><img src="/2021/04/25/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-0x16/6.png"></p><ul><li>随着网络深度的加深，缩放校准中的 <span class="math inline">\(w_v\)</span> 绝对值变大</li><li>作者假设其原因是因为在更深的层中实例特定的语义可能会使特征分布不稳定，因此需要更多的特征缩放校准</li></ul></li><li><p><img src="/2021/04/25/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-0x16/10.png"></p></li><li><p>本文提出了具有代表性的批标准化(RBN)，并配备了一个简单而有效的特征校准方案，以增强实例特有的表示，并保持BatchNorm的优势。中心化校准增强了信息特征，减弱了噪声特征。缩放校准限制了特征强度，使特征分布更加稳定。RBN可以插入到现有的方法中，以微不足道的成本和参数提高性能</p></li></ul>]]></content>
    
    
    <categories>
      
      <category>study</category>
      
    </categories>
    
    
    <tags>
      
      <tag>组会分享</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>论文阅读-0x15</title>
    <link href="/2021/04/12/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-0x15/"/>
    <url>/2021/04/12/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-0x15/</url>
    
    <content type="html"><![CDATA[<p>论文的阅读笔记：</p><p>《 Neighbor2Neighbor: Self-Supervised Denoising from Single NoisyImages 》, <a href="https://github.com/TaoHuang2018/Neighbor2Neighbor">[code]</a>,CVPR 2021</p><span id="more"></span><h2 id="neighbor2neighbor-self-supervised-denoising-from-single-noisy-images">Neighbor2Neighbor:Self-Supervised Denoising from Single Noisy Images</h2><h3 id="abstract">Abstract</h3><ul><li><p>近年来，神经网络的快速发展使图像去噪得到了很大的发展。然而，由于监督学习需要大量的无噪声图像对，限制了这些模型的广泛应用</p></li><li><p>虽然已经有一些尝试只用单一的噪声图像来训练图像去噪模型，现有的自监督去噪方法存在网络训练效率低下、有用信息丢失或依赖于噪声建模等问题</p></li><li><p>在本文中，作者提出了一种非常简单而有效的方法，即Neighbor2Neighbor，<strong>只使用噪声图像</strong>来训练得到一个有效的图像去噪模型</p><ul><li>首先，提出了一种<strong>随机邻域下采样器来生成训练图像对</strong><ul><li>具体来说，用于训练网络的输入和目标都是从相同的噪声图像中抽取的下采样图像，满足成对图像中成对像素是相邻的且外观非常相似的要求</li></ul></li><li>其次，用第一阶段下采样生成的训练数据对来训练去噪网络，并使用<strong>提出的正则化器作为额外的损失</strong>以获得更好的性能</li></ul></li><li><p>所提出的Neighbor2Neighbor框架能够享受网络体系结构设计中最新的监督式降噪网络的进步</p></li><li><p>此外，它<strong>避免了对噪声分布假设的严重依赖</strong></p></li><li><p>作者从理论的角度解释了该方法，并通过广泛的实验进一步验证了它的方法，包括sRGB空间中具有不同噪声分布的合成实验以及在raw-RGB空间中的降噪基准数据集上的实际实验</p></li><li><p>基于cnn的去噪器在很大程度上依赖于大量去除噪声的图像对来进行训练</p><ul><li>在真实的摄影中，收集大量对齐的成对噪声清洁的训练数据是极具挑战性和昂贵的</li><li>此外，由于合成噪声和真实噪声之间的 domain gap域间隙，用合成噪声训练的模型会严重退化</li></ul></li><li><p>为了缓解这一问题，提出了一系列不需要任何干净图像进行训练的无监督和自我监督方法</p><ul><li>在每个场景中使用<strong>多个独立的噪声观测</strong>来训练网络</li><li>设计<strong>特定的盲点网络结构</strong>，仅在单个噪声图像上学习自监督模型，并使用噪声模型进一步改进，例如高斯-泊松模型</li><li>用含噪对noisier-noisy训练网络，在noisy图像的基础上加入合成噪声，得到noisier图像</li></ul></li><li><p>然而这些预定要求在真实去噪场景中是不现实的</p><ul><li>首先，在每个场景捕捉多个噪声观测仍然是非常具有挑战性的，特别是对于运动场景或医学成像</li><li>其次，盲点网络相对较低的精度和繁重的计算量极大地限制了其应用</li><li>此外，当噪声分布已知为先验条件时，带有噪声模型假设的自监督方法可能在合成实验中很好地工作<ul><li>然而，这些方法在处理现实世界噪声分布未知的图像时，会急剧退化</li></ul></li></ul></li><li><p>在本研究中，作者提出了一种新的自监督图像去噪框架Neighbor2Neighbor，克服了上述局限性</p><ul><li>包括一种<strong>基于下采样的训练图像对生成策略</strong>和一种<strong>带正则化项的自监督训练方案</strong><ul><li>具体来说，<strong>训练输入和目标由随机的邻域下采样器生成，从单一的噪声图像中提取出两个下采样的成对图像，两个图像的相同位置上的每个元素在原始噪声图像中相邻</strong></li><li>这样，如果假设<strong>每个像素的噪声以像素值为条件是独立的</strong>，不同位置的噪声之间不存在相关性，那么在原始噪声图像的groundtruth的条件下，<strong>这两个下采样成对噪声图像是独立的</strong></li><li>因此，受Noise2Noise的启发，使用上述训练对来训练去噪网络</li></ul></li><li>此外作者<strong>引入了一个正则化项来解决原始噪点图像上相邻像素之间像素ground-truth的本质差异</strong>（没看懂）</li><li>提出的自监督框架旨在仅使用单一图像训练去噪网络，而不需要对网络结构进行任何修改</li><li>任何在有监督图像去噪任务中表现良好的网络都可以在我们的框架中使用。此外，我们的方法也不依赖于任何噪声模型</li></ul></li><li><p>为了评估所提出的Neighbor2Neighbor，在合成的和真实世界的噪声图像上进行了一系列的实验。大量的实验表明，我们的Neighbor2Neighbor方法优于传统的去噪方法和现有的自监督去噪方法。实验结果表明了该方法的有效性和优越性</p></li><li><p>主要贡献如下：</p><ul><li>我们提出了一种新的自监督图像去噪框架，在该框架中，任何现有的去噪网络都可以在没<strong>有任何干净目标、网络修改或噪声模型假设的情况下进行训练</strong></li><li>从理论的角度，我们<strong>为提出的框架提供了良好的动机</strong></li><li><strong>与最先进的自监督去噪方法相比，该方法表现得非常好</strong>，特别是在真实世界的数据集上，这显示了它在真实世界场景中的潜在应用</li></ul></li><li><p>监督图像去噪</p><ul><li>Zhang等人提出了将卷积神经网络和残差学习相结合的DnCNN进行图像去噪。在对噪声清除的成对图像进行监督学习的情况下，DnCNN的去噪效果大大优于传统图像去噪器。之后，为了进一步提高性能，提出了大量的去噪网络</li><li>然而，这些深度去噪器需要大量成对的噪声-干净的图像对来进行训练。为了进行监督降噪，需要收集大量的训练对。这限制了受监督的降噪器的使用</li></ul></li><li><blockquote><p>仅使用噪声图像的图像去噪</p><ul><li>传统的去噪方法：BM3D、NLM和WNNM</li><li>深度去噪器<ul><li>Ulyanov等人提出了深度图像先验(deep image prior,DIP)，其中图像先验是从CNN网络获取的，而不是专门设计的</li><li>Lehtinen等人引入Noise2Noise对同一场景的多个噪声观测进行深度去噪</li><li>随后提出了包括Noise2Void和Noise2Self在内的自监督降噪模型，以训练每个场景只有一个噪声观测的网络（具体来说，就是使用精心设计的盲点网络来避免学习当前自身位置identity）</li><li>最近，probability Noise2Void，Laine19，和膨胀盲点网络进一步引入了显式噪声建模和概率推理，以获得更好的性能。引入了mask卷积和堆叠扩张卷积层以提高训练速度</li><li>与基于盲点的自监督方法不同，在Noisier2Noise中，从噪声模型生成合成噪声，并将它们添加到单个噪声图像中来生成训练对</li><li>然而，噪声模型很难确定，特别是在真实场景中。上面提到的Noisy-as-Clean也有类似的理念</li><li>此外，Soltanayev和Chun使用Stein的无偏风险估计器(SURE)在单幅有噪图像上训练AWGN去噪模型，而Zhussip等人将其扩展到有噪图像相关对的情况</li><li>Cha和Moon曾经对每个测试图像进行微调监督降噪。然而，基于确定的算法只是针对高斯加性噪声设计的，并且噪声级别是已知的先验</li></ul></li></ul></blockquote></li></ul><h3 id="motivation">Motivation</h3><ul><li>回顾Noise2Noise<ul><li>该方法只需要对相同场景的独立噪声图像，给定相同ground-truth 图像<span class="math inline">\(\mathbf{x}\)</span> 的两个独立的噪声观测值和<span class="math inline">\(\mathbf{y}\)</span> <span class="math inline">\(\mathbf{z}\)</span>，Noise2Noise试图使下列θ值的损失最小化</li><li><span class="math inline">\(\underset{\theta}{\arg \min }\mathbb{E}_{\mathbf{x}, \mathbf{y},\mathbf{z}}\left\|f_{\theta}(\mathbf{y})-\mathbf{z}\right\|_{2}^{2}\)</span><ul><li>其中 <span class="math inline">\(f_{\theta}\)</span> 是由 <span class="math inline">\(\theta\)</span> 参数化的去噪网络</li><li><strong>最小化该损失产生的参数解和使用</strong> <span class="math inline">\(L_2\)</span><strong>loss的监督训练效果相同</strong></li></ul></li></ul></li><li>Paired Images with Similar Ground Truths，与GroundTruths相似的成对图像<ul><li>Noise2Noise减少了对干净图像的需求。然而，捕捉一个场景的多个噪声观测仍然是一个非常具有挑战性的问题</li><li>由于遮挡、运动和光照变化等，两个噪声观测的ground-truth很难完全相同，因此，我们建议将上面的损失公式<strong>扩展到底层干净图像之间的间隙情况</strong></li><li><span class="math inline">\(\varepsilon:=\mathbb{E}_{\mathbf{z} \mid\mathbf{x}}(\mathbf{z})-\mathbb{E}_{\mathbf{y} \mid\mathbf{x}}(\mathbf{y}) \neq \mathbf{0}\)</span></li><li>Theorem 1<ul><li>假设 <span class="math inline">\(\mathbf{y}\)</span> 和 <span class="math inline">\(\mathbf{z}\)</span> 为以 <span class="math inline">\(\mathbf{x}\)</span>为条件的两个独立的噪声图像，然后假设存在一个 <span class="math inline">\(\varepsilon \not = 0\)</span> 使得 <span class="math inline">\(\mathbb{E}_{\mathbf{y} \mid\mathbf{x}}(\mathbf{y}) = \mathbf{x}\)</span> 和 <span class="math inline">\(\mathbb{E}_{\mathbf{z} \mid\mathbf{x}}(\mathbf{z}) = \mathbf{x} + \varepsilon\)</span> ，假设 <span class="math inline">\(\mathbf{z}\)</span> 的方差为 <span class="math inline">\(\sigma_{\mathbf{z}}^{2}\)</span> ，则有</li><li><span class="math inline">\(\begin{aligned} \mathbb{E}_{\mathbf{x},\mathbf{y}}\left\|f_{\theta}(\mathbf{y})-\mathbf{x}\right\|_{2}^{2}&amp;=\mathbb{E}_{\mathbf{x}, \mathbf{y},\mathbf{z}}\left\|f_{\theta}(\mathbf{y})-\mathbf{z}\right\|_{2}^{2}-\sigma_{\mathbf{z}}^{2}\\ &amp;+2 \varepsilon \mathbb{E}_{\mathbf{x},\mathbf{y}}\left(f_{\theta}(\mathbf{y})-\mathbf{x}\right)\end{aligned}\)</span></li></ul></li><li>定理1说明，当间隙 <span class="math inline">\(\varepsilon \not =0\)</span> 时，由于 <span class="math inline">\(\mathbb{E}_{\mathbf{x},\mathbf{y}}(f_{\theta}(\mathbf{y}) - \mathbf{x}) \not \equiv 0\)</span>，优化 <span class="math inline">\(\mathbb{E}_{\mathbf{x}, \mathbf{y},\mathbf{z}}\left\|f_{\theta}(\mathbf{y})-\mathbf{z}\right\|_{2}^{2}\)</span>并不会产生和监督学习损失 <span class="math inline">\(\mathbb{E}_{\mathbf{x},\mathbf{y}}\left\|f_{\theta}(\mathbf{y})-\mathbf{x}\right\|_{2}^{2}\)</span>相同的解</li><li>幸运的是，如果 <span class="math inline">\(\varepsilon \rightarrow0\)</span> ，即<strong>间隙足够小</strong>，则为 <span class="math inline">\(2 \varepsilon \mathbb{E}_{\mathbf{x},\mathbf{y}}\left(f_{\theta}(\mathbf{y})-\mathbf{x}\right) \rightarrow0\)</span> ，则<strong>用有噪图像对 <span class="math inline">\((\mathbf{y},\mathbf{z})\)</span>训练的网络可以作为有监督训练网络的合理近似解</strong></li><li><strong>（上面这部分其实就是说用合成噪声对去训练的网络是无法去拟合有监督学习的，会存在偏差，也就是说用噪声数据对训练得到的网络参数解和监督学习的解不相同，但如果两幅噪声数据对的差异很小可以忽略的时候，就可以当成一个合理近似解）</strong></li><li>注意到当间隙 <span class="math inline">\(\varepsilon = 0\)</span>时， <span class="math inline">\(\sigma_{\mathbf{z}}^{2}\)</span>是一个常数，这时候去最小化理论1中的方程得到 <span class="math inline">\(\underset{\theta}{\arg \min }\mathbb{E}_{\mathbf{x}, \mathbf{y},\mathbf{z}}\left\|f_{\theta}(\mathbf{y})-\mathbf{z}\right\|_{2}^{2}\)</span>，就是Noise2Noise的目标</li></ul></li><li>Extension to Single Noisy Images， 拓展到单个图像去噪<ul><li>受Noise2Noise（训练数据对是同一场景的独立的噪声图像对）的启发，我们进一步提出<strong>从单一噪声图像<span class="math inline">\(\mathbf{y}\)</span>中通过采样生成独立的训练数据对</strong></li><li>具体来说，使用图像对采样器 <span class="math inline">\(G = (g_1,g_2)\)</span> ，从单个有噪图像 <span class="math inline">\(\mathbf{y}\)</span> 生成有噪图像对 <span class="math inline">\((g_1(\mathbf{y}), g_2(\mathbf{y}))\)</span>，两个采样得到的图像对内容十分相似</li><li>与上面类似，我们尝试将采样后的图像对作为两个噪声观测值</li><li><span class="math inline">\(\underset{\theta}{\arg \min }\mathbb{E}_{\mathbf{x},\mathbf{y}}\left\|f_{\theta}(g_1(\mathbf{y}))-g_2(\mathbf{y}))\right\|_{2}^{2}\)</span></li><li>这里与Noise2Noise不同的是，<strong>这两个采样图像的ground-truth是不一样的</strong>，即<span class="math inline">\(\varepsilon =\mathbb{E}_{\mathbf{y} \mid\mathbf{x}}(g_2(\mathbf{y})))-\mathbb{E}_{\mathbf{y} \mid\mathbf{x}}(g_1(\mathbf{y}))) \neq \mathbf{0}\)</span></li><li>根据定理1，直接使用 <span class="math inline">\(\underset{\theta}{\arg \min }\mathbb{E}_{\mathbf{x},\mathbf{y}}\left\|f_{\theta}(g_1(\mathbf{y}))-g_2(\mathbf{y}))\right\|_{2}^{2}\)</span>是不合适的，<strong>会导致过平滑</strong>，因此作者在这里考虑非零间隙<span class="math inline">\(\varepsilon\)</span><ul><li>这里感觉就是如果直接用这个来学习的话，因为是两个不同下采样得到的图像，所以对于一些突变的边界而言，可能就会学习的不太好，会学习到一个平均值，导致过于平滑（个人理解）</li></ul></li><li>考虑到使用干净图像和 <span class="math inline">\(L_2\)</span>loss和干净图像是最佳（理想）去噪器 <span class="math inline">\(f_{\theta}^{*}\)</span> ，给定 <span class="math inline">\(\mathbf{x}\)</span> ，对于 <span class="math inline">\(\ell \in\{1,2\}\)</span> ，它满足 <span class="math inline">\(f_{\theta}^{*}(\mathbf{y})=\mathbf{x}\)</span> 和<span class="math inline">\(f_{\theta}^{*}\left(g_{\ell}(\mathbf{y})\right)=g_{\ell}(\mathbf{x})\)</span></li><li>因此，对于最优网络 <span class="math inline">\(f_{\theta}^{*}\)</span> ，以下情况成立:<ul><li><span class="math inline">\(\mathbb{E}_{\mathbf{y} \mid\mathbf{x}}\left\{f_{\theta}^{*}\left(g_{1}(\mathbf{y})\right)-g_{2}(\mathbf{y})-\left(g_{1}\left(f_{\theta}^{*}(\mathbf{y})\right)-g_{2}\left(f_{\theta}^{*}(\mathbf{y})\right)\right)\right\}\)</span><span class="math inline">\(=g_{1}(\mathbf{x})-\mathbb{E}_{\mathbf{y}\mid\mathbf{x}}\left\{g_{2}(\mathbf{y})\right\}-\left(g_{1}(\mathbf{x})-g_{2}(\mathbf{x})\right)\)</span><span class="math inline">\(=g_{2}(\mathbf{x})-\mathbb{E}_{\mathbf{y}\mid \mathbf{x}}\left\{g_{2}(\mathbf{y})\right\}=0\)</span></li><li>上式中的最后两项，是考虑到了<strong>训练图像和groundtruth之间的差距间隙</strong><ul><li><strong>当间隙为零时，上式中后两项相减消去</strong>，这样就得到了Noise2Noise配对训练的特例</li><li>但如果<strong>间隙不为零，则这两项作为对上式中前两项的ground truthgap的修正，使上式为零</strong></li><li>这里作者设计的太巧妙了！！❗❗❗</li></ul></li></ul></li><li>可以看出来，上式其实提供了一个约束条件，当去噪器 <span class="math inline">\(f_{\theta}\)</span> 是理想的 <span class="math inline">\(f_{\theta}^{*}\)</span>时，该约束条件得到满足。为了利用这个(理想)约束，而不是直接优化方程 <span class="math inline">\(\underset{\theta}{\arg \min }\mathbb{E}_{\mathbf{x},\mathbf{y}}\left\|f_{\theta}(g_1(\mathbf{y}))-g_2(\mathbf{y}))\right\|_{2}^{2}\)</span>，我们考虑以下约束优化问题:<ul><li><span class="math inline">\(\min _{\theta} \mathbb{E}_{\mathbf{y}\mid\mathbf{x}}\left\|f_{\theta}\left(g_{1}(\mathbf{y})\right)-g_{2}(\mathbf{y})\right\|_{2}^{2}\)</span>,s.t. <span class="math inline">\(\mathbb{E}_{\mathbf{y} \mid\mathbf{x}}\left\{f_{\theta}\left(g_{1}(\mathbf{y})\right)-g_{2}(\mathbf{y})-g_{1}\left(f_{\theta}(\mathbf{y})\right)+g_{2}\left(f_{\theta}(\mathbf{y})\right)\right\}=0\)</span></li><li>其中 <span class="math inline">\(\mathbb{E}_{x,y}=  \mathbb{E}_x\mathbb{E}_{y|x}\)</span> ，我们进一步将其表述为以下正则化优化问题:</li><li><span class="math inline">\(\begin{aligned} &amp; \min _{\theta}\mathbb{E}_{\mathbf{x},\mathbf{y}}\left\|f_{\theta}\left(g_{1}(\mathbf{y})\right)-g_{2}(\mathbf{y})\right\|_{2}^{2}\\+&amp; \gamma \mathbb{E}_{\mathbf{x},\mathbf{y}}\left\|f_{\theta}\left(g_{1}(\mathbf{y})\right)-g_{2}(\mathbf{y})-g_{1}\left(f_{\theta}(\mathbf{y})\right)+g_{2}\left(f_{\theta}(\mathbf{y})\right)\right\|_{2}^{2}\end{aligned}\)</span></li></ul></li></ul></li><li>其实看到这里，基本把整篇文章的思想和提出的方法看完了，真的巧妙！太神了，真的有想法！</li></ul><h3 id="proposed-method">Proposed Method</h3><p><img src="/2021/04/12/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-0x15/1.png"></p><ul><li>基于在上一节中的动机，我们提出Neighbor2Neighbor，一个自我监督的框架，通过对单噪声图像观察来训练CNN去噪器。提出的培训方案包括两部分<ul><li>利用随机邻域下采样器生成成对的噪声图像</li><li>在使用下采样图像对进行自我监督训练的同时，进一步引入了正则化损失来解决下采样图像之间的非零ground-truth差距</li></ul></li><li>Generation of Training Image Pairs<ul><li>首先，作者引入了一个邻域下采样器，从单个有噪图像 <span class="math inline">\(\mathbf{y}\)</span> 生成有噪图像对 <span class="math inline">\((g_1(\mathbf{y}), g_2(\mathbf{y}))\)</span>进行训练</li><li>满足之前讨论过的假设：<ul><li>已知 <span class="math inline">\(\mathbf{y}\)</span> 的ground truth<span class="math inline">\(\mathbf{x}\)</span> ，下采样成对噪声图像<span class="math inline">\((g_1(\mathbf{y}), g_2(\mathbf{y}))\)</span>是条件独立的</li><li><span class="math inline">\((g_1(\mathbf{y}),g_2(\mathbf{y}))\)</span> 的ground truth之间的差距gap较小</li></ul></li><li>邻域下采样器的图像对生成示意图：</li><li><img src="/2021/04/12/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-0x15/2.png"></li><li>表示一幅图像 <span class="math inline">\(\mathbf{y}\)</span>，宽度为W，高度为H。相邻子采样器 <span class="math inline">\(G = (g_1,g_2)\)</span> 的细节描述如下<ul><li>将图像 <span class="math inline">\(\mathbf{y}\)</span> 分为 <span class="math inline">\(\lfloor W/k \rfloor × \lfloor H/k \rfloor\)</span>个单元，每个单元的大小为k × k。根据经验，我们设k = 2</li><li>对于第i行和第j列单元，随机选择两个相邻的位置。分别作为子采样器 <span class="math inline">\(G = (g_1, g_2)\)</span> 的第 <span class="math inline">\((i,  j)\)</span> 个元素</li><li>对于所有的 <span class="math inline">\(\lfloor W/k \rfloor × \lfloorH/k \rfloor\)</span> 个单元，重复步骤2，从而生成相邻的子采样器 <span class="math inline">\(G = (g_1, g_2)\)</span></li></ul></li><li>给定图像 <span class="math inline">\(\mathbf{y}\)</span>，导出两个下采样图像 <span class="math inline">\((g_1(\mathbf{y}),g_2(\mathbf{y}))\)</span> ，大小为 <span class="math inline">\(\lfloorW/k \rfloor × \lfloor H/k \rfloor\)</span></li><li>成对图像 <span class="math inline">\((g_1(\mathbf{y}),g_2(\mathbf{y}))\)</span> 的ground-truth相似，因为 <span class="math inline">\((g_1(\mathbf{y}), g_2(\mathbf{y}))\)</span>的成对像素从原始噪声图像 <span class="math inline">\(\mathbf{y}\)</span>相邻采样得到</li><li><span class="math inline">\((g_1(\mathbf{y}),g_2(\mathbf{y}))\)</span> 在给定 <span class="math inline">\(\mathbf{x}\)</span>的条件下，如果进一步假设有噪图像 <span class="math inline">\(\mathbf{y}\)</span> 在给定ground-truth <span class="math inline">\(\mathbf{x}\)</span> 的条件下是像素独立的，则满足<span class="math inline">\((g_1(\mathbf{y}), g_2(\mathbf{y}))\)</span>的独立性要求</li></ul></li><li>Self-Supervised Training with a Regularizer<ul><li>给定来自噪声图像 <span class="math inline">\(\mathbf{y}\)</span>的一对下采样图像 <span class="math inline">\((g_1(\mathbf{y}),g_2(\mathbf{y}))\)</span>，我们使用上一节中提出的正则化损失来训练去噪网络：</li><li><span class="math inline">\(\begin{aligned} \mathcal{L}&amp;=\mathcal{L}_{r e c}+\gamma \cdot \mathcal{L}_{r e g} \\&amp;=\left\|f_{\theta}\left(g_{1}(\mathbf{y})\right)-g_{2}(\mathbf{y})\right\|_{2}^{2}\\ &amp;+\gamma\cdot\left\|f_{\theta}\left(g_{1}(\mathbf{y})\right)-g_{2}(\mathbf{y})-\left(g_{1}\left(f_{\theta}(\mathbf{y})\right)-g_{2}\left(f_{\theta}(\mathbf{y})\right)\right)\right\|_{2}^{2}\end{aligned}\)</span><ul><li><span class="math inline">\(f_{\theta}\)</span>为任意网络设计的去噪网络， <span class="math inline">\(\gamma\)</span>为控制正则化项强度的超参数</li></ul></li><li>为了稳定学习，我们在训练过程中停止 <span class="math inline">\(g_1(f_{\theta}(\mathbf{y})),g_2(f_{\theta}(\mathbf{y}))\)</span> 的梯度，逐渐增加 <span class="math inline">\(\gamma\)</span> 到指定的值</li><li><img src="/2021/04/12/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-0x15/3.png"></li></ul></li></ul><h3 id="experiments">Experiments</h3><p>emm实验不说了</p><h3 id="section">😝😜😋</h3><p>真的很神奇，从noise2noise到这边一篇，感觉每一篇都很有趣，很有想法，而且很多细节设计上真的很巧妙，能够很好的避免一些缺陷。虽然总结起来会很简单，但是去看他的具体设计的话，还是很有收获的。</p><p>最近感觉读论文啥的似乎也不是一个很好的方式，想着这段时间先不读论文了，去跑跑那个noise2noise的代码，然后改一改。这篇论文的代码暂时还没有放出来，所以倒也不是很急，慢慢来，噶油~</p>]]></content>
    
    
    <categories>
      
      <category>study</category>
      
    </categories>
    
    
    <tags>
      
      <tag>图像增强与图像恢复</tag>
      
      <tag>高光谱图像去噪</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>论文阅读-0x14</title>
    <link href="/2021/04/11/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-0x14/"/>
    <url>/2021/04/11/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-0x14/</url>
    
    <content type="html"><![CDATA[<p>论文的阅读笔记：</p><p>《 Noise2Noise: Learning Image Restoration without Clean Data 》, <a href="https://github.com/NVlabs/noise2noise">[code]</a>, ICML2018</p><p>《 Noise2Void: Learning Denoising from Single Noisy Images 》<a href="https://github.com/juglab/n2v">[code]</a>，CVPR 2019</p><span id="more"></span><h2 id="noise2noise-learning-image-restoration-without-clean-data">Noise2Noise:Learning Image Restoration without Clean Data</h2><h3 id="abstract">Abstract</h3><ul><li>作者将基本的统计推理应用于机器学习（学习将损坏的观测数据映射到干净的信号）的信号重建，得到了一个简单而有力的结论：<ul><li><strong>在没有明确的图像先验或损坏的似然模型的情况下，通过只查看损坏样例的情况下学习图像恢复是可能的</strong></li><li>达到甚至超过使用干净的数据训练的水平表现</li><li>作者展示了使用一个单一的模型学习了去噪任务——仅仅基于噪声数据</li></ul></li><li>从损坏或不完整的观测值中重建信号是统计数据分析的一个重要分支<ul><li>随着深度神经网络的最新进展，人们避免了传统的、显式的信号损坏的先验统计建模</li><li>取而代之的是学习将损坏的观测数据映射到未观测到的干净版本</li><li>训练一个回归模型（例如CNN网络），使用大量数据对 <span class="math inline">\((\hat x_i, y_i)\)</span> ：损坏输入 <span class="math inline">\(\hat x_i\)</span> 和对应的干净数据 <span class="math inline">\(y_i\)</span> ，然后训练最小化经验风险<ul><li><span class="math inline">\(\underset{\theta}{\operatorname{argmin}}\sum_{i} L\left(f_{\theta}\left(\hat{x}_{i}\right),y_{i}\right)\)</span></li><li><span class="math inline">\(f_{\theta}\)</span>是CNNs网络的参数映射族</li></ul></li></ul></li><li>使用符号 <span class="math inline">\(\hat x\)</span>来强调以下事实：损坏的输入 <span class="math inline">\(\hat x \simp(\hat x |y_i)\)</span> 是根据干净目标得到的随机变量分布</li><li>获得干净的训练目标通常是困难或乏味的：无噪声照片需要长时间曝光，完整的MRI取样排除了动态受试者等</li><li>在该工作中，作者观察到，可以只使用损坏的图像来得到好的图像<ul><li>而且效果一样好，甚至更好（像使用干净的样本一样）</li><li>此外，也<strong>不需要明确的统计似然模型，也不需要图像先验，而是从训练数据中间接学习到这些信息数据</strong></li></ul></li></ul><h3 id="theoretical-background">Theoretical Background</h3><ul><li>假设我们有一组不可靠的室温测量值 <span class="math inline">\((y_1,y_2, \dots)\)</span><ul><li>估计真实未知温度的一个常见策略是根据损失函数 <span class="math inline">\(L\)</span> 找到一个与测量值的平均偏差最小的数字<span class="math inline">\(z\)</span> :<ul><li><span class="math inline">\(\underset{z}{\operatorname{argmin}}\mathbb E_y\{ L(z,y) \}\)</span></li></ul></li><li>对于 <span class="math inline">\(L_2\)</span> 损失 <span class="math inline">\(L(z,y) = (z-y)^2\)</span> ，该损失函数的最优解在测量值的算数平均值(期望)处取到：<ul><li><span class="math inline">\(z=E_y\{y\}\)</span></li></ul></li><li>对于 <span class="math inline">\(L_1\)</span> 损失 <span class="math inline">\(L(z,y) = |z-y|\)</span> ，该损失函数的最优解在测量值的中值处取到：<ul><li><span class="math inline">\(z=median \{y\}\)</span></li></ul></li><li>对于 <span class="math inline">\(L_0\)</span> 损失 <span class="math inline">\(L(z,y) = |z-y|_0\)</span> ，该损失函数的最优解在测量值的众数处取到：<ul><li><span class="math inline">\(z=mode \{y\}\)</span></li></ul></li></ul></li><li>从统计学角度，这些常用的损失函数都可以解释为似然函数的负对数，而对这些损失函数的优化过程可以看做为最大似然估计（M-estimators,ML）</li><li>训练神经网络回归器是这种点估计过程的推广<ul><li>已知一系列输入-目标对 <span class="math inline">\(( x_i,y_i)\)</span> ，典型的网络训练形式是优化下列目标函数：</li><li><span class="math inline">\(\underset{\theta}{\operatorname{argmin}}\mathbb E_{x,y}\{ L(f_\theta(x),y) \}\)</span><ul><li><span class="math inline">\(f_{\theta}\)</span>是网络的参数映射族</li><li>这里如果进行简化，就是上面提到的那个简单的估计真实未知温度的损失</li></ul></li></ul></li><li>完整训练任务在每个训练样本上分解为相同的最小化问题（如果将整个训练任务分解为几个训练步骤，根据贝叶斯定理可将上述目标函数变为）：<ul><li><span class="math inline">\(\underset{\theta}{\operatorname{argmin}}\mathbb E_{x} \{ \mathbb E_{y|x}\{ L(f_\theta(x),y) \} \}\)</span></li></ul></li><li>该网络可以<strong>通过分别解决每个输入样本的点估计问题来减少这种损失</strong><ul><li>因此，潜在损失的特性是通过神经网络训练来继承的</li><li>（则网络训练的目标函数与前面所说的标量损失函数有相同的形式，也具有相同的特性）</li></ul></li><li>在有限的输入-目标对 <span class="math inline">\((x_i,y_i)\)</span>中，通过方程1 <span class="math inline">\(\underset{\theta}{\operatorname{argmin}} \sum_{i}L\left(f_{\theta}\left(\hat{x}_{i}\right), y_{i}\right)\)</span>训练回归量的过程隐藏了一个微妙的点:与该过程所隐藏的输入和目标之间1:1的映射(虚假的)不同，在现实中的映射是多值的<ul><li>例如，在所有自然图像的超分辨率任务中，低分辨率图像x可以用许多不同的高分辨率图像y来解释，因为关于边缘和纹理的确切位置和方向的知识信息在降采样的过程中丢失了</li><li>也就是说，<span class="math inline">\(p(y|x)\)</span>是与低分辨率x一致的自然图像的高度复杂分布</li><li>使用 <span class="math inline">\(L_2\)</span>loss对成对低分辨率图像和高分辨率图像训练神经网络回归器，网络学习输出所有可能解释的平均值(例如，不同的边缘偏移量)，导致网络预测的空间模糊</li><li>为了克服这种趋势，已经做了大量的工作，例如使用学习到的鉴别器函数作为损失</li></ul></li><li>作者观察到，对于某些问题，这种趋势会带来意想不到的好处</li><li>最小化 <span class="math inline">\(L_2\)</span>loss的一个简单的、乍一看无用的性质是，<strong>在期望上，如果我们用与目标期望匹配的随机数 替换目标，则估计保持不变</strong><ul><li>就是说我们用的这个<strong>随机数的期望</strong>和<strong>目标的期望</strong> 相匹配，那个这个过程就是一致的</li><li>这很容易看出：无论 <span class="math inline">\(y\)</span>的具体分布是什么， <span class="math inline">\(L_2\)</span>loss都成立</li><li>因此，当输入条件的目标分布 <span class="math inline">\(p(y|x)\)</span>被<strong>任意具有相同条件期望值的分布</strong>所替代时，方程 <span class="math inline">\(\underset{\theta}{\operatorname{argmin}} \mathbbE_{x} \{ \mathbb E_{y|x}\{ L(f_\theta(x),y) \} \}\)</span>中的最优网络参数θ仍然保持不变</li><li>这意味着，原则上，我们可以<strong>用零均值噪声破坏神经网络的训练目标，而不改变网络学习的内容</strong></li></ul></li><li>将此与方程 <span class="math inline">\(\underset{\theta}{\operatorname{argmin}} \sum_{i}L\left(f_{\theta}\left(\hat{x}_{i}\right), y_{i}\right)\)</span>中损坏的输入相结合，我们就剩下了经验风险最小化任务<ul><li><span class="math inline">\(\underset{\theta}{\operatorname{argmin}}\sum_{i} L\left(f_{\theta}\left(\hat{x}_{i}\right), \haty_{i}\right)\)</span></li><li>其中<strong>输入和目标现在都从一个损坏的分布(不一定相同)中得到</strong>，<strong>以潜在的、未观察到的干净目标<span class="math inline">\(y_i\)</span> 为条件</strong>，使得 <span class="math inline">\(\mathbb E \{ \hat y_i | \hat x_i \} =y_i\)</span></li></ul></li><li>有趣的是，上述方法都不依赖于损坏的似然模型，也不依赖于底层干净图像流形的密度模型（先验）<ul><li>也就是说，我们不需要显式的 <span class="math inline">\(p({noisy}|{clean})\)</span> 或 <span class="math inline">\(p(clean)\)</span>，<strong>只要我们有根据它们分布的数据</strong></li></ul></li><li>在许多图像还原任务中，损坏的输入数据的期望 是 我们寻求还原的干净目标<ul><li>弱光摄影就是一个例子:长、无噪声曝光是短、独立、有噪声曝光的平均值</li><li>考虑到这一点，上面的研究表明，只要有一对有噪声的图像，就可以学习去除摄像噪声，而不需要潜在的昂贵或困难的长时间曝光图像</li><li>对其他损失函数也可以得出类似的结论</li><li>例如，<span class="math inline">\(L_1\)</span>loss恢复了目标的中位数，这意味着可以训练神经网络修复<strong>具有显著(最高50%)异常值内容</strong>的图像，同样只需要访问这些损坏的图像对</li></ul></li></ul><h3 id="practical-experiments">Practical Experiments</h3><p><img src="/2021/04/11/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-0x14/1.png"></p><ul><li>作者对噪声目标训练的实际性能进行了实验研究<ul><li>从简单噪声分布(高斯分布、泊松分布、伯努利分布)开始</li><li>然后继续研究更加困难、难以解析的蒙特卡罗图像合成噪声</li></ul></li><li>首先研究利用合成的加性高斯噪声对被破坏目标的影响<ul><li>由于噪声的均值为零，使用 <span class="math inline">\(L_2\)</span>loss进行训练来恢复均值</li></ul></li><li>显然，每个训练实例都要求做不可能的事情:<strong>网络不可能成功地将一个噪声实例转换成另一个</strong>。因此，在训练过程中，训练损失实际上并没有减少，而且损失梯度仍然相当大<ul><li>但什么较大、噪声较大的梯度不会影响收敛速度?</li><li>虽然激活梯度确实是有噪声的，但权值梯度实际上是相对干净的，因为<strong>高斯噪声在所有像素中是独立和同分布的</strong></li></ul></li><li>最终作者得出这样的结论：对于加性高斯噪声，损坏的目标在两个级别上优于干净目标，不仅具有相同的性能，而且还具有更好的优势<ul><li>在相同的潜在干净图像上得到更多的损坏实现</li><li>有利于看到更多潜在的干净图像，即使每个图像只有两个损坏的实现</li></ul></li><li>后面作者针对各种不同噪声，都做了一些实验，效果都还是很好的，不过有些噪声比较复杂，就稍微看了一眼，，其实也没咋看懂</li><li>作者已经证明，简单的统计参数导致使用深度神经网络学习信号恢复的新能力<ul><li><strong>在等于或接近使用干净目标数据的性能水平下，有可能在复杂损坏下恢复信号而无需观察干净信号，而无需对噪声或其他损坏进行显式统计表征</strong></li><li>也就是说对于去噪来说，干净的数据是不必要的</li><li>这并不是一个新发现:事实上，考虑一下经典的BM3D算法，它利用单个有噪声图像中的自相似斑块</li><li>作者证明了之前证明的深度神经网络的高恢复性能同样可以在没有干净数据的情况下完全实现，所有这些都基于相同的通用深度卷积模型</li><li>这为许多应用程序带来了显著的好处，消除了收集干净数据的潜在费力需求</li></ul></li></ul><p><img src="/2021/04/11/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-0x14/2.png"></p><h3 id="section">😝😜😋</h3><p>这篇文章还是很神奇的，通过对损失的巧妙分析，证明了noise tonoise的可能性。</p><p>其实这样想想真的挺有道理的，他最终损失那里求得期望，只要你在目标图像那里加的噪声是平均期望为零，那么对整个网络的训练和学习是没有影响的，也就是说网络的参数学习其实是一样的，真的很巧妙。</p><p>不过也是几年前的工作了，再看看最新的继续学习吧~</p><h2 id="noise2void-learning-denoising-from-single-noisy-images">Noise2Void:Learning Denoising from Single Noisy Images</h2><h3 id="abstract-1">Abstract</h3><ul><li>目前，图像去噪领域主要是基于一对有噪声的输入图像和干净的目标图像进行训练的判辩式深度学习方法<ul><li>最近的研究表明，这种方法也可以在没有清洁目标的情况下进行训练</li><li>可以使用独立的噪声图像对，这种方法被称为NOISE2NOISE (N2N)</li></ul></li><li>在这里，作者提出了NOISE2VOID(N2V)，这是一种将这一想法更进一步的训练方案<ul><li>它不需要有噪声的图像对，也不需要干净的目标图像</li><li>N2V允许我们直接对要去噪的数据体进行训练，因此可以在其他方法无法应用时应用</li></ul></li><li>特别有趣的是应用于生物医学图像数据，在这种情况下，通常不可能获得干净或噪声的训练目标</li><li>作者也直言不能指望N2V比那些在训练中拥有更多可用信息的方法表现更好，但依旧优于无训练的去噪方法</li><li>图像去噪就是检查一个有噪声的图像 <span class="math inline">\(\boldsymbol{x} = \boldsymbol{s}+  \boldsymbol{n}\)</span> ，把它分成两个部分：它的信号 <span class="math inline">\(\boldsymbol{s}\)</span> 和我们想要去除的有损噪声<span class="math inline">\(\boldsymbol{n}\)</span> 的信号<ul><li>去噪方法通常依赖于假设 <span class="math inline">\(\boldsymbol{s}\)</span>中的像素值在统计上不是独立的</li><li>也就是说，<strong>观察一个未知的像素的图像上下文，可以很好地让我们对像素强度做出合理的预测</strong></li><li>大量的工作通过马尔可夫随机场(MRFs)明确地建模了这些相互依赖</li><li>近年来，人们用各种方法训练卷积神经网络(CNNs)，从周围的图像块（感受野）中预测像素值</li></ul></li><li>通常，这类系统需要噪声输入图像 <span class="math inline">\(x_j\)</span> 和它们对应的干净目标图像 <span class="math inline">\(s_j\)</span> (ground truth)的训练对 <span class="math inline">\((x_j,s_j)\)</span>。然后对网络参数进行调整，<strong>使网络预测和已知groundtruth之间的误差度量(损失)最小化</strong></li><li>每当groundtruth图像无法获得时，这些方法就无法进行训练，因此对于去噪任务来说就变得毫无用处</li><li><strong>NOISE2NOISE (N2N)训练试图学习同一训练图像 <span class="math inline">\((s + n,s +  n&#39;)\)</span>的独立退化版本对之间的映射，它们包含相同的信号 <span class="math inline">\(s\)</span> ，但独立绘制噪声 <span class="math inline">\(n\)</span> 和 <span class="math inline">\(n&#39;\)</span></strong><ul><li>神经网络不可能学会完美地从一个有噪声的图像中预测另一个图像</li><li>然而，在这个不可能的训练任务上训练的网络，可以产生收敛到与使用ground truth图像传统训练的网络相同的预测结果</li><li>在无法获得groundtruth数据的情况下，N2N仍然可以实现去噪网络的训练。但是，这<strong>需要获取两幅具有独立噪声<span class="math inline">\((n,n&#39;)\)</span> 的相同内容 <span class="math inline">\(\boldsymbol{s}\)</span> 的图像</strong></li><li>尽管N2N训练有这些优点，但该方法至少有两个缺点<ul><li>N2N训练需要成对的噪声图像</li><li>只有在(准)静态场景中才能获得(准)常数 <span class="math inline">\(\boldsymbol{s}\)</span> 的这类成对数据</li></ul></li></ul></li><li>和N2N一样，N2V也利用了这样的观察：在没有干净的groundtruth真实数据的情况下，可以训练出高质量的去噪模型</li><li>但与N2N或传统的训练方法不同，N2V也可以应用于既没有成对噪声图像又没有干净目标图像的数据，即N2V是一种<strong>自监督的训练方法</strong></li><li>在这项工作中，作者做了两个简单的统计假设<ul><li>信号 <span class="math inline">\(\boldsymbol{s}\)</span>不是像素独立的</li><li>在给定信号 <span class="math inline">\(\boldsymbol{s}\)</span>的情况下，噪声 <span class="math inline">\(\boldsymbol{n}\)</span>是有条件的像素独立的</li></ul></li><li>虽然不能期望该方法优于训练过程中有额外信息可用的方法，但可以观察到，N2V结果的去噪性能仅略有下降，而且仍然优于BM3D<ul><li>但由于可以N2V方法的使用条件十分简单，所以在一些场景中能够发挥巨大的实用价值</li></ul></li><li>主要贡献：<ul><li>引入NOISE2VOID，一种新的训练去噪cnn的方法，<strong>只需要单一的、有噪声的图像</strong></li><li>将N2V训练的去噪结果与现有CNN训练方案和未经训练的方法的结果进行比较</li><li>该方法具有良好的理论动机，并且对有效实施方式进行了详细说明</li></ul></li><li>使用N2V时，我们必须坚持更窄的去噪任务，因为我们依赖于这样一个事实，即<strong>多噪声观测可以帮助我们检索真实信号</strong>。但对于诸如 模糊blur 之类的一般扰动则不是这种情况</li><li>我们在多个方法类别的交集中看到了N2V，我们将简要讨论其中每一个的最相关的作品<ul><li>如上所述，这里省略了N2N</li><li>Batson等人介绍了一种基于去除部分输入思想的神经网络和其他系统的自我监督训练方法。他们表明，这种方案不仅可以用于去除像素，也可以用于一般的变量组</li><li><strong>Discriminative Deep LearningMethods，判别式深度学习方法：</strong><ul><li>Jain等首先使用cnn进行去噪任务。他们介绍了今天仍然被成功的方法所使用的基本设置：去噪被视为一个回归任务，CNN学习将其预测和干净的groundtruth真相数据之间计算的损失最小化</li><li>Zhang等人通过引入一种非常深的CNN架构去噪，获得了最先进的结果。该方法基于残差学习的思想。他们的CNN试图预测的不是干净的信号，而是每个像素处的噪声，以便在后续步骤中对信号进行计算。这种结构允许他们训练一个单一的CNN去噪被不同的噪声级别损坏的图像。它们的架构完全省去了池化层</li><li>大约在同一时间，Mao等人为去噪任务引入了一种互补的非常深入的编码器-解码器架构。它们也利用残差学习，但通过在相应的编码和解码模块之间引入对称的跳跃连接来做到这一点。他们能够使用一个单一的网络来处理不同级别的噪声</li><li>Tai等人将循环持久记忆单元作为其体系结构的一部分，并进一步改进了以前的方法</li><li>最近，Weigert等人提出了用于荧光显微镜数据背景下图像恢复的CARE软件框架。他们通过记录成对的低曝光和高曝光图像来获取训练数据。这可能是一个困难的过程，因为生物样本不能在两次暴露之间移动。我们使用他们的实现作为我们实验的起点，包括他们特定的U-Net架构</li></ul></li><li><strong>Internal Statistics Methods，内部统计方法：</strong><ul><li>内部统计方法不需要事先根据groundtruth数据进行训练。相反，他们可以直接应用到一个测试图像，他们提取所有需要的信息。N2V可以被视为这一类别的成员，因为它可以直接对测试图像进行训练</li><li>Buades等人引入了非局部均值，这是一种经典的去噪方法。与N2V一样，该方法根据噪声环境预测像素值</li><li><strong>BM3D是Dabov等人提出的一种经典的基于内部统计的方法</strong>。它基于这样一种理念，即自然图像通常包含重复的图案。BM3D对图像进行去噪处理的方法是<strong>将相似的模式分组并进行联合滤波</strong>。这种方法的缺点是<strong>测试期间的计算成本</strong>。相比之下，<strong>N2V只需要在训练期间进行大量计算。一旦一个CNN被训练为一种特定的数据，它可以有效地应用到任何数量的附加图像</strong></li><li>Ulyanov等人表明，cnn的结构与自然图像的分布有内在的共鸣，可以用于图像恢复，而不需要额外的训练数据。他们将一个随机但恒定的输入输入到一个CNN中，然后训练它来近似一个单一的有噪声的图像作为输出。Ulyanov等人发现，当他们在收敛前的正确时刻中断训练过程时，网络产生一幅正则化去噪图像作为输出</li></ul></li><li><strong>Generative Models，生成模型：</strong><ul><li>Chen等人提出了一种基于生成对抗网络(GANs)的图像恢复方法。作者使用未配对的训练样本组成的噪声和干净的图像。GAN生成器学习生成噪声并创建对应的干净和有噪声图像对，这些图像反过来在传统监督设置中用作训练数据。与N2V不同的是，这种方法在训练过程中需要干净的图像</li><li>Van DenOord等人提出了一个生成模型，不过并不是用于去噪，但在思想上类似于N2V。像N2V一样，他们训练一个神经网络，<strong>根据其周围环境预测一个看不见的像素值</strong>。然后，该网络被用来生成合成图像。然而，当我们为回归任务训练我们的网络时，它们预测每个像素的概率分布。另一个区别在于感受野的结构。当他们使用在图像上移动的不对称结构时，我们总是<strong>在正方形感受野中掩盖中心像素</strong></li></ul></li></ul></li></ul><h3 id="methods">Methods</h3><ul><li><p><strong>Image Foamation，图像构成：</strong></p><ul><li>图像 <span class="math inline">\(\boldsymbol{x} = \boldsymbol{s}+  \boldsymbol{n}\)</span> 的生成是由联合分布绘制的：</li><li><span class="math inline">\(p(\boldsymbol{s},\boldsymbol{n}) =p(\boldsymbol{s})p(\boldsymbol{n}|\boldsymbol{s})\)</span></li><li>假设 <span class="math inline">\(p(\boldsymbol{s})\)</span>是一个任意分布，满足以下条件：</li><li><span class="math inline">\(p(s_i|s_j) \not = p(s_i)\)</span></li><li>对于彼此在一定半径内的两个像素 <span class="math inline">\(i\)</span> 和 <span class="math inline">\(j\)</span> ，<strong>也就是说信号 <span class="math inline">\(\boldsymbol{s}_i\)</span>的像素在统计上不是独立的</strong></li><li>关于噪声 <span class="math inline">\(\boldsymbol{n}\)</span>，我们假设是以下形式的条件分布</li><li><span class="math inline">\(p(\boldsymbol{n}|\boldsymbol{s}) =\prod_{i} p\left(\boldsymbol{n}_{i} \mid \boldsymbol{s}_i\right)\)</span></li><li><strong>也就是说噪声的像素值 <span class="math inline">\(\boldsymbol{n}_i\)</span>在给定信号的情况下是条件独立的</strong></li><li>进一步假设<strong>噪声为零均值</strong>，从而可以得到</li><li><span class="math inline">\(\mathbb{E}[\boldsymbol{n}_i] = 0\\\mathbb{E}[\boldsymbol{x}_i] = \boldsymbol{s}_i\)</span></li><li>也就是说，<strong>如果我们获取多个图像，相同的信号，但不同的噪声实现，并将它们平均，结果将接近真实的信号</strong><ul><li>这方面的一个例子是使用固定的三脚架相机记录静态场景的多张照片</li></ul></li></ul></li><li><p><strong>Traditional SupervisedTraining，传统的监督训练：</strong></p><ul><li>训练一个CNN来实现从 <span class="math inline">\(\boldsymbol{x}\)</span> 到 <span class="math inline">\(\boldsymbol{s}\)</span>的映射。我们将假设一个全卷积网络(FCN)，以一幅图像作为输入，预测另一幅作为输出</li><li>在这里，我们想对这样一个网络采取稍微不同但相同的观点：<ul><li>CNN的<strong>输出中每个像素预测 <span class="math inline">\(\hat{\boldsymbol{s}}_i\)</span> 具有输入像素的一定receptive field 接受场<span class="math inline">\(\boldsymbol{x}_{RF(i)}\)</span>，即影响像素预测的像素集合</strong>。一个像素的接受场通常是像素周围的一个正方形区域</li><li><strong>基于此考虑，我们也可以将CNN看作一个函数，以patch <span class="math inline">\(\boldsymbol{x}_{RF(i)}\)</span>作为输入，对patch中心的单个像素 <span class="math inline">\(i\)</span>输出预测 <span class="math inline">\(\hat{\boldsymbol{s}}_i\)</span></strong></li></ul></li><li>按照这个思路，可以通过提取重叠的小块，并将其逐个输入到网络中，来实现对整个图像的去噪。因此，我们可以将CNN定义为函数<ul><li><span class="math inline">\(f(\boldsymbol{x}_{RF(i)};\theta) = \hat{\boldsymbol{s}}_i\)</span></li><li>其中 <span class="math inline">\(\theta\)</span>为我们要训练的CNN参数的向量</li></ul></li><li>在传统的监督训练中，我们得到一组训练对 <span class="math inline">\(\left(\boldsymbol{x}^{j},\boldsymbol{s}^{j}\right)\)</span> ，每个训练对由一个有噪声的输入图像<span class="math inline">\(x^j\)</span> 和一个干净的ground truth目标<span class="math inline">\(s^j\)</span> 组成</li><li>通过应用我们基于patch的CNN视图，我们可以看到我们的训练数据成对 <span class="math inline">\(\left(\boldsymbol{x}_{RF(i)}^{j},\boldsymbol{s}_i^{j}\right)\)</span><ul><li>其中 <span class="math inline">\(\boldsymbol{x}_{RF(i)}^j\)</span>是从训练输入图像 <span class="math inline">\(\boldsymbol{x}^j\)</span>中提取的像素 <span class="math inline">\(i\)</span> 周围的一个patch，<span class="math inline">\(\boldsymbol{s}_i^j\)</span>为对应的目标像素值（从同位置的ground truth图像 <span class="math inline">\(\boldsymbol{s}^j\)</span> 中提取）</li></ul></li><li>我们现在使用这些对来调整参数 <span class="math inline">\(\theta\)</span> 以最小化像素损失<ul><li><span class="math inline">\(\underset{\boldsymbol{\theta}}{\arg \min} \sum_{j} \sum_{i} L\left(f\left(\boldsymbol{x}_{\mathrm{RF}(i)}^{j} ;\boldsymbol{\theta}\right)=\hat{\boldsymbol{s}}_{i}^{j},\boldsymbol{s}_{i}^{j}\right)\)</span></li><li>这里我们考虑标准的均方误差损失MSE loss</li><li><span class="math inline">\(L\left(\hat{s}_{i}^{j},s_{i}^{j}\right)=\left(\hat{s}_{i}^{j}-s_{i}^{j}\right)^{2}\)</span></li></ul></li></ul></li><li><p><strong>Noise2Noise Training：</strong></p><ul><li>N2N使我们能够应付没有ground truth训练数据的情况。相反，我们从嘈杂的图像对 <span class="math inline">\((\boldsymbol{x}^j,\boldsymbol{x}^{\prime j})\)</span>开始，其中<ul><li><span class="math inline">\(\boldsymbol{x}^{j}=\boldsymbol{s}^{j}+\boldsymbol{n}^{j}\)</span>and <span class="math inline">\(\boldsymbol{x}^{\primej}=\boldsymbol{s}^{j}+\boldsymbol{n}^{\prime j}\)</span>,</li><li>也就是说，这两幅训练图像在噪声分量 <span class="math inline">\(\boldsymbol{n}^{j}\)</span> 和 <span class="math inline">\(\boldsymbol{n}^{\prime j}\)</span>上是相同的，在我们的图像生成模型中，它们只是来自同一分布的两个独立样本</li></ul></li><li>现在，我们可以再次应用基于patch的视角，将训练数据视为对 <span class="math inline">\(\left(\boldsymbol{x}_{RF(i)}^{j},\boldsymbol{x}_i^{\prime j}\right)\)</span> ，由从 <span class="math inline">\(\boldsymbol{x}_j\)</span> 提取的噪声输入patch<span class="math inline">\(\boldsymbol{x}_{RF(i)}^j\)</span> 和从 <span class="math inline">\(\boldsymbol{x}^{\prime j}\)</span> 的位置 <span class="math inline">\(i\)</span> 提取的噪声目标 <span class="math inline">\(\boldsymbol{x}_i^{\prime j}\)</span> 组成</li><li>使用 <span class="math inline">\(\underset{\boldsymbol{\theta}}{\arg\min } \sum_{j} \sum_{i}L\left(f\left(\boldsymbol{x}_{\mathrm{RF}(i)}^{j} ;\boldsymbol{\theta}\right)=\hat{\boldsymbol{s}}_{i}^{j},\boldsymbol{s}_{i}^{j}\right)\)</span>来最小化损失，但这里使用的是噪声目标 <span class="math inline">\(\boldsymbol{x}_i^{\prime j}\)</span>，而不是ground truth <span class="math inline">\(\boldsymbol{s}_i^j\)</span><ul><li>即使我们试图学习从噪声输入到噪声目标的映射，训练仍然会收敛到正确的解</li><li>这一现象的关键在于<strong>噪声输入的期望值等于干净的信号</strong></li></ul></li></ul></li><li><p><strong>Noise2Void Training：</strong></p><p><img src="/2021/04/11/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-0x14/3.png"></p><ul><li>这里我们更进一步。我们建议<strong>从单一的有噪声的训练图像 <span class="math inline">\(\boldsymbol{x}^j\)</span>中获得我们的训练样本的两个部分：输入和目标</strong></li><li>如果我们简单地提取一个patch作为输入，并使用它的中心像素作为目标。那么我们的网络将直接将<strong>输入色块中心的值映射到输出</strong>中来学习标识</li><li>为此我们假设该网络的接收域 <span class="math inline">\(\tilde{\boldsymbol{x}}_{RF(i)}^j\)</span> 在<strong>其中心有一个blind-spot盲点</strong></li><li>这样改网络受正方形邻域内除其所在位置的输入像素 <span class="math inline">\(\boldsymbol{x}_i\)</span> 之外的所有输入像素的影响<ul><li>我们把这种网络称为盲点网络</li></ul></li><li>盲点网络可以使用上述任何一种训练方案进行训练<ul><li>与普通的网络一样，我们可以分别使用一个干净的目标或一个有噪声的目标来进行传统的训练或N2N</li><li>盲点网络可用于预测的信息要少一些，我们可以预期，与正常网络相比盲点网络的准确性会略微下降</li><li>然而考虑到整个接受场中只有一个像素被移除，我们可以假设它仍然表现得相当好</li></ul></li><li>盲点体系结构的基本优点是它无法学习自身（ The essential advantage ofthe blind-spot architecture is its inability to learn the identity ）<ul><li>由于我们假设<strong>噪声在给定信号的情况下是逐像素独立的，因此相邻像素不携带有关<span class="math inline">\(\boldsymbol{n}_i\)</span>值的信息</strong></li><li>因此，网络不可能产生优于其先验期望值的估计值</li><li>然而，<strong>信号被假定包含统计相关性。因此，网络仍然可以通过观察其周围环境来估计一个像素的信号</strong><span class="math inline">\(\boldsymbol{s}_i\)</span></li></ul></li><li>因此，盲点网络允许我们<strong>从相同的有噪声的训练图像中提取输入的patch和目标值</strong>。我们可以通过最小化经验风险来训练它<ul><li><span class="math inline">\(\underset{\boldsymbol{\theta}}{\arg \min} \sum_{j} \sum_{i}L\left(f\left(\tilde{\boldsymbol{x}}_{\mathrm{RF}(i)}^{j} ;\boldsymbol{\theta}\right), \boldsymbol{x}_{i}^{j}\right)\)</span></li></ul></li><li>我们已经看到盲点网络原则上可以只用单个的有噪声的训练图像来训练，为此作者提出了一种mask方案，用任何标准的CNN都能达到相同的性能：我们用<strong>从周围区域随机选取的值来替换每个输入patch中心的值</strong>。这有效地消除了像素的信息，并防止网络学习到自身</li></ul></li><li><p><strong>Implementation Details：</strong></p><ul><li>如果我们天真地执行上面的训练方案，不幸的是它仍然不是非常有效：我们必须处理整个patch来计算单个输出像素的梯度。为了缓解这个问题，我们使用下面的近似技术<ul><li>给定一个有噪声的训练图像 <span class="math inline">\(\boldsymbol{x}_i\)</span> ，我们随机提取大小为64 ×64像素的小块，这比我们网络的感受野大</li><li>在每个patch中，我们随机选择N个像素，采用分层采样避免聚类</li><li>然后我们屏蔽这些像素，并<strong>使用原始的噪声输入值作为它们所在位置的目标</strong></li><li>这样我们可以同时计算它们所有的梯度，同时忽略预测图像的其余部分</li><li>这是通过使用标准的Keras流水线实现的，该流水线带有专门的损失函数，除选定的像素外，该函数对所有像素都为零</li></ul></li></ul></li></ul><h3 id="experiment">Experiment</h3><ul><li>实验实现的部分就不写了，效果没有很大的提升，不过在一些特殊领域中，可以发挥他的作用</li></ul><p><img src="/2021/04/11/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-0x14/4.png"></p><ul><li>图5:N2V训练网络的故障案例<ul><li>(a)地面真值测试图像中单个像素误差最大的作物(用红色箭头表示)</li><li>(b)传统训练的网络在同一图像上的结果</li><li>(c)我们N2V训练网络的结果。网络无法预测这个明亮和孤立的像素</li><li>(d)总误差最大的ground truth测试图像中的作物</li><li>(e)传统训练的网络在同一图像上的结果</li><li>(f)我们N2V训练网络的结果</li><li>两种网络都不能保留图像的颗粒结构，但N2V训练的网络丢失了更多的高频细节</li></ul></li><li>在这一部分最后，作者也指出了该方法的很多问题<ul><li>传统网络和N2V都不能保留图像的颗粒结构</li><li>N2V训练的网络丢失了更多的高频细节</li></ul></li><li>作者认为这些错误是一个很好的说明，说明了N2V方法的局限性——<strong>N2V的一个基本假设是信号的可预测性</strong><ul><li>图5中显示的两个测试图像都包含了难以预测的高度不规则性</li><li><strong>从周围环境预测一个像素的信号越困难，N2V预测的误差预计就越大</strong>，当然，对于传统训练和N2N也是如此</li><li>然而，传统方法可以利用感受野中心像素的值，但这个值被N2V mask了</li></ul></li></ul><p><img src="/2021/04/11/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-0x14/5.png"></p><ul><li>图6:结构噪声对N2V训练网络预测的影响结构噪声违背了我们认为噪声与像素无关的假设(参见公式3)。<ul><li>(a)一张被结构噪声污染的照片。隐藏的棋盘图案几乎看不见</li><li>(b)传统训练的CNN去噪结果</li><li><ol start="3" type="a"><li>N2V训练的CNN去噪结果。噪声的独立成分被去除，但结构成分保留</li></ol></li><li>(d)真实显微镜数据中的结构化噪声</li><li>(e)经过N2V训练的CNN去噪结果。噪音中隐藏的模式被揭示出来了</li><li>注意，由于缺乏训练数据，在这种情况下不可能使用N2N或传统的训练方案</li></ul></li><li>在图6中，我们说明了我们方法的另一个限制——<strong>N2V无法区分违背像素独立性假设的信号和结构噪声</strong><ul><li>我们通过将人工生成的结构噪声应用于图像来演示这一行为</li><li>N2V训练的CNN去除了噪声中不可预测的成分，但揭示了隐藏的模式</li><li>有趣的是，我们在Fluo-C2DL-MSC数据集的真实显微镜数据中发现了同样的现象</li><li>用N2V训练的CNN去噪揭示了<strong>成像系统的系统性错误，可见为条纹图案</strong></li></ul></li><li><img src="/2021/04/11/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-0x14/6.png"></li><li>总结：<ul><li>作者提出了NOISE2VOID，一种新的训练方案，它只需要单一的噪声采集来训练去噪的cnn<ul><li>证明了N2V在各种成像方式上的适用性，如摄影、荧光显微镜和低温透射电子显微镜</li><li>只要我们的信号可预测和像素独立噪声的初始假设得到满足，N2V训练的网络就可以与传统的和N2N训练的网络竞争</li><li>此外，我们还分析了违反这些假设时N2V训练的行为</li></ul></li><li>作者相信，正如我们在这里提出的NOISE2VOID训练方案，将允许我们训练强大的去噪网络<ul><li>我们已经展示了多个例子，如何在首先要处理的同一数据体上训练去噪网络</li><li>因此，N2V训练将为大量应用打开大门，例如生物医学图像数据</li></ul></li></ul></li></ul><h3 id="section-1">😝😜😋</h3><p>这样也是篇很神奇的文章，在N2N的基础上更进了一步。其实文章的主题把这个已经介绍的很详细了，我就不想在单独写了，相关工作部分也说了很多。不过主要思想其实就是那个图，使用一个mask思想加上N2N，真的很巧妙。</p><p>虽然实验结果不是很棒，但他的应用场景还是很多的，而且效果也要好于非训练方法。</p><p>这两篇文章的思想真的都特别创新感觉，尤其是N2N，然后在此基础上进一步设计得到N2V，可能直觉感觉不太可能，但如果去看数学理论部分的话，真的是可以实现的，而且效果也是有的。同时这几个实验也都做的好棒，还能对应的到一些有趣的现象解释，真的神奇~</p>]]></content>
    
    
    <categories>
      
      <category>study</category>
      
    </categories>
    
    
    <tags>
      
      <tag>图像增强与图像恢复</tag>
      
      <tag>高光谱图像去噪</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>论文阅读-0x13</title>
    <link href="/2021/04/06/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-0x13/"/>
    <url>/2021/04/06/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-0x13/</url>
    
    <content type="html"><![CDATA[<p>论文的阅读笔记：</p><p>《 HYPERSPECTRAL IMAGE DENOISING BASED ON MULTI-STREAM DENOISINGNETWORK 》, 高琰师姐的一篇论文&amp;代码</p><p>《 Joint Demosaicing and Denoising with Self Guidance 》，CVPR2020</p><p>主要是学习一下师姐的这份代码，然后老师说可以在这个基础上多改改，也是学习学习~</p><span id="more"></span><h2 id="hyperspectral-image-denoising-based-on-multi-stream-denoising-network">HYPERSPECTRALIMAGE DENOISING BASED ON MULTI-STREAM DENOISING NETWORK</h2><h3 id="abstract">Abstract</h3><ul><li>提出了一种噪声估计子网和去噪子网组成的网络，基于multi-stream去噪网络的盲去噪方法<ul><li>在噪声估计子网络中，设计了一个多尺度融合模块来捕获不同尺度下的噪声</li><li>然后利用去噪子网得到最终去噪图像</li><li>emm很像CBDNet</li></ul></li><li>Hyperspectral images(HSIs)通常由许多通道组成，从可见光谱到红外光谱。HSIs能同时获取空间和光谱信息，比其他数据源提供更丰富的场景信息</li><li>传统的HSI去噪方法通常采用逐波段去噪策略<ul><li>将每个波段看作是一幅二维图像，然而忽略了光谱信息，这些策略往往会导致较大的光谱畸变</li></ul></li><li>与传统的二维图像去噪方法不同，块匹配4-D滤波算法是一种适用于HSIs的三维图像去噪方法。但是，该方法没有考虑到不同波段间噪声分布的不一致性，导致频谱域性能较差</li><li>深度学习中大多是基于特定的噪声级别去噪，一旦噪声级别发生变化，就很难达到预期的性能，导致去噪过低或过低<ul><li>为了解决这一问题，研究人员成功地采用了盲去噪技术</li><li>一方面，利用<strong>包含不同噪声级别的噪声图像的非常大的训练数据集</strong>对去噪模型进行优化，但现有的网络要同时学习所有类型的噪声可能是非常困难的</li><li>另一方面，<strong>引入噪声估计或噪声标记来指导去噪过程</strong>。Zhang等人提出了一种快速灵活的去噪网络(FFDNet)，在图像去噪中表现出了相应的性能。FFDNet结合噪声图像和噪声级估计去除复杂噪声，从而达到去噪的鲁棒性。然而，不确定的噪声水平仍然会影响上述方法的性能</li><li>因此，设计一种广义的去噪模型具有很大的挑战性</li></ul></li></ul><h3 id="methodology">METHODOLOGY</h3><p><img src="/2021/04/06/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-0x13/1.png"></p><ul><li>该方法的框架如图所示。它包括两个子网:1)噪声估计子网和2)去噪子网</li><li>噪声估计子网<ul><li>为了捕获噪声特征，设置了三个不同核大小的多尺度模块，每个多尺度模块由六个block组成<ul><li><img src="/2021/04/06/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-0x13/2.png"></li><li><span class="math inline">\(\mathbf{M}_{i}=\operatorname{cat}\left[B_{1},B_{2}, \ldots, B_{6}\right]\)</span><ul><li><span class="math inline">\(\mathbf{M}_{i}\)</span>表示第i个多尺度模块，<span class="math inline">\(B_j\)</span>表示第j个块</li></ul></li></ul></li><li>这三个多尺度模块的大小分别为3×3、5×5和7×7，然后将输出连接在一起</li><li>利用在金字塔池化模块进一步得到多尺度描述，然后将四个通道输出再连接在一起</li><li>为了学习通道之间的关系特征图，使用global max pooling(GMP)在空间域上对特征表示P进行挤压，得到一个空间向量 <span class="math inline">\(V \in \mathbb{R}^{4C \times 1 \times 1}\)</span>，然后将V输入到两层全连接得到一个权重向量S（上面那条通路了）</li><li>最后用S对P进行加权，得到噪声估计</li></ul></li><li>去噪子网<ul><li>使用一个UNet</li></ul></li><li>loss<ul><li>特征提取和去噪都是在端到端框架下进行的。因此，采用均方误差(meansquared error,MSE)损失和感知损失作为基本损失函数来指导去噪模型的学习</li><li>MSE loss<ul><li><span class="math inline">\(\mathcal{L}_{M}=M S E\operatorname{Loss}(G, D)=\frac{1}{C H W} \sum_{t=1}^{C HW}\left(G_{t}-D_{t}\right)^{2}\)</span></li></ul></li><li>the perceptual loss<ul><li><span class="math inline">\(\mathcal{L}_{P}=\ell^{\phi, j}(G,D)=\frac{1}{C_{j} H_{j}W_{j}}\left\|\phi_{j}(G)-\phi_{j}(D)\right\|_{2}^{2}\)</span></li></ul></li><li>然后像CBDNet那样，使用了非对称损失，对于过高和过低的噪声损失采用不同的惩罚（非对称）<ul><li><span class="math inline">\(\mathcal{L}_{\text {asymm}}=\sum_{i}\left|\alpha-\mathbb{I}_{\left(N_{i}-N_{i}^{\prime}\right)&lt;0}\right|\cdot\left(N_{i}-N_{i}^{\prime}\right)^{2}\)</span></li><li>根据经验将α设置为0.25</li></ul></li><li>综上<ul><li><span class="math inline">\(\mathcal{L} = \mathcal{L}_{M} +\mathcal{L}_{P} + \lambda_{asymm} \mathcal{L}_{asymm}\)</span></li></ul></li></ul></li></ul><h3 id="experiment">Experiment</h3><ul><li>ICVL数据集<ul><li>该数据集包含201幅图像。ICVL数据集的空间分辨率为1392 ×1300，覆盖31个光谱通道</li><li>为了扩展训练数据集，将每幅训练图像裁剪成大小为64 × 64 ×31的多个patches</li></ul></li><li>使用Pavia Centre数据集来微调模型<ul><li>在测试部分，在Pavia University数据集上对模型进行了评估</li></ul></li><li>这两个数据集是通过ROSIS传感器获取的，经过处理后，PaviaCentre图像的大小为1096×715×102，而PaviaUniversity图像的大小为610×340×103</li><li>采用batch为64的Adam优化器对该方法进行了优化。学习速率初始化为 <span class="math inline">\(10^{−4}\)</span> ，使用0.0005的权重衰减</li><li>基于以上设置，对网络进行100个epoch的训练</li><li>对于噪声设置，添加噪声分为两种情况<ul><li>添加噪声等级为30,50,70的AWGN</li><li>随机添加噪声级别为10-70的AWGN</li></ul></li></ul><h3 id="section">😝😜😋</h3><p>emm代码和论文部分都挺简单的，上次值班的时候遇到师姐，师姐说是发的一个很水的会议，感觉确实还是满水的，不过自己也没啥具体要做的东西，所以最近还是想着多看一些代码和对应的论文实现，然后去找找灵感，多看，多做记录，然后可以拿到多光谱这方面去尝试应用一下</p><h2 id="joint-demosaicing-and-denoising-with-self-guidance">JointDemosaicing and Denoising with Self Guidance</h2><h3 id="abstract-1">Abstract</h3><ul><li>去马赛克和去噪通常位于计算摄影流程的早期阶段，在现代相机图像处理中起着重要的作用</li><li>最近，一些神经网络已经显示出联合去马赛克和去噪（JDD）的有效性</li><li>大多数方法首先将一个拜耳原始图像（Bayer rawimage）分解成一个四通道的RGGB图像，然后将其送入一个神经网络<ul><li>这种做法忽略了一个事实，即与红色和蓝色通道相比，绿色通道的采样率是双倍的</li></ul></li><li>在本文中，作者提出了一种自导引网络(self-guidance network, SGNet)<ul><li>在该网络中，首先估计绿色通道，然后作为一种引导来恢复输入图像中的所有缺失值</li><li>此外，由于不同频率的区域在图像恢复过程中存在不同程度的退化。作者提出一个density-map（密度地图）指导，以帮助模型处理广泛的频率范围</li></ul></li><li>图像去马赛克是图像信号处理(ISP)流程中的开始模块之一，是计算机视觉中的一个基本问题<ul><li>它旨在从彩色滤波器阵列（CFA）（例如RGGB的Bayer图案）丢失了三分之二的图像信息后，在不完整的观察中重建全分辨率彩色图像</li></ul></li><li>传统的去马赛克和去噪处理是按顺序进行的。然而，最近的研究显示了联合方法的优点</li><li>深度学习方法能够有效地利用原始图像中通道内和通道间的依赖性来完成缺失的信息<ul><li>这些方法中的大多数将Bayer原始图像分解为四通道的RGGB图像，并将其送入神经网络</li><li>利用卷积神经网络本身来发现RGGB通道之间的关系，这样它们就可以互相补充来恢复缺失的像素值</li><li>然而，原始Bayer图像中红色，绿色和蓝色通道的采样率不同，没有得到充分利用。与红色和蓝色通道相比，绿色通道以双倍速率采样</li><li><strong>充分利用绿色通道中的信息，有利于缺失像素值的恢复</strong></li><li>先恢复绿色通道，再利用绿色通道与红色通道之间、绿色通道与蓝色通道之间的通道间相关性来恢复RGB通道</li></ul></li><li>在这项工作中，受这些方法的启发，作者提出了一个卷积神经网络(CNN)来明确探索输入Bayer原始图像本身的指导，用于联合去噪和去马赛克</li><li>一个典型的CNN对所有位置和所有图像应用相同的参数集，而不管图像中的具体内容是什么<ul><li>不过频率和噪声因图像和位置而异，因此，这种内容不可知的操作将限制神经网络的泛化能力及其处理马赛克和去噪任务的能力</li><li>在本研究中，与传统方法类似，作者首先仅基于绿色通道进行初始估计</li><li>初始恢复的绿色通道作为引导，在图像上进行空间自适应卷积</li><li>这样，丰富的信息和绿色通道得到整合，并在不同的位置进行不同的运用</li></ul></li><li>此外，作者发现<strong>不同频率的区域在图像重建中存在不同程度的困难</strong><ul><li>在去噪和去噪的过程中，通过模型来判断哪个区域比较难处理是很有帮助的</li><li>这与图像去噪中的噪声图有一些相似之处，因为<strong>不同的区域和不同的图像所面临的挑战是不同的</strong></li><li><strong>估计图像的密度图并将其输入到模型中，使模型能够处理广泛的频率范围</strong></li></ul></li><li>主要贡献如下：<ul><li>提出了一种基于density-map（密度图）和green-channel（绿色通道）引导的自导引网络(SGNet)，用于联合去马赛克和去噪</li><li>提出了自适应阈值边缘损失和边缘感知平滑损失两种方法来同时恢复纹理和去除噪声</li><li>在合成数据和真实数据集上的定量和定性实验结果表明，模型优于最先进的方法</li></ul></li><li>图像去马赛克用于从一个彩色滤波器阵列的下采样输出中重建一个完整的彩色图像<ul><li>在实际应用中，由于Bayer模式经常受到噪声的干扰，通常不能独立进行马赛克处理</li><li>考虑到实际情况并受益于混合问题处理，越来越多的研究开始同时进行去马赛克和去噪</li><li>联合去马赛克和去噪可以消除处理后的错误累积</li></ul></li><li>引导图像恢复<ul><li>在许多图像恢复任务中，很多以前的工作都是利用外部信息来恢复图像，这些方法也都验证了外部制导信息对图像恢复的重要性</li><li>然而对于图像恢复的自我引导策略的研究却很少，自我引导信息通常很难挖掘，有时需要一些先验知识<ul><li>Gu等人提出了一种快速图像去噪的自引导网络，其中低分辨率分支的特征可以引导高分辨率分支的特征</li></ul></li><li>本文针对去马赛克任务提出了两种自引导方法，即绿色通道引导和密度图引导</li></ul></li></ul><h3 id="the-proposed-method">The Proposed Method</h3><p><img src="/2021/04/06/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-0x13/3.png"></p><ul><li>为了在联合去马赛克和去噪的任务中取得良好的性能，充分利用输入的RGGB原始图像中的信息至关重要</li><li>在这里，作者建议探索输入图像中的绿色通道和密度信息，并使用它们作为更好的性能指导</li><li>RGGB原始输入图像 <span class="math inline">\(I_{RGGB}^{Bayer} \in\mathbb R^{2H \times 2H}\)</span> 可以分解得到四个通道 <span class="math inline">\(I_{G1}^{Bayer}, I_{G2}^{Bayer}, I_{R}^{Bayer},I_{B}^{Bayer} \in \mathbb R^{H \times W \times 1}\)</span>，对应于Bayer模式的四个元素</li><li>首先对绿色通道的缺失元素进行初步估计，因为在输入的两个绿色通道 <span class="math inline">\(I_{G1}^{Bayer}, I_{G2}^{Bayer}\)</span>中有更丰富的信息</li><li>绿色通道 <span class="math inline">\(\hat I_G\)</span>的初始估算作为一个指导，并应用到主分支，帮助恢复所有通道的缺失元素</li><li>此外计算密度图 <span class="math inline">\(M_D\)</span>来表示不同区域的去噪难易水平，并将该密度图作为额外的输入输入到主分支网络</li><li>最后给出了用于训练整个模型的损失</li></ul><h4 id="density-map-guidance">Density-map guidance</h4><ul><li>通常图像中既有高频率的复杂区域，也有低频率的平滑区域。这些区域对联合去马赛克去噪的任务提出了不同的挑战<ul><li>因此，盲目地对整个图像进行同样的处理是次优的</li><li>受噪声图在图像去噪中成功的启发，作者设计了密度图，让网络知道输入图像每个位置的难度等级</li><li>在密度图中，<strong>纹理密集的区域对应高频图案，纹理较少的区域对应低频图案</strong></li></ul></li><li><img src="/2021/04/06/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-0x13/4.png"></li><li>计算方法如下<ul><li>⭐ <span class="math inline">\(M_D = h(g_2(I_{gray} -g_1(I_{gray};K_1);K_2))\)</span></li><li>其中 <span class="math inline">\(g_1, g_2\)</span> 为使用不同大小核<span class="math inline">\(K_1, K_2\)</span> 的高斯模糊操作，<span class="math inline">\(h( \cdot )\)</span> 是非线性函数， <span class="math inline">\(I_{gray}\)</span> 是四个解耦信道的平均值</li><li><span class="math inline">\(h(X) = \frac {X-min(X)} {max(X) - min(X)+ \epsilon}\)</span></li><li><span class="math inline">\(I_{gray} = ((I_{G1}^{Bayer} +I_{G2}^{Bayer})/2 + I_{R}^{Bayer} + I_{B}^{Bayer})/3\)</span></li></ul></li><li>对图像应用高斯模糊操作 <span class="math inline">\(g_1\)</span>后，纹理密集的区域会变得模糊，输入和输出之间的差异会非常大</li><li>然后对差异图应用另一个高斯模糊操作 <span class="math inline">\(g_2\)</span> ，以得到一个平滑的密度图</li><li>通过非线性函数将其进一步归一化到0到1的范围内，<strong>如上图所示</strong></li><li>然后将该估计密度图与四个解耦通道连接，作为主要重建分支的输入</li><li>噪声图 <span class="math inline">\(I^{noise}\)</span>表示训练过程中添加的高斯噪声的水平</li></ul><h4 id="green-channel-guidance">Green-channel guidance</h4><ul><li><p><img src="/2021/04/06/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-0x13/5.png"></p></li><li><p>首先在Bayer图像中每2×2块中的两个位置提取绿色像素，并获得两个绿色通道，将它们与噪声图一起送到绿色通道重建分支以产生RGB输出图像的绿色通道的初始结果</p></li><li><p>绿色通道重构分支由一个Residual-in-Residual Dense Block(RRDB)组成</p><ul><li>RRDB包含多层Residual连接和Dense连接</li></ul></li><li><p>在这个分支的末端使用一个depth-to-spacelayer（深度-空间层）来得到一个重建的绿色通道 <span class="math inline">\(\hat I_G \in \mathbb R^{2H \times 2W \times1}\)</span></p><ul><li>去马赛克任务与超分辨率任务有一些相似之处，因为这两个任务都需要恢复缺失的元素，并且基本上需要进行2倍的上采样</li><li>以前的很多方法，直接将这个重建绿色通道直接加入到原始输入 <span class="math inline">\(I_{RGGB}^{Bayer}\)</span>中，这样在每个位置进行相同的卷积操作。作者认为这样不是最优的</li></ul></li><li><p>为此作者提出用 <span class="math inline">\(\hat I_G\)</span>以一种新颖的方式引导主重构分支</p></li><li><p><img src="/2021/04/06/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-0x13/6.png"></p></li><li><p>将空间自适应卷积运算应用于主重构分支的中间特征映射，使集成过程适应绿色通道的内容</p><ul><li>空间自适应卷积是内容感知的，在不同的位置进行不同的卷积</li><li>它能够增加网络的容量，并有助于处理图像中不同的频率和噪声</li><li>对于主重构分支 <span class="math inline">\(\mathbf{v}_{j} \in\mathbb{R}^{c^{\prime}}, j=1,2, \ldots, H \times W\)</span>的中间特征图中位置j的特征，空间自适应卷积 <span class="math inline">\(\mathbf{o}_{i} \in \mathbb{R}^{c}, i=1,2, \ldots,2H \times 2W\)</span> 的输出计算如下</li><li><span class="math inline">\(\mathbf{o}_{i} = \sum_{j \in \Omega(i)}\left\{G(\mathbf{f}_{1}, \mathbf{f}_{2})\mathbf{W}_{p_{i,j}}\mathbf{v}_{j} + \mathbf{b} \right\}\)</span></li><li><span class="math inline">\(\Omega(\cdot)\)</span> 定义了一个s ×s大小的卷积窗口， <span class="math inline">\(\mathbf{W} \in\mathbb{R}^{c&#39; \times c \times s \times s}\)</span> 是卷积权重，<span class="math inline">\(p_{i,j}\)</span> 是 <span class="math inline">\(\mathbf{W}\)</span> 与 <span class="math inline">\(i,j\)</span> 相关的索引， <span class="math inline">\(\mathbf{b} \in \mathbb{R}^{c&#39;}\)</span>表示偏差， <span class="math inline">\(\mathbf{f}_i\)</span> 是特征向量<span class="math inline">\(\mathbf{f}\)</span> 在位置 i 处的向量</li><li><span class="math inline">\(\mathbf{f}\)</span> 大小为 <span class="math inline">\(2H \times 2W \times 8\)</span>，由初始估计的绿色通道 <span class="math inline">\(\hat I_G\)</span>经过两个卷积层得到</li><li><span class="math inline">\(G( \cdot, \cdot)\)</span>是一个依赖于两个位置之间距离的高斯函数，是空间自适应卷积的核心，定义为</li><li><span class="math inline">\(G\left(\mathbf{f}_{i},\mathbf{f}_{j}\right)=\exp\left(-\frac{1}{2}\left(\mathbf{f}_{i}-\mathbf{f}_{j}\right)^{\top}\left(\mathbf{f}_{i}-\mathbf{f}_{j}\right)\right)\)</span></li></ul></li></ul><h4 id="training-losses">Training losses</h4><ul><li>初始的绿通道估计和最终的RGB图像重建都需要一个损失，因此引入两个损失来监督模型的训练，以获得更好的去噪性能</li><li><img src="/2021/04/06/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-0x13/7.png"></li><li>Adaptive-threshold edge loss——自适应阈值边缘损失算法<ul><li>尽管如上所述，通过额外的密度图 <span class="math inline">\(M_D\)</span>，网络被告知每像素的难度级别，但输入图像中的每个像素都以相同的强度进行监督。然而，在训练过程中，具有许多高频率细节的区域比其他区域的恢复更重要，更值得关注。为了解决这一问题，作者提出了一种自适应阈值边缘损失算法</li><li>作者将Canny边缘检测器应用于网络输出 <span class="math inline">\(I^O\)</span> 和目标RGB图像 <span class="math inline">\(I^T\)</span>来进行边缘提取，从而获得两个二值边缘图 <span class="math inline">\(E(I^O), E(I^T)\)</span></li><li>不过采用个固定的低阈值的边缘检测器仍然不能满足于不同区域</li><li>所以作者这里就是把一幅图像分成了好几个patch <span class="math inline">\(P_i, i = 1,2\dots, n\)</span>，然后为每一个patch找一个自适应的阈值<ul><li><strong>对于边缘较多的斑块，提高低阈值；而对于边缘较少的斑块，降低阈值</strong></li></ul></li><li>低阈值 <span class="math inline">\(\theta, i = 1,2,\dots,n\)</span>计算如下：<ul><li><span class="math inline">\(\theta_i = \theta_0 + k\frac{s_{P_i}}{max(s_{P_1},s_{P_2},\dots,s_{P_n})}, \theta_0, k &gt;0\)</span></li><li><span class="math inline">\(s_{P_i}\)</span> 是 <span class="math inline">\(P_i\)</span> 中所有像素值的和</li></ul></li><li>一旦得到两个二值边缘图 <span class="math inline">\(E(I^O),E(I^T)\)</span>，就可以通过计算检测到的像素作为边缘像素的比例，来近似得到一个patch<span class="math inline">\(p(E(P_i,\theta_i))\)</span> 的边缘概率</li><li>然后根据 <span class="math inline">\(p(E(P_i,\theta_i))\)</span>来计算交叉熵损失，不过自然图像中，边缘区域仍然是小部分，所以在计算交叉熵损失的时候引入一个平衡权重<span class="math inline">\(\beta\)</span><ul><li><span class="math inline">\(\begin{aligned} L_{e d g e}&amp;=\sum_{i=1}^{n}-\beta * p\left(E\left(P_{i}^{T} ;\theta_{i}\right)\right) * \log \left(p\left(E\left(P_{i}^{O} ;\theta_{i}\right)\right)\right) \\ &amp;-(1-\beta)*\left(1-p\left(E\left(P_{i}^{T} ; \theta_{i}\right)\right)\right) * \\&amp; \log \left(1-p\left(E\left(P_{i}^{O} ;\theta_{i}\right)\right)\right) \end{aligned}\)</span></li><li>其中 <span class="math inline">\(\beta\)</span> 定义为</li><li><span class="math inline">\(\beta = | E(I^T) |/| I^T |\)</span></li><li><span class="math inline">\(|E(I^T)|\)</span>为边缘像素数，在计算出的边缘损失边缘的基础上，对具有强边缘的区域进行了严格的监督，使得小的错误导致大的惩罚</li></ul></li><li>整个过程如最上面的图一样，要比这些文字公式说明清楚明了的多</li></ul></li><li>Edge-aware smoothness loss——平滑损失<ul><li>为了获得良好的去噪性能，采用全变分(TV)正则化方法平滑噪声和非预期伪影</li><li>然而，TVloss在充满文本模式的区域就会失效。因此作者提出一种边缘感知平滑损失 <span class="math inline">\(L_{smooth}\)</span> ，其公式为:<ul><li><span class="math inline">\(L_{\text {smooth }}=\|\nabla O \circ\exp (-\lambda \nabla O)\|\)</span></li><li><span class="math inline">\(\|\nabla O\| = \|\nabla O_h \| +\|\nabla O_v \|\)</span> ，是水平方向和垂直方向的梯度之和， <span class="math inline">\(\lambda\)</span> 是平衡边缘感知强度的参数</li></ul></li><li>即这个损失是TV loss乘以一个指数平滑项<ul><li>在平滑区域，相对于纹理和边缘丰富的区域，它要大一些</li><li>在这种边缘感知平滑损失的情况下，模型学习在平滑区域去除噪声的同时能够保持纹理区域的边缘</li></ul></li></ul></li><li>总损失：<ul><li><span class="math inline">\(L = L_{edge} + \lambda_1 L_{smooth} +\lambda_2 L_{l_1} + \lambda_3 L_g\)</span></li><li>其中 <span class="math inline">\(L_{l_1}\)</span>是输出图像和真实图像之间的 <span class="math inline">\(l_1\)</span>损失，<span class="math inline">\(L_g\)</span> 是 <span class="math inline">\(\hat I_G\)</span> 和真实绿色通道图像之间的 <span class="math inline">\(l_1\)</span> 损失</li></ul></li></ul><h3 id="experiment-1">Experiment</h3><p><img src="/2021/04/06/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-0x13/8.png"></p><h3 id="section-1">😝😜😋</h3><p>感觉整篇论文还是有不少创新点的，其中针对不同频率区域，使用不同的参数进行处理是很符合现实需要的，我想着之后自己坐高光谱图像去噪啥的话，也尽量把这方面考虑进去。不过作者提到的这些想法的实现其实都还是蛮简单的，也能很好的理解，那几张图要比文字公式说明清晰多了。</p><p>其他方面的话，没有感觉特别的，但作者对网络的各部分都有针对地进行了改进，还是很有想法的，值得学习！噶油~</p>]]></content>
    
    
    <categories>
      
      <category>study</category>
      
    </categories>
    
    
    <tags>
      
      <tag>图像增强与图像恢复</tag>
      
      <tag>高光谱图像去噪</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>论文阅读-0x12</title>
    <link href="/2021/03/08/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-0x12/"/>
    <url>/2021/03/08/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-0x12/</url>
    
    <content type="html"><![CDATA[<p>论文的阅读笔记：</p><p>《 SCOUTER: Slot Attention-based Classifier for Explainable ImageRecognition 》, ARXIV</p><p>《 Coordinate Attention for Efficient Mobile Network Design 》， CVPR2021</p><span id="more"></span><h2 id="scouter-slot-attention-based-classifier-for-explainable-image-recognition">SCOUTER:Slot Attention-based Classifier for Explainable Image Recognition</h2><p>SCOUTER:用于解释图像识别的基于时隙注意的分类器</p><h3 id="abstract">Abstract</h3><ul><li>对于可解释的AI，现有的方法大多基于梯度或中间特征，并没有直接参与分类器的决策过程</li><li>在本文作者提出了一种基于时隙注意力的分类器，称为SCOUTER，用于透明而准确的分类</li><li>与其他基于注意力的方法主要有两个区别点<ul><li>SCOUTER的解释涉及每个类别的最终置信度，提供了更直观的解释</li><li>所有类别都有其相应的肯定或否定解释，说明“为什么图像属于某个类别”或“为什么图像不属于某个类别”</li></ul></li><li>同时作者设计了一种新的损失，它可以控制模型的行为以在肯定解释和否定解释之间切换，以及解释区域的大小</li><li>了解深度学习模型如何进行预测具有重要意义，特别是对于医学诊断等领域，采用黑盒模型存在潜在风险。因此，可解释人工智能(explainableartificial intelligence，XAI)作为一种能够深入研究模型推理过程的技术，受到了广泛的关注</li><li>XAI中最流行的范式是属性解释（attributiveexplanation），它涉及像素或区域对最终预测的贡献水平<ul><li>因此，可以用它来回答“为什么图像x属于类别t”的问题，解释通常是通过<strong>显示热图来突出显示支持预测的区域</strong>，这种可视化对理解模型的行为有很大帮助</li><li>这里自然会出现一个问题:这些区域是如何影响决策的。也就是说，属性解释强调了一些<strong>视觉成分</strong>，这些视觉成分可以为决策提供积极的支持;同样的，他们也可以提供否定解释</li></ul></li><li>作者针对XAI，提出了SCOUTER（Slot-based COnfigUrable and TransparentclassifiER）<ul><li>针对每一个类别 <span class="math inline">\(I\)</span>，都存在一个支持集 <span class="math inline">\(\mathcal{S}_{l}=\left\{s_{l 1}, s_{l 2},\ldots\right\}\)</span> （其中的元素支持输入图像对于类别 <span class="math inline">\(I\)</span> 的判定），而SCOUTER的目标是找到一个<span class="math inline">\(\mathcal{S}_{l}\)</span> 的子集 <span class="math inline">\(\mathcal{S}_{l} &#39;\)</span>，其中包含一个或多个支持元素。</li><li><strong>SCOUTER的决策仅仅基于在输入图像中发现</strong>的 <span class="math inline">\(\mathcal{S}=\left\{\mathcal{S}_{l}&#39; | l = 1,2,\ldots\right\}\)</span>（每一类的支持子集），而不是使用难以解释的黑盒分类器</li><li>这种透明性使SCOUTER配置可以找到正面(S+)或负面(S−)支持，同时可视化正面和负面解释</li></ul></li><li>在这种可解释分类器的新范式下，<strong>更小的支持区域可能会更好地促进每种支持的语义解释</strong><ul><li>也就是说，找到眼睛、鼻子、嘴唇等的组合可能比直接找到一张脸更能解释问题</li><li>为此作者在损失函数中引入了一个新的术语来约束支持区域的大小</li><li>不过这种对支持区域的约束本身可能会降低分类性能，但多个支撑的组合可以弥补丢失的线索</li></ul></li><li>SCOUTER是建立在最近出现的slotattention之上的，它<strong>为图像表示提供了一种以对象为中心的方法</strong>。基于这种方法，作者提出了一个可解释的缝隙注意(xSlot)模块</li><li><strong>xSlot模块的输出直接用作对应于S的每个类别的置信度值</strong>，因此不再需要常用的全连接(FC)层分类器</li><li><img src="/2021/03/08/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-0x12/1.png"></li><li>作者工作的主要贡献：<ul><li>提出了一个透明的分类器，提供精确和有意义的解释，直接参与决策过程</li><li>设计了一个loss来调整不同任务/数据集的探索区域，可以更好地满足各种应用的需求</li><li>引入了积极解释和消极解释的新概念，后者是一种新型解释，当机器的决策与用户的期望相悖时，这种解释非常有帮助</li></ul></li><li>在计算机视觉任务中，深度神经网络普遍是以下结构：骨干网络backbonenetwork B + 下游网络downstream network<ul><li>不过在网络的末尾研究者一般会使用一个简单的分类器来完成图像分类任务，包括一个或两个FC层和softmax。因为虽然FC分类器具有黑盒性质，但它是最普遍和最具表现力的选择</li><li>而作者在本文探讨了<strong>使用可解释分类器的可能性，以使决策过程透明，同时保持分类性能</strong></li></ul></li><li>XAI方法主要有三种，即可视化法、蒸馏法和本征法（visualization,distillation, intrinsic methods）<ul><li>其中可视化方法就是提供一个热图，用这种形式来表示说明输入或中间特征的重要性</li><li>属性解释：为每个类产生视觉解释(即热图)</li><li>反事实解释：给出了“如何改变图像 <span class="math inline">\(x\)</span> (属于 <span class="math inline">\(t_a\)</span> )，使其看起来像 <span class="math inline">\(t_b\)</span> 中的图像”的解释</li><li>作者提出的判别解释：回答“为什么图像 <span class="math inline">\(x\)</span> 属于 <span class="math inline">\(t_a\)</span> 而不是 <span class="math inline">\(t_b\)</span> 的问题<ul><li>和作者提出来的否定解释类似，不过区别性解释实际上是试图识别区分 <span class="math inline">\(t_a\)</span> 与单一类别 <span class="math inline">\(t_b\)</span>的事实，而否定性解释则是旨在识别否定目标类别 <span class="math inline">\(t_a\)</span> 本身的事实</li></ul></li><li>蒸馏方法：基本思想是使用固有的透明模型来<strong>模拟训练过的黑盒深度神经网络的输出和行为</strong>。内在方法将解释作为其模型的一部分，因此，这种XAI方法可能更可取，因为它们不需要在做出决策后生成解释</li></ul></li><li>自注意力机制——Self-attention<ul><li>这个最早出现在Transformers中，其中自注意力层逐个扫描输入元素，并使用对整个输入的权重聚合来更新他们</li><li>Slotattention也是基于这种机制从图像中提取以对象为中心的特征。然而，原始的Slotattention仅在一些合成的图像数据集上进行测试</li><li>而这里的SCOUTER是基于Slotattention，设计为一个可解释的分类器适用于自然图像</li></ul></li></ul><h3 id="scouter">SCOUTER</h3><ul><li>给定图像x，分类模型的目标是在类别集合 <span class="math inline">\(\Omega = \{ \omega_1, \omega_1, \ldots , \omega_n,\}\)</span> 中找到其最可能的类别 <span class="math inline">\(l\)</span></li><li>对于神经网络，可以首先通过一个backbone networkB来提取特征，然后将特征表示映射到一个置信度的得分向量，使用FC层和softmax作为分类器</li><li>作者在论文中将这个的分类器替换为SCOUTER，包含xSlot注意力模块<ul><li>xSlot注意力模块对于给出的特征表示，给出对于每一个类别的置信度</li></ul></li><li>包括骨干网在内的整个网络用SCOUTERloss进行训练，控制解释区域的大小，并在积极和消极的解释之间转换</li></ul><h4 id="xslot-attention">xSlot Attention</h4><ul><li>在原始的slot注意力机制中，slot是基于对特征图的关注而聚集的局部区域的表示<ul><li>骨干网B上附加一个Lslot的单个时隙注意力模块，产生L个不同的特征表示作为输出</li><li>当存在多个感兴趣的对象时，此配置非常方便。这个想法可以很容易地传递出来，即在输入图像中发现支撑物S，从而做出确定的决定</li></ul></li><li><strong>xSlot注意模块的每个slot都与一个类别相关联</strong>，并让用户相信输入图像属于这个类别</li><li>在slot注意机制下，类别 <span class="math inline">\(l\)</span>的slot需要在图像上找到相应的支持 <span class="math inline">\(\mathcal{S}_{l}\)</span></li><li>对于特征图 <span class="math inline">\(F\)</span> ，xSlot注意力模块更新时隙slot <span class="math inline">\(w_l^{(t)}\)</span>，其中 <span class="math inline">\(w_l^{(t)}\)</span> 表示 <span class="math inline">\(t\)</span> 更新后的slot， <span class="math inline">\(l \in \Omega\)</span>是与该slot的类别，<strong>slot被初始化为随机权值</strong>，即<ul><li><span class="math inline">\(w_{l}^{(0)} \sim \mathcal{N}(\mu,\operatorname{diag}(\sigma)) \in \mathbb{R}^{n \timesc^{\prime}}\)</span></li><li>其中 <span class="math inline">\(\mu,\sigma\)</span>是高斯分布的均值和方差，<span class="math inline">\(c&#39;\)</span>是权重向量的大小，用 <span class="math inline">\(W^{(t)} \in\mathbb{R}^{1 \times c^{\prime}}\)</span> 表示所有类别的slot</li></ul></li><li>slot <span class="math inline">\(W^{(t+1)}\)</span> 由 <span class="math inline">\(W^{(t)}\)</span> 和特征图 <span class="math inline">\(F\)</span> 进行更新得到</li><li>首先，特征图 <span class="math inline">\(F\)</span>通过一个1x1卷积来减少通道数，使用ReLU进行非线性变换<ul><li><span class="math inline">\(F&#39; = ReLU(Conv(F)) \in \mathbb{R}^{c^{\prime} \times d}\)</span> ，即特征图 <span class="math inline">\(F\)</span> 的空间维数被平展开来（d=hw）</li></ul></li><li>然后 <span class="math inline">\(F&#39;\)</span>通过位置嵌入（position embedding）来加入空间位置信息<ul><li><span class="math inline">\(\widetilde F = F&#39; + PE\)</span>，PE就是位置嵌入</li></ul></li><li>之后使用两个多层感知器（MLPs）Q和K，每一个都有三个FC层和他们之间的ReLU非线性激活<ul><li><span class="math inline">\(Q(W^{(t)}) \in \mathbb{R}^{n \timesc^{\prime}}, K( \widetilde F) \in \mathbb{R}^{c^{\prime} \timesd}\)</span></li></ul></li><li>通过sigmoid来得到点积注意力 <span class="math inline">\(A^{(t)}\)</span><ul><li><span class="math inline">\(A^{(t)} = \sigma (Q(W^{(t)})K(\widetildeF)) \in (0,1)^{n \times d}\)</span></li></ul></li><li>然后将注意力权重加到空间维度上来计算特征的权重总和<ul><li><span class="math inline">\(U^{(t)} = A^{(t)}F&#39; \in\mathbb{R}^{n \times c^{\prime} }\)</span></li></ul></li><li>slot <span class="math inline">\(W^{(t+1)}\)</span><strong>通过一个门控单元（GRU）来进行更新</strong><ul><li><span class="math inline">\(W^{(t+1)} =GRU(U^{(t)},W^{(t)})\)</span></li><li>这里作者将 <span class="math inline">\(U^{(t)},W^{(t)}\)</span>分别作为输入状态和隐藏状态，在原有slot 注意力的基础上，作者对slot进行了<span class="math inline">\(T=3\)</span> 次的更新</li></ul></li><li>xSlot注意力模块的最终输出是 <span class="math inline">\(U^{(t)}\)</span> 中每一类 <span class="math inline">\(l\)</span> 中所有元素的和，是F的函数<ul><li><span class="math inline">\(xSlot(F) = U^{(t)}1_{c&#39;} \in\mathbb{R}^{n}_{+}\)</span></li><li>其中1是所有 <span class="math inline">\(c&#39;\)</span>元素为1的列向量</li><li>以上给出的置信度通常由FC层计算，而SCOUTER只是对xSlot注意模块的输出进行了汇总</li></ul></li><li>另外在原始的slot注意力模块中，对特征进行线性变换，即 <span class="math inline">\(V(\widetilde F)\)</span> ，然后使用 <span class="math inline">\(A^{(t)}\)</span> 对其加权<ul><li>不过作者在xSlot中省略了这种转换，认为模块在Q，K，GRU中已经有了足够学习参数</li></ul></li><li><img src="/2021/03/08/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-0x12/2.png"></li></ul><h4 id="损失函数">损失函数</h4><ul><li><span class="math inline">\(L_{SCOUTER} = L_{CE} + \lambdaL_{Area}\)</span></li><li>交叉熵损失和一个区域面积损失</li><li><span class="math inline">\(L_{\text {Area }}=\mathbf{1}_{n}^{\top}A^{(T)} \mathbf{1}_{d}\)</span><ul><li>即 <span class="math inline">\(A^{(T)}\)</span> 中所有元素的总和，当<span class="math inline">\(\lambda\)</span>更大时，SCOUTER通过选择更少更小的支持来获取更小的支持区域</li></ul></li></ul><h4 id="switching-positive-and-negative-explanation">Switching Positiveand Negative Explanation</h4><ul><li>上面的损失函数，在网络训练中只能提供正向解释，因为 <span class="math inline">\(A^{(T)}\)</span>中的较大元素表示预测是基于相应特征进行的</li><li>为此作者引入超参数 <span class="math inline">\(e \in \{ +1,-1\}\)</span></li><li><span class="math inline">\(o=\mathrm{xSlot}_{e}(F)=e \cdot U^{(T)}\mathbf{1}_{c^{\prime}} \quad \in \mathbb{R}_{+}^{n}\)</span><ul><li>从而来实现正负支持</li></ul></li><li>后面就是对这个的一些解释，基本就是在e不同的取值下，会得到不同的区域解释，用来支持分类结果</li></ul><h3 id="experients">Experients</h3><ul><li>实验部分没啥，有几个点稍微记录一下</li><li>随着分类类别（&gt;100）的增多，网络的训练效果会变得不稳定<ul><li>“这可能是因为越来越难以找到一贯出现在同一类别的所有图像中但其他类别却不共用的有效支助。这个缺点限制了SCOUTER在小型或中型数据集上的应用”</li></ul></li><li><img src="/2021/03/08/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-0x12/3.png"></li></ul><h3 id="section">😝😜😋</h3><p>这篇论文其实还是蛮有意思的，对于深度学习中的黑盒行为，进行了可解释化，不过网络结构刚开始没太理解，后来发现就是self-attention，，，在此基础上，作者进行了改动，然后将结果进行可视化，这里的可视化就是用一种热图的形式来对分类结果进行支持</p><p>感觉这种方法，在这种图像分类中其实还是蛮有用的，不过如果想用在图像修复说的话，感觉不太好迁移过去，没有啥实际的作用吧，不过也是一个不断学习的过程</p><h2 id="coordinate-attention-for-efficient-mobile-network-design">CoordinateAttention for Efficient Mobile Network Design</h2><h3 id="abstract-1">Abstract</h3><ul><li>通道注意力机制对于提升模型性能效果显著（如SEattention），但通常忽略了位置信息，而位置信息对于生成空间选择性注意力图非常重要</li><li>在本文中，作者提出了一种新的移动网络注意力机制，将位置信息嵌入到通道注意中，称之为CoordinateAttention</li><li>与通过二维全局池化将特征张量转换为单一特征向量的通道注意不同，CoordinateAttention将通道注意力分解为两个一维特征编码过程，分别沿两个空间方向聚合特征<ul><li>通过这种方式，可以在一个空间方向上捕获长期的依赖关系，同时可以在另一个空间方向上保留精确的位置信息</li><li>然后将得到的特征图featuremap分别编码成一对方向感知和位置敏感的注意力图attentionmap，可以将其互补地应用于输入特征图，以增强关注对象的表示</li></ul></li><li>CoordinateAttention是简单的，可以灵活地插入经典的移动网络，如MobileNetV2、MobileNeXt和EfficientNet，几乎没有计算开销</li><li>这里作者<strong>主要是针对于移动网络，所以需要考虑移动网络有限的计算能力</strong>，而目前最流行的移动网络注意力机制仍然是SEattention，不过却忽略了位置信息的重要性</li><li>后来的工作尝试通过降低输入张量的通道维数，然后使用卷积计算空间注意力，从而利用位置信息。不过卷积只能捕获局部关系，却无法建模远景任务所必需的长期依赖关系</li><li>在本文中，作者通过将位置信息嵌入到通道注意中提出了一种新的有效的注意机制，使移动网络能够在较大的区域内进行注意力计算，同时避免产生大量的计算开销<ul><li>为了缓解二维全局池化造成的位置信息丢失，作者<strong>将通道注意力分解为两个并行的一维特征编码过程</strong>，有效地将空间坐标信息整合到生成的注意图中</li><li><strong>利用两个一维全局池化操作分别将垂直和水平方向的输入特征聚合为两个独立的方向感知特征映射</strong></li><li>然后，这两个嵌入方向特定信息的特征图被分别编码成两个注意力图，每一个都捕捉输入特征地图沿一个空间方向的远距离依赖关系</li><li>这样，位置信息就可以保存在生成的注意力图中。<strong>然后这两种注意映射通过乘法应用到输入特征映射上，以强调注意力区域</strong></li><li>作者将所提出的注意方法命名为CoordinateAttention，因为它能够区分空间方向(即坐标)并生成坐标感知的注意力图</li></ul></li><li>作者提出的Coordinate Attention的优势：<ul><li>不仅能捕获跨通道的信息，还能捕获方向和位置敏感的信息，有助于模型更准确地定位和识别支持区域</li><li>灵活和轻量级，可以很容易地插入移动网络的经典模块中</li><li>作为一个预训练模型，可以为移动网路的下游任务带来显著性能提升，特别是对于那些具有密集预测(例如，语义分割)的任务</li></ul></li><li>目前最先进的移动网络大多基于深度可分离卷积和反向残差模块（depthwiseseparable convolutions and the inverted residual block ）</li><li>对于Non-local/self-attention，其计算量过大，并不适应于移动网络<ul><li>所以这里作者将二维全局池化分解两个一维编码过程，从而来减少计算量</li></ul></li></ul><h3 id="coordinate-attention">Coordinate Attention</h3><p><img src="/2021/03/08/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-0x12/4.png"></p><ul><li>对于CoordinateAttention模块，其输入和输出（增强表示后）尺寸是相同的</li><li>标准卷积本身是很难对信道之间的关系进行建模，显式地构建通道间的依赖关系可以增加模型对信息通道的敏感性，这些信息通道对最终分类决策的贡献更大。此外，使用全局平均池还可以帮助模型捕获全局信息</li><li>SE block<ul><li>分为压缩和激励两部分（squeeze andexcitation），分别用于全局信息嵌入和信道关系的自适应重新校准（就是加权嘛）</li><li><span class="math inline">\(z_{c}=\frac{1}{H \times W}\sum_{i=1}^{H} \sum_{j=1}^{W} x_{c}(i, j)\)</span><ul><li>输入 <span class="math inline">\(\boldsymbol{X} \in \mathbb{R}^{C\times H \times W}\)</span></li></ul></li><li><span class="math inline">\(\hat{\mathbf{X}}=\mathbf{X} \cdot\sigma(\hat{\mathbf{z}})\)</span><ul><li>通道乘法，进行加权，其中 <span class="math inline">\(\hat{\mathbf{z}}\)</span> 为变换函数</li><li><span class="math inline">\(\hat{\mathbf{z}} =T_2(ReLU(T_1(\mathbf{z})))\)</span></li><li><span class="math inline">\(T_1,T_2\)</span>为两个线性变换，用来学习捕捉每个通道的权重</li></ul></li></ul></li><li>CoordinateAttention通过精确的位置信息对通道关系和远程依赖性进行编码，分为coordinateinformation embedding and coordinate attentiongeneration生成两个步骤</li><li>coordinate information embedding<ul><li>作者对SE的权重计算公式进行分解，转化为一对一维特征编码操作</li><li>具体来说，给定输入X，使用空间大小为(H,1)和(1,W)的poolingkernel<strong>分别沿着水平坐标和垂直坐标对每个通道进行编码</strong></li><li><span class="math inline">\(z_{c}^{h}(h)=\frac{1}{W}\sum_{0&lt;=i&lt;W} x_{c}(h, i)\)</span></li><li><span class="math inline">\(z_{c}^{w}(w)=\frac{1}{H}\sum_{0&lt;=j&lt;H} x_{c}(j, w)\)</span></li><li>这两种转换也允许注意力块捕捉沿着一个空间方向的长期依赖关系，并保存沿着另一个空间方向的精确位置信息，这有助于网络更准确地定位感兴趣的对象</li></ul></li><li>coordinate attention generation<ul><li>首先，对于移动环境中的应用来说，新的转换应该尽可能地简单和廉价</li><li>其次，它可以充分利用捕获到的位置信息，使感兴趣的区域能够准确地突出显示</li><li>最后但最重要的是，它还应该能够有效地捕捉通道间的关系</li><li>对于上面得到的聚合特征映射，使用一个1x1卷积连接起来，然后使用非线性激活</li><li><span class="math inline">\(\mathbf{f}=\delta\left(F_{1}\left(\left[\mathbf{z}^{h},\mathbf{z}^{w}\right]\right)\right)\)</span><ul><li><span class="math inline">\(\mathbf{f} \in \mathbb{R}^{C/r \times (H+ W)}\)</span>是对水平方向和垂直方向的空间信息进行编码的中间特征映射</li><li>r控制SE块大小的缩减率</li></ul></li><li>然后沿着空间维度，将 <span class="math inline">\(\mathbf{f}\)</span>分解为 <span class="math inline">\(\mathbf{f}^{h} \in \mathbb{R}^{C/r\times H}\)</span> 和<span class="math inline">\(\mathbf{f}^w \in\mathbb{R}^{C/r \times W}\)</span></li><li>接着利用两个1 × 1卷积将 <span class="math inline">\(\mathbf{f}^h\)</span> 和 <span class="math inline">\(\mathbf{f}^w\)</span> 变换为和输入 <span class="math inline">\(X\)</span> 相同通道数的张量<ul><li><span class="math inline">\(\mathbf{g}^h=\sigma\left(F_{h}\left(\mathbf{f}^h\right)\right)\)</span></li><li><span class="math inline">\(\mathbf{g}^w=\sigma\left(F_{w}\left(\mathbf{f}^w\right)\right)\)</span></li></ul></li><li>综上</li><li><span class="math inline">\(y_c(i,j) = x_c(i,j) \times g_c^h(i)\times g_c^w(j)\)</span></li></ul></li><li><img src="/2021/03/08/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-0x12/5.png"></li></ul><h3 id="section-1">😝😜😋</h3><p>emm这篇论文有点简单，，，没啥好说的，可能也是针对移动端，所以设计的简单很多</p>]]></content>
    
    
    <categories>
      
      <category>study</category>
      
    </categories>
    
    
    <tags>
      
      <tag>图像增强与图像恢复</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>论文阅读-0x11</title>
    <link href="/2021/01/28/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-0x11/"/>
    <url>/2021/01/28/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-0x11/</url>
    
    <content type="html"><![CDATA[<p>左旺孟老师视频里面涉及的最后一篇论文</p><p>论文的阅读笔记：</p><p>《 Unpaired Learning of Deep Image Denoising 》 左旺孟，ECCV 2020</p><span id="more"></span><h2 id="unpaired-learning-of-deep-image-denoising">Unpaired Learning ofDeep Image Denoising</h2><h3 id="abstract">Abstract</h3><ul><li>作者研究了从一组未配对的干净和有噪声图像中学习图像盲去噪网络的任务</li><li>作者进一步假设噪声可以是依赖于信号的，但空间上是不相关的</li><li>提出了一种结合自监督学习和知识精馏的两阶段学习方法<ul><li>对于自我监督学习，使用扩张盲点网络(D-BSN)来学习真实噪声图像去噪。由于噪声的空间独立性，采用堆叠1×1卷积层的网络来估计每幅图像的噪声水平图。D-BSN和图像特定噪声模型(CNNest)都可以通过最大化约束的数似然率进行联合训练。给定D-BSN的输出和估计的噪声水平图，基于贝叶斯规则可以进一步获得更好的去噪性能</li><li>至于知识蒸馏，首先将学习到的噪声模型应用于清洁图像，以合成一对训练图像，然后在第一阶段使用真实的噪声图像和相应的去噪结果来形成另一组配对。然后可以通过使用这两个配对集训练现有的去噪网络来蒸馏得到最终降噪模型</li></ul></li><li>目前深度卷积神经网络在图像去噪领域进步很大，先是针对高斯白噪声（WGN），然后使用CNN针对真实噪声去噪，虽然已经取得了性能的提升，但现有的大多数CNN去噪器的成功很大程度上依赖于有监督学习和大量成对的噪声干净图像（获取？）<ul><li>一方面，给定噪声模型的形式和参数，可以从无噪声的干净图像中合成出噪声图像，以构成成对的训练集。但是，实际的噪声通常很复杂，并且现实摄影中的相机内信号处理（ISP）流水线进一步增加了噪声的复杂性，所以很难用基本的参数完全表征噪声模型</li><li>另一方面，可以通过设计合适的方法来构建配对集，以获取对应于给定真实噪声图像的几乎无噪声（或清晰）的图像。对于现实世界的摄影，可以通过平均多个噪声图像或校准和后处理低ISO图像来获取几乎无噪点的图像。不过这种几乎无噪声的图像可能会存在过度平滑的问题，而且获取成本昂贵。此外，这种几乎无噪声的图像采集可能不适用于其他成像机制(如显微镜或医学成像)</li></ul></li><li><img src="/2021/01/28/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-0x11/1.png"></li><li>Noise2Noise(N2N)模型来从成对的噪声实例中学习映射，而不是使用成对训练集的监督学习<ul><li>不过它要求每对图像中底层的干净图像是完全相同的，噪声是独立地从相同的分布中提取的，这限制了它的实用性</li></ul></li><li>Noise2Void(N2V)模型，该模型采用<strong>盲点网络(BSN)</strong>，仅从有噪声的图像中学习CNN去噪器<ul><li>不过BSN在训练时的计算效率非常低，并且没有利用盲点处的像素值，导致去噪性能下降</li></ul></li><li>提出了一个两阶段的方案，即自监督学习和知识蒸馏，从一组不成对的干净的和有噪声图像来学习图像盲去噪网络（不是基于GAN的非成对学习）<ul><li>首先只利用噪声图像学习BSN和特定图像的噪声水平估计网络cnnest，进行图像去噪和噪声建模</li><li>然后将学习到的噪声模型应用于干净图像来合成成对的训练图像集</li><li>同时在第一阶段也将真实噪声图像和对应的去噪结果形成另一组成对图像</li><li>至于知识蒸馏，作者仅使用上述两个配对集就可以训练出最先进的CNN去噪器，例如MWCNN</li></ul></li><li>假设干净的图像是空间相关的，使得利用BSN体系结构仅从有噪图像学习盲去噪网络是可行的。为了提高训练效率，提出了一种利用扩张卷积和全卷积网络(FCN)的新型扩张BSN(即D-BSN)，在训练过程中通过一次前向传递来预测所有像素的去噪结果</li><li>进一步假设噪声是像素无关的，但也可以是信号相关的。因此，一个像素的噪声级别可以是一个常量，也可以只取决于单个像素值。考虑到不同图像的噪声模型和参数可能有所不同，<span class="math inline">\(CNN_{est}\)</span>通过堆叠1×1卷积层来满足上述要求。利用无组织的噪声图像集合，D-BSN和 <span class="math inline">\(CNN_{est}\)</span>都可以通过最大化约束的对数似然进行联合训练。给定D-BSN和CNNest的输出，使用贝叶斯规则来获得第一阶段的去噪结果。对于第二阶段给定的干净图像，随机选取特定图像的<span class="math inline">\(CNN_{est}\)</span> 合成有噪声的图像</li><li>论文贡献：<ul><li>提出了一种结合自监督学习和知识蒸馏的两阶段学习方法，从无配对的干净和噪声图像中学习盲去噪网络。特别是采用自监督学习进行图像去噪和噪声建模，从而形成两个互补的成对集合，提取出最终的去噪网络</li><li>为了提高训练效率和满足假设的噪声特性，提出了一种新的扩张型盲点网络(D-BSN)和一种基于图像的噪声水平估计网络(<span class="math inline">\(CNN_{est}\)</span>)。利用无组织的噪声图像集合，D-BSN和cnnest可以通过最大化约束的对数似然来联合训练</li><li>在不同类型的合成噪声上的实验表明，该非成对学习方法优于N2V和Laine19，与完全监督学习方法不相上下。MWCNN(unpaired)也在真实照片上表现良好，在DND数据集上显著超过基于GAN的unpairedlearning (GCBD)</li></ul></li></ul><h3 id="proposed-method">Proposed Method</h3><ul><li>给定噪声图像 <span class="math inline">\(\mathbf{y}\)</span>，干净图像 <span class="math inline">\(\mathbf{x}\)</span> ，用 <span class="math inline">\(\widetilde {\mathbf{x}}\)</span> 表示 <span class="math inline">\(\mathbf{y}\)</span> 的干净图像<ul><li><span class="math inline">\(\mathbf{y} = \widetilde {\mathbf{x}} +\mathbf{n}\)</span></li><li>其中 <span class="math inline">\(\mathbf{n}\)</span>表示噪声信息</li><li>假设图像 <span class="math inline">\(\mathbf{x}\)</span>是空间相关的，噪声 <span class="math inline">\(\mathbf{n}\)</span>是像素无关、信号依赖的高斯分布，即像素 <span class="math inline">\(i\)</span>的噪声方差（噪声水平）仅由底层无噪声图像 <span class="math inline">\(\widetilde {\mathbf{x}}\)</span> 在像素 <span class="math inline">\(i\)</span> 处的值决定</li><li><span class="math inline">\(var(n_i) = g_{\widetilde {\mathbf{x}}}({\mathbf{\widetilde x}_i})\)</span><ul><li><span class="math inline">\(g_{\widetilde {\mathbf{x}}}({\mathbf{\widetilde x}_i})\)</span>可以看作是多元异方差高斯模型中的一种噪声水平函数(noise level function,NLF)。与线性NLF不同， <span class="math inline">\(g_{\widetilde{\mathbf{x}}}( {\mathbf{\widetilde x}_i})\)</span>可以是任何非线性函数，因此在噪声建模中更具表现力</li></ul></li></ul></li><li><img src="/2021/01/28/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-0x11/2.png"><ul><li>在第一阶段，构建了一个新的盲点网络D-BSN，以及一个图像特定的噪声模型CNNest<ul><li>引入自监督损耗，通过最大化约束的对数似然，在 <span class="math inline">\(\mathbf{y}\)</span> 的基础上联合训练D-BSN和 <span class="math inline">\(CNN_{est}\)</span></li><li>对于给定的真实噪声图像 <span class="math inline">\(\mathbf{y}\)</span> , D-BSN和 <span class="math inline">\(CNN_{est}\)</span> 协作得到第一级去噪结果 <span class="math inline">\(\mathbf{\hat x_{\mathbf{y}}}\)</span> 和估计的NLF<span class="math inline">\(g_{\mathbf{y}}(\mathbf{y})\)</span></li></ul></li><li>在第二阶段，采用知识蒸馏（Knowledge Distillation）策略，利用 <span class="math inline">\(\mathcal{X}, \mathcal{Y},\hat{\mathcal{X}}^{(1)}=\left\{\hat{\mathbf{x}}_{\mathbf{y}} \mid\mathbf{y} \in \mathcal{Y}\right\}\)</span> （噪声图像 <span class="math inline">\(\mathbf{y}\)</span> ，干净图像 <span class="math inline">\(\mathbf{x}\)</span> 和第一级去噪结果 <span class="math inline">\(\mathbf{\hat x_{\mathbf{y}}}\)</span>）和一组特定图像的 <span class="math inline">\(NLFs\{g_{\mathbf{y}}(\mathbf{y})\mid \mathbf{y} \in \mathcal{Y} \}\)</span>（噪声估计），以完监督的方式提取最先进的深度去噪网络<ul><li>对于给定的干净图像 <span class="math inline">\(\mathbf{x}\)</span>，随机选择图像特定的NLF <span class="math inline">\(g_{\mathbf{y}}(\mathbf{y})\)</span> ，利用 <span class="math inline">\(g_{\mathbf{y}}(\mathbf{y})\)</span> 生成 <span class="math inline">\(\mathbf{x}\)</span> 的NLF。用 <span class="math inline">\(\mathbf{n}_{0} \sim \mathcal{N}(0,1)\)</span>表示均值为零、方差为一的随机高斯噪声</li><li>因此可以得到对应的合成噪声图像（其实就是干净图像 <span class="math inline">\(\mathbf{x}\)</span>加上噪声信息，不过这里的噪声信息通过一个 <span class="math inline">\(g_{\mathbf{y}}(\mathbf{y})\)</span>生成的NLF噪声估计水平进行了处理）</li><li><span class="math inline">\(\tilde{\mathbf{y}}=\mathbf{x}+g_{\mathbf{y}}(\mathbf{x})\cdot \mathbf{n}_{0}\)</span></li><li>同时利用噪声图像 <span class="math inline">\(\mathbf{y}\)</span>，和他在第一阶段得到的去噪结果 <span class="math inline">\(\mathbf{\hatx_{\mathbf{y}}}\)</span> 就构成了第二对noisy-clean图像对</li></ul></li><li>然后就是用上面两组noisy-clean图像对来进行训练学习，损失为<ul><li><span class="math inline">\(\mathcal{L}_{\text {distill}}=\sum_{\mathbf{x} \in\mathcal{X}}\|\operatorname{CDN}(\tilde{\mathbf{y}})-\mathbf{x}\|^{2}+\lambda\sum_{\mathbf{y} \in\mathcal{Y}}\left\|\mathrm{CDN}(\mathbf{y})-\hat{\mathbf{x}}_{\mathbf{y}}\right\|^{2}\)</span></li><li>也很好理解这个东西</li></ul></li><li>上面第二阶段的卷积去噪网络CDN可以是任何现有的CNN去噪器，作者在实现中以MWCNN为例</li></ul></li><li>D-BSN<ul><li><img src="/2021/01/28/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-0x11/3.png"></li><li>对于某一位置的输出，BSN的核心是排除当前位置输入值的影响(即盲点要求)</li><li>对于第一层卷积，可以很容易地使用掩蔽卷积来做到，对于噪声图像 <span class="math inline">\(\mathbf{y}\)</span> ，<span class="math inline">\(\mathbf{w}_k\)</span> 表示第k层的3x3卷积，对应mask<span class="math inline">\(\mathbf{m}\)</span>，将其中心位置赋0，其他位置为1</li><li><span class="math inline">\(\mathbf{f}^{(1)}_k = \mathbf{y} *(\mathbf{w}_k \circ \mathbf{m})\)</span><ul><li>其中 <span class="math inline">\(*,\circ\)</span>表示卷积核元素相乘</li><li>不过随着卷积层的堆叠，最后还是会用到这个位置的信息</li></ul></li><li>所以作者在第一层之后就使用空洞卷积（dilatedconvolution），在3x3卷积上使用膨胀因子为2（在5x5卷积上，膨胀因子为3），这样的话，就不会用到当前位置的信息<ul><li>第一层仍然是普通的mask卷积</li><li>后面作者有解释，其实就是卷积之后正好跳过了当前点的信息</li></ul></li><li>同时如果使用1x1卷积和跳跃连接，也不会破坏这个盲点要求，因此可以使用这几个方法来构建FCN，即D-BSN</li><li><img src="/2021/01/28/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-0x11/4.png"></li><li>网络的具体结构如上</li><li>multiple dilated convolution(MDC)多尺度空洞卷积，使用到了残差连接，两个网络分支的结果最终直接连接起来，经过四个1x1卷积来得到最终的特征表达</li></ul></li><li><span class="math inline">\(CNN_{est}\)</span></li><li><img src="/2021/01/28/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-0x11/5.png"></li><li>给定真实噪声图像 <span class="math inline">\(\mathbf{y}\)</span>底层的干净图像 <span class="math inline">\(\widetilde{\mathbf{x}}\)</span> ，假定噪声在条件上独立于像素</li><li>假设噪声是信号依赖的多元高斯信号，并进一步要求NLF <span class="math inline">\(g_{\mathbf{\widetilde x}}(\mathbf{\widetildex})\)</span> 是图像特有的，以提高模型的灵活性</li><li>然后作者这里使用了1x1卷积的FCN框架来学习噪声模型，这样的话也可以保证某一位置的噪声水平只依赖于同一位置的输入值</li><li>不过对于 <span class="math inline">\(g_{\mathbf{\widetildex}}(\mathbf{\widetilde x})\)</span> ，它的输入要求是干净图像，所以 <span class="math inline">\(CNN_{est}\)</span> 以噪声图像作为输入，学习 <span class="math inline">\(g_{\mathbf{y}}(\mathbf{y})\)</span>，来进行近似</li><li>这里作者提出每个图象在 <span class="math inline">\(CNN_{est}\)</span>中都有自己的网络参数来学习特定图像的NLF</li><li>Self-Supervised Loss and Bayes Denoising<ul><li>因为真实噪声图像 <span class="math inline">\(\mathbf{y}\)</span>底层的干净图像 <span class="math inline">\(\widetilde{\mathbf{x}}\)</span> 和真实噪声图像 <span class="math inline">\(\mathbf{y}\)</span>的真实NLF不可用，所以选择自监督学习</li><li>对于给定位置 <span class="math inline">\(i\)</span> ，有 <span class="math inline">\(y_i = \widetilde x_i + n_i\)</span> ，其中 <span class="math inline">\(n_{i} \sim \mathcal{N}\left(\mathbf{0},\mathbf{\Sigma}_{i}^{\mathbf{n}}\right)\)</span><ul><li>其中 <span class="math inline">\(y_i,\widetildex_i,n_i,\mathbf{0}\)</span> 都是C x 1向量</li></ul></li><li><span class="math inline">\(\boldsymbol{\mu}\)</span>是D-BSN直接预测得到的干净图像，假设 <span class="math inline">\(\boldsymbol{\mu} = \widetilde {\mathbf{x}} +\mathbf{n^{\boldsymbol \mu}}\)</span> ，其中 <span class="math inline">\(n_{i}^{\boldsymbol \mu} \sim\mathcal{N}\left(\mathbf{0},\mathbf{\Sigma}_{i}^{\boldsymbol{\mu}}\right)\)</span></li><li>进一步假设 <span class="math inline">\(n_i\)</span> 和 <span class="math inline">\(\mu_{i}\)</span> 是独立的，可以看到 <span class="math inline">\(\boldsymbol{\mu}\)</span> 要比真实噪声图像 <span class="math inline">\(\mathbf{y}\)</span> 更接近 <span class="math inline">\(\widetilde {\mathbf{x}}\)</span> ，<span class="math inline">\(\left|\boldsymbol{\Sigma}_{i}^{\mathbf{n}}\right|\gg\left|\boldsymbol{\Sigma}_{i}^{\boldsymbol{\mu}}\right| \approx0\)</span></li><li>不过 <span class="math inline">\(\widetilde {\mathbf{x}}_i\)</span>不可用，所以引入 <span class="math inline">\(\epsilon_{i}=y_{i}-\mu_{i}\)</span> ，其中 <span class="math inline">\(\epsilon_{i} \sim \mathcal{N}\left(\mathbf{0},\mathbf{\Sigma}_{i}^{\mathbf{n}}+\mathbf{\Sigma}_{i}^{\boldsymbol{\mu}}\right)\)</span></li><li>emmm后面就看不懂了，就是推导出了在贝叶斯框架下的loss</li></ul></li><li>不过将其应用于真实噪声时，由于真实照片中的噪声是空间相关的，违反了像素独立的噪声假设，从而限制了该方法的直接应用。不过作者认为这种假设在分离信号(空间相关)和噪声(像素无关)时是十分重要的</li></ul><h3 id="section">😝😜😋</h3>]]></content>
    
    
    <categories>
      
      <category>study</category>
      
    </categories>
    
    
    <tags>
      
      <tag>自监督学习</tag>
      
      <tag>底层视觉应用</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>论文阅读-0x10</title>
    <link href="/2021/01/25/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-0x10/"/>
    <url>/2021/01/25/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-0x10/</url>
    
    <content type="html"><![CDATA[<p>这两篇是左旺孟老师视频里面关于图像压缩的部分，当时看的时候，也是被搞得云里雾里的，后来看了好多网上的笔记博客才慢慢搞懂，也是打算好好看，学习一下。上一篇里面的论文都是16年左右的，后面几篇就到18年或20年最新的了，感觉很多工作都是对之前工作的发展改进。</p><p>论文的阅读笔记：</p><p>《 Learning Convolutional Networks for Content-weighted ImageCompression 》 左旺孟，CVPR 2018</p><p>《 Efficient and Effective Context-Based ConvolutionalEntropyModeling for Image Compression 》 左旺孟，TPAMI 2020</p><span id="more"></span><h2 id="learning-convolutional-networks-for-content-weighted-image-compression">LearningConvolutional Networks for Content-weighted Image Compression</h2><h3 id="abstract">Abstract</h3><ul><li>有损图像压缩是一个优化问题，其优化目标是失真率，优化对象是编码器、化器和解码器</li><li>作者认为图像的局部信息内容是变化的，因此<strong>根据图像的不同区域来自适应地选择bitrate（码率，K/N），在内容加权importancemap的指导下分配码率</strong>，所以importancemap的总和可以作为离散熵估计的连续选择来控制压缩率</li><li>作者还采用了一个<strong>二元机（binarizer）来实现量化功能</strong>。为了让二元机在BP过程中可微，作者引入了一个代理函数（proxyfunction），在BP中代替二元操作。</li><li>一个图像压缩系统通常由编码器（encoder），量化器（quantizer）和解码器（decoder）三个部分组成编解码器</li><li>作者压缩框架由四个主要组件组成:卷积编码器、重要性映射网络、二进制化器（二元机）和卷积解码器。通过引入连续重要性图和代理功能，所有组件都可以<strong>以端到端的方式进行联合优化</strong></li><li>使用CNN来进行图像压缩<ul><li>对于图像编码和解码，灵活的非线性分析和生成转换可以通过叠加多个卷积层实现</li><li>其次，它允许以端到端的方式联合优化非线性编码器和解码器</li></ul></li><li>仍需解决的问题<ul><li>有损图像压缩可以被表述为一个联合失真率优化来学习编码器、量化器和解码器。编码器和解码器可以表示为cnn并通过反向传播进行优化，但<strong>量化器的学习不可微</strong>仍然是一个具有挑战性的问题</li><li>该系统的目标是<strong>将压缩率和失真率共同降低</strong>，在学习过程中还需要估计熵率并使其最小化。由于量化的结果，定义在离散码上的<strong>熵率</strong>（一个长度为n的随机变量序列，该序列的熵随n增长的增长率）也是一个离散函数，并且需要一个连续的近似</li><li>因此这篇文章主要就是为了解决量化和熵率预测问题</li></ul></li><li>现有的深度学习方法，为每一个位置都分配相同长度的码元。显然，局部信息量（localinformativecontent）是空间位置相关的，因此比特率也应该是空间位置相关的。因此<strong>作者提出了一个基于内容权重的重要性图（content-weightedimportancemap）。其输出一个与输入同尺寸的图。每一个点的值是一个非负数值，指示编码长度</strong>。此时，重要性图各点求和，就可以作为压缩率的连续预测，进而作为压缩率控制器。此时就不再需要预测熵率了</li><li>对于一张给定的图片，编码器输出为E(x)，然后二值化器（二元机）<span class="math inline">\(B(E(x))\)</span>将便把其输出中大于0.5的置为1，小于0.5的置为0。在反向传播时，该二元机被一个代理函数近似，使其可训练</li><li>作者还设计了一个卷积熵编码器，通过上下文预测当前位置编码，并将其应用到上下文自适应二进制算术编码(CABAC)框架中，进一步压缩二进制码元和重要性图</li><li>对于现有的图像标准，例如JPEG和JPEG 2000，编解码器实际上是单独优化的<ul><li>在编码阶段，首先对图像进行线性变换。然后利用量化和无损熵编码来最小化压缩率</li><li>在解码阶段，设计了解码算法和反变换来减小失真。</li></ul></li></ul><h3 id="content-weighted-image-compression">Content-weighted ImageCompression</h3><ul><li><img src="/2021/01/25/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-0x10/1.png"><ul><li>对于一张给定的图片x，卷积编码器输出为E(x)</li><li>二值化器<span class="math inline">\(B(E(x))\)</span>将便把其输出中大于0.5的置为1，小于0.5的置为0</li><li>importance map网络将中间层featuremap作为输入，产生内容加权的importance map P(x)，然后将P(x)量化为mask<span class="math inline">\(M(P(x))\)</span>，它与<span class="math inline">\(B(E(x))\)</span>尺寸相同</li><li>然后根据<span class="math inline">\(M(P(x))\)</span>修剪二进制码元，通过卷积解码器输出重建图片</li></ul></li></ul><h4 id="convolutional-encoder-and-decoder">Convolutional encoder anddecoder</h4><ul><li>框架中的编码器和解码器都是完全卷积的网络，可以通过反向传播进行训练</li><li>解码器D(c)的网络结构与编码器的网络结构是对称的<ul><li>c是图像x的码元</li></ul></li></ul><h4 id="binarizer二元机">Binarizer（二元机）</h4><ul><li>由于最后一个卷积层采用sigmoid非线性，编码器输出e = E(x)应在[0,1]范围内。用<span class="math inline">\(e_{ijk}\)</span>表示e中的一个元素。二进制化定义为<ul><li><span class="math inline">\(B\left(e_{i jk}\right)=\left\{\begin{array}{ll}1, &amp; \text { if } e_{ijk} &gt; 0.5\\ 0, &amp; \text { if } e_{i j k} &lt;=0.5\end{array}\right.\)</span></li></ul></li><li>不过可以看出来这种二值化函数，在神经网络的反向传播中是无法进行参数更新的，梯度处处为零</li><li>为此，作者引入了一个代理函数 <span class="math inline">\(\widetildeB\left(e_{i j k}\right)\)</span> 来近似 <span class="math inline">\(B\left(e_{i j k}\right)\)</span><ul><li>不过在在正向传播中仍然还是用 <span class="math inline">\(B\left(e_{ij k}\right)\)</span> ，在反向传播中使用 <span class="math inline">\(\widetilde B\left(e_{i j k}\right)\)</span></li><li><span class="math inline">\(\widetilde B\left(e_{i jk}\right)=\left\{\begin{array}{ll}1, &amp; \text { if } e_{ijk} &gt; 1\\ e_{i j k}, &amp; \text { if } 1 &lt;= e_{i j k} &lt;= 0 \\ 0, &amp;\text { if } e_{i j k} &lt; 0 \end{array} \right.\)</span></li><li>这样的话他的梯度是很好算的</li><li><span class="math inline">\(\widetilde B&#39;\left(e_{i jk}\right)=\left\{\begin{array}{ll}1, &amp; \text { if } 1 &lt;= e_{ijk}&lt;= 0 \\ 0, &amp; \text { otherwise } \end{array}\right.\)</span></li></ul></li></ul><h4 id="importance-map">Importance map</h4><ul><li>在之前的一些方法中，量化后的码元长度是空间不变的，然后使用熵编码进一步压缩编码</li><li>作者认为图像中的平滑区域应该比具有显著对象或丰富纹理的区域更容易被压缩。因此，分配给平滑区域的比特应该少一些，而分配给信息内容较多的区域的比特应该多一些<ul><li>为此作者提出了importance map用于比特分配以及压缩率控制</li><li><img src="/2021/01/25/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-0x10/2.png"></li></ul></li><li>作者使用内容加权的重要图来进行比特分配和压缩率控制<ul><li>只有一个通道的feature map</li><li>大小应该与编码器输出的大小相同</li><li>其值的范围在（0，1）之间</li></ul></li><li>用h×w表示重要性图p的大小，n表示编码器输出的特征图的数量。为了指导比特分配，首先要<strong>将p中的每个元素量化为一个不大于n的整数，然后生成一个大小为n×h×w的重要掩码m</strong></li><li>对于得到的单通道importance map P(x)，首先把它量化为整数（量化器）：<ul><li><span class="math inline">\(Q\left(p_{ij}\right)=\left\{\begin{array}{ll}l-1, &amp; \text { if } \frac{l-1}{L}\leq p_{i j}&lt;\frac{l}{L}, l=1, \ldots, L \\ L, &amp; \text { if }p_{i j}=1\end{array}\right.\)</span><ul><li><span class="math inline">\(Q\left(p_{ij}\right)\)</span>一共有L个量化等级，从0到L-1，并且 (n mod L) = 0</li><li>每一个量化级别对应 n/L 比特，<span class="math inline">\(p_{ij} \in(0,1)\)</span></li></ul></li><li>对于<span class="math inline">\(Q\left(p_{ij}\right)\)</span>=0的点，不用分配比特权重，在解码阶段它可以通过上下文重建</li></ul></li><li>通过<span class="math inline">\(Q\left(p_{ij}\right)\)</span>，我们可以得到importance mask :<ul><li><span class="math inline">\(\mathbf{m}_{k ij}=\left\{\begin{array}{ll}1, &amp; \text { if } k \leq \frac{n}{L}Q\left(p_{i j}\right) \\ 0, &amp; \text { else}\end{array}\right.\)</span></li><li>kij分别对应nhw</li></ul></li><li>最后需要编码的码元为<span class="math inline">\(c=M(p) \circB(e)\)</span>，其中<span class="math inline">\(\circ\)</span>表示元素乘法<ul><li>假设每一个featuremap的每一个(k,i,j)位置分配1bit，那么原本共需要<span class="math inline">\(n \times h \times w\)</span> bit，现在只需要 <span class="math inline">\(\sum \frac n L Q_{p_{i,j}}\)</span> bit</li></ul></li><li>不过上面的量化和mask操作也是不可微的，无法进行反向传播，所以用以下函数进行等价<ul><li><span class="math inline">\(\mathbf{m}_{k ij}=\left\{\begin{array}{ll}1, &amp; \text { if }\left\lceil\frac{kL}{n}\right\rceil&lt;L p_{i j}+1 \\ 0, &amp; \text { else}\end{array}\right.\)</span></li><li>其中<span class="math inline">\(\left\lceil\frac{kL}{n}\right\rceil\)</span>是上限函数，与二元机类似，也使用梯度直接估计</li><li><span class="math inline">\(\frac{\partial \mathbf{m}_{k ij}}{\partial p_{i j}}=\left\{\begin{array}{l}L, \text { if } L p_{i j}-1\leq\left\lceil\frac{k L}{n}\right\rceil&lt;L p_{i j}+2 \\ 0, \text {else }\end{array}\right.\)</span></li></ul></li></ul><h4 id="损失函数">损失函数</h4><ul><li>所提出的内容加权图像压缩系统可以表述为一个失真率优化问题。目标是尽量减少失真损失和码率损失</li><li>引入了一个折中参数<span class="math inline">\(\gamma\)</span>来平衡压缩率和失真</li><li><span class="math inline">\(\mathcal{L}=\sum_{\mathbf{x} \in\mathcal{X}}\left\{\mathcal{L}_{D}(\mathbf{c}, \mathbf{x})+\gamma\mathcal{L}_{R}(\mathbf{x})\right\}\)</span><ul><li>其中c为x的编码，<span class="math inline">\({L}_{D}(\mathbf{c},\mathbf{x})\)</span> 为失真损失（MSE损失），<span class="math inline">\({L}_{R}(\mathbf{x})\)</span> 为码率损失</li></ul></li><li>作者在码率损失中引入了一个阈值r，来惩罚高于r的编码长度，使学习到的压缩系统达到与给定的压缩率相当的压缩率<ul><li><span class="math inline">\(\mathcal{L}_{R}(\mathbf{x})=\left\{\begin{array}{ll}\sum_{i,j}(P(\mathbf{x}))_{i j}-r, &amp; \text { if } \sum_{i,j}(P(\mathbf{x}))_{i j}&gt;r \\ 0, &amp; \text { otherwise}\end{array}\right.\)</span></li></ul></li></ul><h3 id="convolutional-entropy-encoder">Convolutional entropyencoder</h3><ul><li>由于没有熵约束，上面提到的压缩系统生成的编码在熵率方面不是最优的</li><li>一般有两种熵压缩方法，即Huffmantree和算术编码。其中算术编码在定义良好的<strong>上下文环境</strong>下表现出较好的压缩率，作者采用了算术编码</li><li><img src="/2021/01/25/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-0x10/3.png"></li><li>采用了基于CABAC框架的二进制算术编码<ul><li>设c为n个二进制位图的编码，m为相应的重要掩码</li><li>在对c进行编码时，通过修改编码表，重新定义上下文，并使用CNN进行概率预测</li><li>在编码进度上，简单地将每个二进制位图从左到右逐行编码，并跳过相应重要掩码值为0的位</li></ul></li><li>将 <span class="math inline">\(c_{kij}\)</span> 的上下文定义为 <span class="math inline">\(CNTX(c_{kij})\)</span>，同时考虑来自它的邻域和邻域映射的信息<ul><li>这里作者是将 <span class="math inline">\(CNTX(c_{kij})\)</span>定义为一个5×5×4的长方体，然后划分可用位（蓝色）和不可用位置（灰色）</li></ul></li><li>一种常用的概率预测方法是建立并维护一个频率表。不过由于 <span class="math inline">\(CNTX(c_{kij})\)</span>长方体的尺寸太大，无法建立频率表。所以作者引入了一个CNN模型来进行概率预测。如上图所示，卷积熵编码器<span class="math inline">\(En(CNTX(c_{kij}))\)</span>以长方体为输入，输出 <span class="math inline">\(c_{kij}\)</span>为1的概率。因此，学习熵编码器的损失可以写成:<ul><li><span class="math inline">\(\mathcal{L}_{E}=\sum_{i, j, k} m_{k ij}\left\{c_{k i j} \log_{2}\left(\operatorname{En}\left(\operatorname{CNTX}\left(c_{k ij}\right)\right)\right)\right.\)</span> <span class="math inline">\(\left.+\left(1-c_{k i j}\right) \log_{2}\left(1-\operatorname{En}\left(C N T X\left(c_{k ij}\right)\right)\right)\right\}\)</span></li></ul></li><li>作者还将卷积熵编码器扩展到量化的重要性图。为了利用二进制算术编码，采用多个二进制码映射来表示量化的重要性映射。然后训练卷积熵编码器来压缩二进制编码映射</li></ul><h3 id="section">😝😜😋</h3><p>其实这篇论文涉及到的图像压缩，也是自己第一次接触，里面很多知识点也是慢慢学习，然后也大概都明白了，包括他的网络结构和提到的CABAC算法，不过还是让我比较疑惑的一点就是他提出的卷积熵编码是怎么在网络中使用的，，，不够清楚他在那一部分使用了，感觉放到哪个部分都会怪怪的</p><p>主要就是提出了一个内容相关的权重图——importancemap来对图像内容进行加权，从而控制不同区域的码率分配，从而能够更好地进行压缩，然后就是对网络中各种量化器的可微操作，保证网络能够直接端到端训练学习。然后就是对CABAC的改进，使用了一个卷积网络来进行概率预测，使用到了更多的上下文信息。</p><h2 id="efficient-and-effective-context-based-convolutionalentropy-modeling-for-image-compression">Efficientand Effective Context-Based ConvolutionalEntropy Modeling for ImageCompression</h2><p>基于上下文的卷积熵模型用于图像压缩，虽然还没看，但是看了看左旺孟老师的视频，感觉还是对之前工作的一种改进，在上面工作的基础（对当前点的预测使用一个<span class="math inline">\(CNTX(c_{kij})\)</span>长方体的上下文信息）上，参考PixelRNN的思想，来更好地并行化操作，并且能够包含足够多的上下文信息。</p><h3 id="abstract-1">Abstract</h3><ul><li>准确估计自然图像的概率结构是图像压缩的关键</li><li>尽管最近端到端优化图像压缩取得了显著的成功，但为了简化熵建模，潜码通常被假定为完全统计分解</li><li>不过作者认为这个假设通常并不成立，可能会影响压缩性能。于是提出了基于上下文的卷积网络(CCNs)，用于高效和有效的熵建模<ul><li>引入了三维之字形扫描顺序和三维分码技术来定义并行熵解码的编码上下文</li><li>这两种方法都可以归结为在CCNs的卷积滤波器上放置平移不变二进制掩码</li><li>ps：基本都是PixelRNN的思想和一些改进</li></ul></li><li>无损压缩允许从压缩的位流进行完美的数据重构，目标是将更短的码字分配给更多的“可能的”代码<ul><li>典型的例子包括Huffman编码、算术编码和范围编码</li></ul></li><li>有损压缩丢弃输入数据的“不重要”信息，重要性的定义取决于应用程序</li><li>在有损压缩中，需要在<strong>率失真（ Rate–distortion optimization ，对视频/图像的有损（画面品质）与比特率（编码所需的数据量）同时进行最优化）</strong>之间进行权衡，其中码率由离散码的熵计算，失真由信号保真度度量</li><li>有损图像压缩的一种流行的方案是变换编码，它由三个操作组成:变换、量化和熵编码<ul><li>转换将图像映射到一个潜在的码元表示，它更适合于开发人类感知方面。早期的变换对所有比特率都是线性的、可逆的和固定的，最近的变换采用了深度神经网络(DNNs)的形式，旨在实现非线性和更可压缩的表示。基于dnn的变换大多是不可逆的，不过其促使在变换过程中丢弃感知上不重要的图像特征。从而有机会学习不同比特率下的不同变换，以获得最佳的率失真性能</li><li>误差只产生于量化过程</li><li>熵编码负责无损地将量化码压缩到位流中进行存储和传输</li></ul></li><li>在无损或有损图像压缩中，由编码器和解码器共享的潜在码的离散概率分布(即熵模型)是决定压缩性能的关键<ul><li>根据香农信源编码定理，给定一个需要编码的向量<span class="math inline">\(\mathbf y = \{y_{0}, \ldots,y_{M}\}\)</span>，y的最佳编码长度为 <span class="math inline">\(\left\lceil-log_n P(\mathbfy)\right\rceil\)</span><ul><li>这里使用二进制编码所以n=2</li></ul></li><li>然后作者说在没有进一步约束的情况下，很难估计高维空间中的P(y)，由于这个原因，大多数熵编码方案假设y在统计上被完全分解为相同的边际分布，导致编码长度为<span class="math inline">\(\left\lceil- \sum_{i=0}^M log_2P(y_i)\right\rceil\)</span></li><li>不过概率论中的链式法则提供了一个更准确的近似<ul><li><span class="math inline">\(P(\boldsymbol{y}) \approx\prod_{i=0}^{M} P\left(y_{i} \mid \operatorname{PTX}\left(y_{i},\boldsymbol{y}\right)\right)\)</span></li><li>这里的PTX就代表了当前位置 <span class="math inline">\(y_i\)</span>之前的所有上下文信息</li></ul></li><li>不过随着<span class="math inline">\(PTX(y_i,\mathbf y)\)</span>范围的变大，很难通过构造直方图来估计该条件概率，所以作者参考PixelRNN来构建长距离建模，增加上下文信息</li></ul></li><li>作者提出了基于上下文的卷积网络(CCNs)来进行有效和高效的熵建模<ul><li>给定y，作者指定了一个三维之字形编码顺序，从而在对<span class="math inline">\(y_i\)</span>进行编码的时候它的上下文信息</li><li>熵编码期间的并行计算很简单，因为每个代码的上下文都是已知的，并且很容易获得</li><li>然而，在熵解码过程中并不总是如此。首先对 <span class="math inline">\(y_i\)</span> 的部分上下文进行顺序解码，从而对<span class="math inline">\(P(y_{i} \mid \operatorname{PTX}\left(y_{i},\boldsymbol{y}\right)\)</span> 进行估计，这是非常缓慢的</li><li>为了解决这个问题，作者引入了一种3D编码划分技术，它将y按照所提出的编码顺序分成多个组。根据各自的上下文，每个组内的编码都假定是条件独立的，因此可以并行解码</li></ul></li><li>为了验证所提出的CCNs，作者将其与算术编码结合进行熵建模<ul><li>为了实现图像的无损压缩，将输入的灰度图像转换为8个二进制平面，通过优化信息论中的熵损失，训练一个CCN来预测<span class="math inline">\(y_i\)</span> 的伯努利分布</li><li>对于有损图像压缩，我们将 <span class="math inline">\(y_i\)</span>的分类分布参数化为离散混合高斯分布(MoG)，其参数(即混合权值、均值和方差)由三个CCN估计</li><li>基于CCN的熵模型通过对训练图像数据库的分析和合成变换(即原始像素空间和潜码空间的映射)进行联合优化，权衡了码率和失真</li><li>这一小部分是完全没理解，，</li></ul></li><li>DNN-Based Entropy Modeling<ul><li>熵建模的第一步也是最重要的一步是估计概率P(y)，对于大多数图像压缩技术，y被认为是统计上独立的，其熵可以通过边缘分布计算得到，因此经过高度非线性分析变换的自然图像仍然具有很强的统计冗余</li><li>在自然语言处理（NLP）中，循环神经网络(RNN)和长短期记忆(LSTM)是两种常用的建模长期依赖性的工具。在图像处理方面，PixelRNN最早尝试利用远程像素依赖性来生成图像<ul><li>不过上述方法在计算上效率低下，需要一次前向传播来生成(或估计)单个像素的概率</li></ul></li><li>为了加速PixelCNN,提出了多尺度PixelCNN，它能够在初始图像上采样两倍大的中间图像条件。可以重复此过程以生成最终的高分辨率结果。当将多尺度PixelCNN视为熵模型时，我们必须无损地压缩初始图像并将其作为边界信息发送给解码器进行熵解码<ul><li>和之前看的这部分内容感觉不太一样，，</li></ul></li><li>基于上下文的熵建模的DNNs引入了一个尺度先验，它存储每个 <span class="math inline">\(y_i\)</span>的方差参数作为边界信息。更丰富的边信息通常导致更准确的熵建模</li><li>Li等人提取每个 <span class="math inline">\(y_i\)</span>的小代码块作为上下文，采用简单的DNN进行熵建模（就是上一篇论文）。该方法计算复杂度与PixelRNN相似</li><li>Li等人和Mentzer等人用掩码DNNs实现了并行熵编码。然而，由于上下文的依赖性，熵解码必须顺序执行，这仍然非常缓慢</li></ul></li><li>DNN-Based Lossy Image Compression<ul><li>端到端有损图像压缩的一个主要问题是量化函数的梯度几乎处处为零，使得基于梯度下降的优化无效</li><li>针对量化产生的零梯度问题，提出了不同的解决策略<ul><li>从信号处理的角度来看，量化器可以用与量化箱（quantizationbin）相同宽度的加性i.i.d.均匀噪声来近似。这个近似的一个理想性质是，得到的密度是y的概率质量函数的连续松弛</li><li>引入了连续函数(没有零梯度问题)来近似量化函数。原始量化器用于前向传递，而其连续代理用于反向传递</li></ul></li></ul></li></ul><h3 id="ccns-for-entropy-modeling">CCNS FOR ENTROPY MODELING</h3><ul><li>作者为了进行高效的基于上下文的熵编码，对网络结构做了两个假设:<ul><li>对于编码块 <span class="math inline">\(\boldsymbol{y} \in\mathbb{Q}^{M \times H \times W}\)</span>（MHW为通道、高、宽），其中第t层卷积层的输出为 <span class="math inline">\(\boldsymbol{v}^{(t)} \in M \times H \times W\times N_t\)</span> ，其中 <span class="math inline">\(N_t\)</span>代表用来表示 <span class="math inline">\(\boldsymbol{y}\)</span>的特征块的数量<ul><li>假设I在输入代码块y和输出特征表示 <span class="math inline">\(v^{(T)}\)</span> 之间建立了一对多的对应关系。换句话说，第i个通道和第j个特征块在空间位置（p，q）上的特征 <span class="math inline">\(v_{i,j}^{(t)}(p,q)\)</span> 与 <span class="math inline">\(y_i(p,q)\)</span> 唯一关联</li></ul></li><li>假设 <span class="math inline">\(CTX(y_i(p,q),\boldsymboly)\)</span> 为 <span class="math inline">\(y_i(p,q)\)</span>的之前的码集（全部上下文信息）， <span class="math inline">\(SS(v_{i,j}^{(t)}(p,q))\)</span> 为 <span class="math inline">\(v_{i,j}^{(t)}(p,q)\)</span>接收域码集，用来支持其计算（支持集），那么有 <span class="math inline">\(SS(v_{i,j}^{(t)}(p,q)) \inCTX(y_i,(p,q),\boldsymbol y)\)</span><ul><li>假设II<strong>确保 <span class="math inline">\(v_{i,j}^{(t)}(p,q)\)</span> 的计算仅取决于 <span class="math inline">\(CTX(y_i(p,q),\boldsymbol y)\)</span>的子集</strong></li></ul></li><li>这两个假设共同保证了在完全卷积网络中基于上下文的熵建模的合法性，这可以通过将平移不变的二进制掩码放置到卷积滤波器来实现</li></ul></li><li>可以假设在一个二维码块中， <span class="math inline">\(\boldsymbol{y} \in \mathbb{Q}^{H \timesW}\)</span> ，第t层的mask卷积为：<ul><li><span class="math inline">\(v_{i}^{(t)}(p,q)=\sum_{j=1}^{N_{t}}\left(u_{j}^{(t)} *\left(m^{(t)} \odot w_{i,j}^{(t)}\right)\right)(p, q)+b_{i}^{(t)}\)</span><ul><li>其中*表示二维卷积，<span class="math inline">\(\odot\)</span>表示哈达玛积 ，<span class="math inline">\(w_{i,j}^{(t)}\)</span>是一个二维卷积核，<span class="math inline">\(m^{(t)}\)</span>表示对应的mask，<span class="math inline">\(b_{i}^{(t)}\)</span>为偏差</li><li>根据假设1，输入 <span class="math inline">\(u_i^{(t)}\)</span>和输出 <span class="math inline">\(v_i^{(t)}\)</span> 和 <span class="math inline">\(\boldsymbol{y}\)</span> 大小一致，输入码块 <span class="math inline">\(\boldsymbol{y}\)</span> 对应于 <span class="math inline">\(u_0^{(0)}\)</span></li></ul></li></ul></li><li>对于全卷积网络的输入层，产生 <span class="math inline">\(v_i^{(0)}(p,q)\)</span> 的编码 <span class="math inline">\(\Omega_{p, q} = \{ y(p+ \mu,q+ \nu) \}\)</span>，其中 <span class="math inline">\((\mu,\nu) \in \Psi\)</span>是以(0,0)为中心的局部指标集，那么对应可以得到码集<ul><li><span class="math inline">\(\operatorname{SS}\left(v_{i}^{(0)}(p,q)\right)=\operatorname{CTX}\left(y(p, q), \Omega_{p, q}\right) \subset\operatorname{CTX}(y(p, q), \boldsymbol{y})\)</span></li><li>其中设置mask</li><li><span class="math inline">\(\mathbf{m}^{(0)}(\mu,\nu)=\left\{\begin{array}{ll}1,&amp; \text { if }\Omega_{p, q}(\mu,\nu) \in \operatorname{CTX}(y(p, q),\boldsymbol{y}) \\ 0, &amp; \text { otherwise}\end{array}\right.\)</span></li></ul></li><li>虽然写的公式看着很麻烦，但实际上就是用了一个mask卷积，证明了它的平移不变性</li><li><img src="/C:/Users/pan/Desktop/1.22组会分享/自监督上下文建模及底层视觉应用/24.png"><ul><li>上图中， <span class="math inline">\(CTX(y_i(p,q),\boldsymboly)\)</span> 就是当前点之前的所有上下文信息， <span class="math inline">\(SS(v_{i,j}^{(t)}(p,q))\)</span>是当前接受域中的上下文信息， <span class="math inline">\(m^{(0)}\)</span> 表示标色部分的mask卷积</li></ul></li><li>在第t层，假设 <span class="math inline">\(m^{(t)} = m^{(0)}\)</span>，对于编码 <span class="math inline">\(y(p+\mu,q+\nu) \inCTX(y_i(p,q),\boldsymbol y)\)</span> ，有<ul><li><span class="math inline">\(\begin{aligned}\operatorname{SS}\left(u_{j}^{(t)}(p+\mu, q+\nu)\right) &amp; \subset\operatorname{CTX}(y(p+\mu, q+\nu), \boldsymbol{y}) \\ &amp; \subset\operatorname{CTX}(y(p, q), \boldsymbol{y}), \end{aligned}\)</span></li><li>意思就是只要 <span class="math inline">\(y(p+\mu,q+\nu)\)</span> 在<span class="math inline">\(y(p,q)\)</span> 的上下文中，那么就可以从<span class="math inline">\(u_i^{(t)}(p+\mu,q+\nu)\)</span> 中计算 <span class="math inline">\(v^{(t)}(p,q)\)</span></li><li>另外t&gt;0的 <span class="math inline">\(u_j^{(t)}(p,q)\)</span> 由<span class="math inline">\(CTX(y_i(p,q),\boldsymbol y)\)</span>生成，进而计算得到 <span class="math inline">\(v_i^{(t)}(p,q)\)</span>，所以第t层的mask可以写为</li><li><span class="math inline">\(\mathbf{m}^{(t)}(\mu,\nu)=\left\{\begin{array}{ll}m^{(0)},&amp; \text { if }(\mu,\nu) \neq (0,0) \\ 1, &amp; \text { otherwise}\end{array}\right.\)</span></li></ul></li><li>利用上面的平移不变掩码，可以进行并行编码，但解码过程仍然需要顺序进行</li><li><img src="/C:/Users/pan/Desktop/1.22组会分享/自监督上下文建模及底层视觉应用/25.png"></li><li>为了加快熵解码的速度，作者进一步删除代码之间的依赖关系，但存在模型准确性的风险<ul><li>具体操作就是将 <span class="math inline">\(\boldsymbol{y}\)</span>划分成K组 <span class="math inline">\(\{GP_{0}(\boldsymbol{y}), \ldots,GP_{K-1}(\boldsymbol{y})\}\)</span>，并假设各组之间在统计上是独立的</li><li>导致 <span class="math inline">\(y(p,q) \inGP_k(\boldsymbol{y})\)</span> 的部分上下文 <span class="math inline">\(PTX(y(p,q),\boldsymbol{y}) =\{GP_{0}(\boldsymbol{y}), \ldots,GP_{K-1}(\boldsymbol{y})\}\)</span></li><li>这样的话，各部分的解码就可以并行进行</li><li>不过具体的划分方式就会很大程度上收到编码顺序的影响<ul><li>对于光栅编码，<strong>直接排除当前行</strong>的操作是最简单的，但对于CABAC编码而言，去掉的当前行信息是十分重要的</li><li>所以作者将编码顺序切换到了zigzag方式（如上图），其中 <span class="math inline">\(GP_k(\boldsymbol{y}) = \{ y(p,q) | p+q=k\}\)</span> , <span class="math inline">\(PTX(y(p,q),\boldsymbol{y}) =\{ y(p&#39;,q&#39;) | p&#39;+q&#39;&lt;k \}\)</span> ，对应mask</li><li><span class="math inline">\(\mathbf{m}^{(t)}(\mu,\nu)=\left\{\begin{array}{ll}0,&amp; \text { if }\mu+\nu \neq 0 \\ 1, &amp; \text { otherwise}\end{array}\right.\)</span></li><li>如上图(d)，这样的话，对于黄色位置，它的上下文信息都能够很好的包含进去（蓝色同理），这样的话，当前解码过程（<span class="math inline">\(p+q=k\)</span>）中就可以并行进行</li></ul></li></ul></li><li>进一步可以将其扩展到三维中<ul><li><img src="/C:/Users/pan/Desktop/1.22组会分享/自监督上下文建模及底层视觉应用/34.png"></li><li>感觉这个理解起来略抽象，不过还好，照着二维的方法进一步拓展就得到这个了</li></ul></li></ul><h3 id="ccn-based-entropy-models-for-lossless-image-compression">CCN-BASEDENTROPY MODELS FOR LOSSLESS IMAGE COMPRESSION</h3><p>基于ccn的无损图像压缩熵模型</p><ul><li>首先对图像进行二值化操作得到 <span class="math inline">\(x \in\mathbb{R}^{H \times W}\)</span> ，从而得到一个3D的编码块</li><li><span class="math inline">\(y_{r}(p, q)=\left\lfloor\frac{x(p,q)}{2^{7-r}}\right\rfloor \bmod 2, \quad r=0,1, \ldots, 7\)</span></li><li>其中，作者用r = 0索引最有效位平面。CCN以 <span class="math inline">\(\boldsymbol{y}\)</span>为输入，并产生一个相同大小的特征块 <span class="math inline">\(\boldsymbol{v}\)</span>(为了便于标注，省略了上标(T))来计算伯努利分布 <span class="math inline">\(P(y_r(P,  q)|SS(v_r(P, q))\)</span>的均值估计</li><li><img src="/2021/01/25/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-0x10/7.png"><ul><li>其中a中的MConv表示mask卷积，b和c表示学习到的CCN的算数编码和解码</li><li>使用了四个残差连接来加速训练，最后一层使用了sigmoid激活，其他都是ReLU</li></ul></li></ul><h3 id="ccn-based-entropy-models-for-lossy-image-compression">CCN-BASEDENTROPY MODELS FOR LOSSY IMAGE COMPRESSION</h3><p>基于ccn的有损图像压缩熵模型</p><p><img src="/2021/01/25/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-0x10/8.png"></p><ul><li>在有损图像压缩中，我们的目标是最小化码率和失真的加权和 <span class="math inline">\(\ell_{r}+\lambda \ell_{d}\)</span></li><li>本文提出的压缩方法由四个部分组成:分析变换 <span class="math inline">\(g_a\)</span>、量化器 <span class="math inline">\(g_d\)</span>、基于CCN的熵模型和合成变换 <span class="math inline">\(g_s\)</span><ul><li>分析变换gata以彩色图像 <span class="math inline">\(\boldsymbol{x}\)</span> 为输入，产生潜码表示 <span class="math inline">\(z\)</span>，进一步量化后产生离散码块 <span class="math inline">\(\boldsymbol{y}\)</span><ul><li><span class="math inline">\(g_a\)</span>由3层卷积组成，每个卷积之后是一个因子为2的下采样。在每次下采样后，采用由7个卷积组成的密集块（DenseBlock）</li><li>在最后一个密集块之后，添加了另一个带有M个滤波器的卷积层来产生z</li></ul></li><li>合成变换 <span class="math inline">\(g_s\)</span>具有分析变换的镜像结构。其中，采用深度-空间整形对特征图进行上采样。最后一个卷积层与三个滤波器负责在RGB空间产生压缩图像</li><li>对于量化器 <span class="math inline">\(g_d\)</span> ，将其第 <span class="math inline">\(r\)</span> 个通道的量化中心用 <span class="math inline">\(\left\{\omega_{r, 0}, \ldots, \omega_{r,L-1}\right\}\)</span> 参数化，其中L是量化等级数<ul><li>给定一组固定的 <span class="math inline">\(\omega\)</span>，我们通过将 <span class="math inline">\(z_r(p, q)\)</span>映射到它最近的中心来实现量化，从而使量化误差最小化</li><li><span class="math inline">\(y_{r}(p, q)=g_{d}\left(z_{r}(p,q)\right)=\underset{\left\{\omega_{r,l}\right\}}{\operatorname{argmin}}\left\|z_{r}(p, q)-\omega_{r,l}\right\|_{2}^{2}\)</span></li><li>不过由于梯度处处为零，无法进行反向传播，所以使用一个连续代理 <span class="math inline">\(\hat g_{d}\)</span></li><li>在训练中，前向传播依旧使用 <span class="math inline">\(g_{d}\left(z_{r}(p, q)\right)\)</span>，反向传播的时候使用 <span class="math inline">\(\hatg_{d}\)</span></li><li>然后使用最小化均方误差(MSE)来优化量化中心 <span class="math inline">\(\omega\)</span></li><li><span class="math inline">\(\left.\ell(\boldsymbol{\omega})=\frac{1}{M H W}\sum_{r, p, q} \| z_{r}(p, q)-y_{r}(p, q)\right)\|_{2}^{2}\)</span></li></ul></li></ul></li><li>在不知道量化编码 <span class="math inline">\(\boldsymbol{y}\)</span>的分类分布的情况下，作者使用离散化的MoG分布，其参数由所提出的CCNs预测，把<span class="math inline">\(C\)</span> 分量的可微MoG分布写成<ul><li><span class="math inline">\(y_{r}(p, q) \sim \sum_{i=0}^{C-1}\pi_{i} \mathcal{N}\left(y_{r}(p, q) ; \mu_{i},\sigma_{i}^{2}\right)\)</span></li><li>其中，<span class="math inline">\(\pi,\mu_{i},\sigma_{i}^{2}\)</span> 表示第i个分量的混合权重、平均值和方差</li><li>然后，将 <span class="math inline">\(y_r(p,q)\)</span>的概率计算为编码所在的量化区间 <span class="math inline">\(\Delta\)</span> 的积分</li><li><span class="math inline">\(P\left(y_{r}(p, q)\right)=\int_{\Delta}\sum_{i=0}^{C-1} \pi_{i} \mathcal{N}\left(\xi ; \mu_{i},\sigma_{i}^{2}\right) d \xi\)</span></li></ul></li><li>提出的有损图像压缩熵模型，该模型由三个相同结构的ccn组成，如上图（右）所示<ul><li>每个CCN由9个mask卷积组成，其中有3个剩余连接用于生成C特征块，与MoG中的组件数量相匹配</li><li>分别输出混合权值、平均值和方差来建立离散化的MoG分布</li></ul></li><li>与无损图像压缩类似，将优化后的熵模型与算术编码相结合，进行实验比较</li></ul><h3 id="experiments">Experiments</h3><ul><li><img src="/2021/01/25/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-0x10/9.png"><ul><li>所提出的CCN与性能最佳的MCN模型相匹配，这表明在所提出的zigzag编码顺序和码分技术下，CCN将最重要的代码作为当前处理的代码的部分上下文</li></ul></li><li><img src="/2021/01/25/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-0x10/12.png"><ul><li>与其他神经网络相比，在压缩比率上比较一致，但是解码速度上要快上将近100倍</li></ul></li><li><img src="/2021/01/25/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-0x10/10.png"><ul><li>CCN与其他无损图像压缩标准的比较；消融实验，CCN(N,S)表示有N个特征块，S× S滤波器大小的CCN</li></ul></li><li>在编码方面， <span class="math inline">\(CCN_{light}\)</span>的速度最快，其次是多尺度PixelCNN和SIN(性能最好的变量)</li><li>尽管编码时间相似，但它们的解码复杂性却截然不同。多尺度PixelCNN是最快的解码器，其次是SIN。由于解码的顺序性质，MCN是最慢的。CCN在保持几乎相同的比特率的情况下，通过所提出的码分割技术实现了对MCN的显著改进。此外，<span class="math inline">\(CCN_{light}\)</span> 加速了CCN30倍以上，在模型效率和模型精度之间取得了很好的平衡</li><li>除CCNlight外，所有竞争模型的复杂度都相当</li><li><img src="/2021/01/25/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-0x10/11.png"><ul><li>可以看到在更高的比特率下对图像进行编码和解码需要更多的时间。这是因为在更高的比特率下使用更大的M来保存更有知觉意义的信息，对应更多的卷积运算和更长的处理时间</li></ul></li></ul><h3 id="section-1">😝😜😋</h3><p>天哪，我终于看完了，太不容易了，看的我头疼，这篇论文莫名地感觉技术含量贼高，，，可能也是因为是最新发表的论文所以里面涉及的一些内容都是蛮难的</p><p>感觉作者也是从数学角度上对图像压缩做了一次分析，然后用了好多公式做了个推理，虽然感觉就是说明了mask卷积的用法，但公式部分的推导过程和理解还是蛮费脑子的。在说明完这一部分之后，作者对之前的mask卷积进行了改进，在解码过程中忽略掉当前行的信息，从而实现并行操作，不过会丢失掉很重要的信息，所以作者更改了编码顺序，从而保证信息的传递</p><p>再后面，就是作者提出了对于无损/有损压缩的网络结构，其实都还是蛮常规的网络结构，没有太多花里胡哨的东西，然后将训练得到的CCN网络用来进行图像压缩</p><p>emm怎么说呢，也不能说十分了解，不过对于他的思想和一些做法都能够掌握，但是他里面的一些具体做法和论文中的说法，还是看不懂，感觉怪怪的，有点没有头绪，，，就像是突然蹦出来的一句话，先这样吧，以后有机会可以再翻出来好好看看</p>]]></content>
    
    
    <categories>
      
      <category>study</category>
      
    </categories>
    
    
    <tags>
      
      <tag>自监督学习</tag>
      
      <tag>底层视觉应用</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>论文阅读-0x09</title>
    <link href="/2021/01/23/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-0x09/"/>
    <url>/2021/01/23/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-0x09/</url>
    
    <content type="html"><![CDATA[<p>最近轮到我准备组会分享了，然后老师给了我一个左旺孟老师的视频，是关于“自监督上下文建模及底层视觉应用”相关的，里面涉及的不少内容都没有接触过，所以看的过程也是磕磕绊绊，然后想着把里面涉及到的论文都好好看一下，感觉再回过头去看视频应该会轻松一些。</p><p>论文的阅读笔记：</p><p>《 Pixel Recurrent Neural Networks 》 Google Deepmind 2016，ICML2016</p><p>《 Conditional Image Generation with PixelCNN Decoders 》 GoogleDeepmind ， NIPS 2016</p><span id="more"></span><h2 id="pixel-recurrent-neural-networks">Pixel Recurrent NeuralNetworks</h2><h3 id="abstract">Abstract</h3><ul><li>自然图像的分布建模是无监督学习中的一个标志性问题。该任务需要一个同时具有表现力、可处理性和可伸缩性的图像模型</li><li>作者提出了一个深度神经网络，按顺序沿着两个空间维度来预测图片中的像素。对原始像素值的离散概率建模，并对图像中的完整依赖关系进行编码</li><li>生成图像模型是无监督学习中的一个核心问题</li><li><strong>概率密度模型</strong>可用于各种各样的任务，从图像压缩到图像inpainting和去模糊等重建形式，再到生成新图像</li><li>生成式模型实际上有无穷无尽的图像数据可供学习。然而，由于图像的高维性和高度结构化，估计自然图像的分布是非常具有挑战性的</li><li>生成模型的一个最重要的障碍是建立复杂的、有表现力的、易于处理和可扩展的模型。这种权衡产生了各种各样的生成模型，每种模型都有自己的优势。大部分工作集中随机潜在变量模型，如VAE，其目的是提取有意义的表征，但往往伴随着难以处理的推理步骤，这可能会阻碍他们的表现</li><li>对图像中像素的联合分布进行可跟踪建模的一种有效方法是将其转换为条件分布的乘积</li><li>这种方法已被采用在自回归模型中，如NADE和全可见神经网络</li><li>因子分解将联合建模问题转化为序列问题，在序列问题中，需要学习之前所有生成的像素来预测下一个像素</li><li>不过为了模拟像素之间的高度非线性和远距离相关性以及由此产生的复杂条件分布，就必须建立一个具有高度表达性的序列模型</li><li>本文作者提出了一个二维RNN网络——PixelRNN由多达12个快速的二维长短期记忆(LSTM)层组成。这些层在其状态下使用LSTM单位<ul><li>该二维结构确保信号在从左到右和从上到下两个方向上都能很好地传播</li><li>设计了两种类型的层：</li><li>第一种是行LSTM层，沿着每一行进行卷积</li><li>第二种类型是对角线BiLSTM层，其中以一种新颖的方式沿着图像的对角线应用卷积</li><li>同时使用了残差连接的形式，从而让网络能够达到12层的深度</li></ul></li><li>同时作者也提出了另一种简化的架构，对CNN网络使用mask卷积作为具有固定依赖范围的序列模型<ul><li>一个由15层组成的全卷积网络组成，它在整个层中保留其输入的空间分辨率，并在每个位置输出一个条件分布</li></ul></li><li>作者使用一个简单的softmax层实现的多项分布将像素建模为离散值</li></ul><h3 id="model">Model</h3><ul><li>作者的目标是估计自然图像上的分布，该分布用于可跟踪地计算图像的可能性，并生成新的图像</li><li>网络每次扫描一行图像，每次扫描一行中的一个像素。对于每一个像素点，通过上下文信息来预测给出当前位置像素的条件分布</li><li>图像像素上的联合分布被分解为条件分布的乘积。预测中使用的参数在图像中的所有像素位置上共享</li><li>LSTM网络的优点是它能有效地处理对对象和场景理解至关重要的远程依赖关系</li><li>把图像x写成一维序列<span class="math inline">\(x_{1}, \ldots,x_{n^2}\)</span>,像素是逐行从图像中提取的。<strong>为了估计联合分布p(x)我们把它写成条件分布在像素上的乘积，从而进行逐行逐像素的生成</strong><ul><li><span class="math inline">\(p(\mathbf{x})=\prod_{i=1}^{n^{2}}p\left(x_{i} \mid x_{1}, \ldots, x_{i-1}\right)\)</span></li><li>其中 <span class="math inline">\(p\left(x_{i} \mid x_{1}, \ldots,x_{i-1}\right)\)</span> 是在给定之前所有像素<span class="math inline">\(x_{1}, \ldots, x_{i-1}\)</span>的情况下，第i个像素<span class="math inline">\(x_i\)</span> 的条件概率</li><li>每个像素 <span class="math inline">\(x_i\)</span>依次由三个值共同确定，每个值分别对应颜色通道红、绿和蓝(RGB)，所以可以得到分布</li><li><span class="math inline">\(p\left(x_{i} \mid x_{&lt;i}\right) =p\left(x_{i,R} \mid x_{&lt;i}\right) p\left(x_{i,G} \midx_{&lt;1},x_{i,R}\right) p\left(x_{i,B} \mid x_{&lt;1}, x_{i,R},x_{i,G}\right)\)</span></li><li>即每一种颜色都取决于其他通道以及之前生成的所有像素</li></ul></li><li><img src="/2021/01/23/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-0x09/2.png"></li><li><strong>在训练和评估过程中，像素值的分布是并行计算的，而图像的生成是顺序的</strong></li><li>所以我们的训练目标就是最大化这个乘积（最大化似然），不过为了表达这个复杂的条件概率，可以使用神经网络来表达该复杂函数</li><li>之前的方法对图像中的像素值采用连续分布。相比之下，作者将p(x)模型化为离散分布，公式中的每个条件分布都是一个多项式，用softmax层建模</li><li>每个通道变量<span class="math inline">\(x_{i, *}\)</span>简单地从256个不同的值中进行选择。离散分布具有简单的表征性，并且具有任意多模态且形状无先验的优点。实验中我们还发现，与连续分布相比，离散分布更易于学习，且具有更好的性能</li><li><img src="/2021/01/23/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-0x09/1.png"></li></ul><h4 id="pixel-recurrent-neural-networks-1">Pixel Recurrent NeuralNetworks</h4><p><img src="/2021/01/23/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-0x09/3.png"></p><h5 id="row-lstm">Row LSTM</h5><ul><li>一种单向层，它对整行图像进行从上到下的逐行处理，同时计算整行特性</li><li>用一维卷积完成，对于像素<span class="math inline">\(x_i\)</span>该层捕获像素上方的大致三角形范围的上下文信息</li><li>一维卷积的Kernel大小是k x1其中k&gt;=3。k的值越大，所捕获的上下文范围就越大</li><li>卷积中的权值共享保证了计算特征沿每一行的平移不变性</li><li>LSTM有一个input-to-state组件和一个循环state-to-state组件，他们一起确定了LSTM核心的四个门<ul><li><img src="/2021/01/23/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-0x09/4.png"></li><li>为了增强LSTM的并行能力，首先对整个二维输入映射计算得到input-to-state组件部分<ul><li>使用k x1卷积逐行进行计算，然后使用mask卷积以仅包含先前的像素（有效的上下文）</li><li>产生一个大小为4h × n ×n的张量，表示输入映射中每个位置的四个门向量，其中h是输出特征映射的数量</li></ul></li><li>当前状态的上一个隐藏状态和单元状态<span class="math inline">\(h_{i-1},c_{i-1}\)</span>，每个状态的大小都是h x nx 1</li><li><span class="math inline">\(\begin{aligned}\left[\mathbf{o}_{i},\mathbf{f}_{i}, \mathbf{i}_{i}, \mathbf{g}_{i}\right]&amp;=\sigma\left(\mathbf{K}^{s s} \circledast\mathbf{h}_{i-1}+\mathbf{K}^{i s} \circledast \mathbf{x}_{i}\right) \\\mathbf{c}_{i} &amp;=\mathbf{f}_{i} \odot\mathbf{c}_{i-1}+\mathbf{i}_{i} \odot \mathbf{g}_{i} \\ \mathbf{h}_{i}&amp;=\mathbf{o}_{i} \odot \tanh \left(\mathbf{c}_{i}\right)\end{aligned}\)</span><ul><li>其中h x n x 1大小的 <span class="math inline">\(x_i\)</span>是输入的第i行。<span class="math inline">\(\circledast\)</span>表示卷积运算，<span class="math inline">\(\odot\)</span>表示元素相乘</li><li><span class="math inline">\(K^{ss},K^{is}\)</span>是state-to-state和input-to-state的卷积核权重，后者按照上述方法进行预先计算</li><li>对于输出门、遗忘门和输入门<span class="math inline">\(o_i,f_i,i_i\)</span> ， <span class="math inline">\(\sigma\)</span> 是sigmoid激活，对于内容门<span class="math inline">\(g_i\)</span>，<span class="math inline">\(\sigma\)</span> 是tanh激活</li><li>每一步都通过一次计算输入映射得到整个行的新状态</li></ul></li></ul></li><li>不过由于是三角形感受野，所以也是没有完整地包含所有上下文信息</li></ul><h5 id="diagonal-bilstm">Diagonal BiLSTM</h5><ul><li>DiagonalBiLSTM的设计既可以并行计算，也可以捕获任意大小的图像的整个有效上下文信息</li><li>从两个方向扫描图像，从顶部的一个叫出发，向下向左（向下向右）开始，到达底部的相反角<ul><li><img src="/2021/01/23/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-0x09/5.png"></li><li>首先将整个输入映射进行倾斜（将输入映射的每一行相对于前一行偏移一个位置），得到新的倾斜表示，即一个大小为n×(2n−1)的映射</li><li>这样就可以很方便的进行卷积操作，基本和Row LSTM很相似了<ul><li>input-to-state：对于两个方向，只需要使用一个1 x 1的卷积<span class="math inline">\(K^{is}\)</span>即可，来得到四个门的4h × n ×n张量</li><li>state-to-state：然后需要使用一个2 x 1的列卷积<span class="math inline">\(K^{ss}\)</span>，采用前面的隐藏状态和单元状态，结合input-to-state组件的结果，并生成下一个隐藏状态和单元状态<ul><li>这里作者也提到，通过这样一个2 x1的卷积核，在每一步处理最小数量的信息，从而完成了全局高度非线性的计算</li><li>而且扩大这个卷积核大小也不会扩宽全局感受野</li></ul></li></ul></li><li>最后通过移除偏移位置，输出特征映射被倾斜回n×n原大小映射</li><li>这个计算在两个方向上都是重复的。这样的话是会得到两个输出映射，同时也为了防止层看到未来的像素（感觉这句话没啥用，但能理解），<strong>右输出映射将向下移动一行并添加到左输出映射</strong></li><li>也就是说先进性左上角向右下角的计算，然后将最终这个结果下移一行，然后添加到另一个方向的计算过程中，从而得到最终的结果</li></ul></li></ul><h5 id="else">else</h5><ul><li>Residual Connections<ul><li><img src="/2021/01/23/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-0x09/6.png"></li></ul></li><li>Masked Convolution<ul><li>网络中每一层每个输入位置的h特征被分成三部分，每一部分对应一个RGB通道</li><li>当预测当前像素 <span class="math inline">\(x_i\)</span>的R通道时，只使用 <span class="math inline">\(x_i\)</span>左上方生成的像素作为上下文。在预测G通道时，除了之前生成的像素外，R通道的值也可以作为上下文。同样，对于B通道，R和G通道的值都可以使用</li><li>作者在这里使用了两种掩码（掩码A和掩码B）<ul><li>掩码 A 仅应用于 PixelRNN中的第一个卷积层，限制与相邻像素和当前像素中已预测的那些颜色的连接</li><li>掩码 B应用于所有后续input-to-state卷积转换，并通过允许从颜色到自身的连接来放松掩码A 的限制</li><li>每次更新后，将input-to-state卷积中的相应权值归零</li></ul></li></ul></li></ul><h5 id="pixelcnn">PixelCNN</h5><ul><li>Row LSTM 和 DiagonalBiLSTM有一个潜在的无界依赖范围。每个状态都需要按顺序计算会导致计算成本过高。所以作者使用标准的卷积层来捕获一个有界的接受域，并计算所有像素位置的特征</li><li>使用多个卷积层来保持空间分辨率，并且没有使用池化层</li><li>同时在卷积中使用了Mask卷积，来避免使用到没有预测的上下文信息</li><li>PixelCNN相对于PixelRNN的并行化优势仅在训练或评估测试图像时可用。这两种网络的图像生成过程都是连续的，因为每个采样像素都需要作为输入返回到网络中</li></ul><h5 id="multi-scale-pixelrnn">Multi-Scale PixelRNN</h5><ul><li>Multi-Scale PixelRNN由unconditional PixelRNN和一个或多个conditionalPixelRNNs组成</li><li>unconditional PixelRNN首先以标准方式生成一个较小的s xs图像，从原始图像中下采样得到</li><li>然后conditional PixelRNNs将s x s图像作为额外的输入，生成一个更大的nx n图像<ul><li>条件网络类似于标准的PixelRNN，但是它的每一层都带有一个s xs图像的上采样版本</li><li>在上采样过程中，使用带反卷积层的卷积网络构造大小为c × n ×n的放大特征图，其中c是上采样网络输出的feature的个数</li><li>在加入过程中，对于conditional PixelRNNs中的每一层，将c x n xn映射为4h x n x n，并将其添加到相应层的input-to-state映射中</li></ul></li><li>没太看懂这部分，不过那个示意图还是能理解的，，，</li></ul><h3 id="end">End</h3><ul><li>具体网络结构<ul><li><img src="/2021/01/23/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-0x09/7.png"></li></ul></li></ul><h3 id="section">😝😜😋</h3><p>这篇论文的内容也算是反反复复看了好几遍了，也看了网上很多相关的文章介绍，虽然是16年的文章了，但里面涉及到的一些知识点和方法还是蛮陌生的</p><p>其实感觉作者提出的方法并不是很复杂，但是他的实现和里面包含的理论还是感觉有点复杂的，可能也是因为自己基础不牢的原因吧，这样完整看下来之后，感觉其实也没什么特别好整理分享出来的，但是确实自己学习到了很多东西。论文里面作者主要是使用了一些比较巧妙但也是比较系统化的方法实现了对生成模型的一种构建，感觉自己对于一些地方不清楚了解的地方也是主要集中于生成模型这个东西，同时里面使用了巧妙地并行编码方式和mask卷积来达到了作者的预期目标。</p><h2 id="conditional-image-generation-with-pixelcnn-decoders">ConditionalImage Generation with PixelCNN Decoders</h2><h3 id="abstract-1">Abstract</h3><ul><li>基于PixelCNN架构的新的图像密度模型来探索条件图像生成</li><li>该模型可以以任何向量为条件，包括描述性标签或标签，或由其他网络创建的潜在嵌入<ul><li>当以ImageNet数据库中的类标签为条件时，该模型能够生成不同的、真实的场景，代表不同的动物、对象、风景和结构</li><li>当卷积网络给出一张看不见的脸的单一图像时，它会生成同一个人的各种新肖像，这些人的面部表情、姿势和光照条件都不同</li><li>作者还展示了条件PixelCNN可以在图像自动编码器中作为一个强大的解码器</li><li>该模型中的Gated卷积层提高了PixelCNN的log-likelihood（对数似然），匹配了PixelRNN在ImageNet上的最新性能，大大降低了计算成本</li></ul></li><li>利用神经网络进行图像建模的最新进展使得生成捕获训练数据高级结构的各种自然图像成为可能。尽管此类无条件模型本身令人着迷，但是<strong>图像建模的许多实际应用都要求模型以先验信息为条件</strong><ul><li>例如，一个图像模型用于强化学习规划在视觉环境中需要根据给定的状态和操作来预测未来场景。类似地，诸如去噪，去模糊，修复，超分辨率和着色之类的图像处理任务依赖于生成基于噪点或不完整数据的改进图像</li></ul></li><li>该网络返回显式概率密度，使其直接应用于压缩和概率规划与探索等领域</li><li>该体系结构的基本思想是使用自回归连接对图像逐像素建模，<strong>将联合图像分布分解为条件的乘积</strong></li><li>原论文中提出了两种变体<ul><li>PixelRNN，其像素分布采用二维LSTM建模</li><li>PixelCNN，其像素分布采用卷积网络建模</li><li>pixelRNN通常性能更好，但pixelCNN训练起来更快，因为卷积更容易并行化</li></ul></li><li>结合这两个模型的优点，通过引入一个有门选的变量PixelCNN(GatedPixelCNN)<ul><li>匹配在CIFAR和ImageNet上PixelRNN的对数似然</li></ul></li><li>作者还引入了带有条件变量的Gated PixelCNN的(Conditional PixelCNN)<ul><li>它允许我们对给定潜在向量嵌入的自然图像的复杂条件分布建模</li><li>证明了一个单一的条件PixelCNN模型可以被用来从不同的类别中生成图像，如狗、割草机和珊瑚礁，通过简单地对类进行一个one-hotencoding</li><li>同样作者使用嵌入来捕获图像的高级信息，从而生成大量具有类似特征的图像。来对嵌入编码的不变性有了更深入的了解——例如，可以基于单一的图像生成同一个人的不同姿态。同样的框架也可以用来分析和解释深层神经网络中的不同层次和活动</li></ul></li></ul><h3 id="gated-pixelcnn">Gated PixelCNN</h3><ul><li><img src="/2021/01/23/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-0x09/8.png"></li><li>PixelCNNs(和PixelRNNs)将图像x上像素的联合分布建模为以下条件分布的乘积，其中<span class="math inline">\(x_i\)</span>为单个像素:<ul><li><span class="math inline">\(p(\mathbf{x})=\prod_{i=1}^{n^{2}}p\left(x_{i} \mid x_{1}, \ldots, x_{i-1}\right)\)</span></li><li>像素依赖关系的顺序是光栅扫描顺序：逐行逐像素扫描每行</li><li>因此，每个像素都依赖于它上面和左边的所有像素，如上图(左)所示</li></ul></li><li>为了保证CNN只能使用当前像素上方和左侧像素的信息，卷积的滤波器被mask，如上图(中间)所示</li><li>对每个像素依次建模三个颜色通道(R, G, B)，B以(R, G)为条件，G以R为条件<ul><li>这是通过将网络每一层的featuremap分割成三个，并调整掩模张量的中心值来实现的</li><li>然后使用softmax对每个颜色通道的256个可能值进行建模</li></ul></li><li>PixelCNN通常由一组隐层卷积层组成，以N x N x 3的图像作为输入，并产生Nx N x 3 x 256的预测作为输出<ul><li>在训练期间，使用卷积并行地对所有像素进行预测</li><li>在采样期间，预测是连续的：每个像素被预测之后都会被反馈到网络中以预测下一个像素<ul><li>这种序列性对于生成高质量图像至关重要，因为它允许每个像素以高度非线性和多模态的方式依赖于之前的像素</li></ul></li></ul></li><li>PixelRNN使用空间LSTM层，效果要比PixelCNN好，作者认为是因为LSTM中的循环连接允许网络中的每一层访问之前像素的整个邻域，而PixelCNN可用的邻域区域随着卷积堆栈的深度线性增长<ul><li>不过可以通过增加卷积层深度来解决这一问题</li><li>另一个潜在的优势是PixelRNNs包含乘法单位(以LSTM门的形式)，这可能帮助它建模更复杂的交互</li></ul></li><li>为此作者使用<strong>门控激活单元</strong>替换了原pixelCNN中卷积层之间的矫正线性单元：<ul><li><span class="math inline">\(\mathbf{y}=\tanh \left(W_{k, f} *\mathbf{x}\right) \odot \sigma\left(W_{k, g} *\mathbf{x}\right)\)</span><ul><li><span class="math inline">\(\sigma\)</span>是sigmoid非线性激活函数，k是层数，<span class="math inline">\(\odot\)</span> 表示元素相乘，*表示卷积操作</li></ul></li><li>由此得到Gated PixelCNN</li></ul></li><li>在上面图(右上角)中，作者展示了输入图像上一个3×3mask滤波器的有效接受域的逐渐增长。不过可以看到输入图像的很大一部分被掩蔽卷积结构忽略了——“盲点”（blindspot）<ul><li>可以覆盖多达四分之一的潜在接收场(当使用3x3滤波器时)，这意味着当前像素右侧的任何内容都不会被考虑在内</li></ul></li><li><img src="/2021/01/23/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-0x09/9.jpg"></li><li>为消除这个盲点，可将每一层的 receptive field分为两个部分，通过合并两个卷积网络堆栈来消除盲点<ul><li>vertical部分，是当前像素上方的掩码皆为1，其本行和下方行的掩码皆为0，使得receptive field 在没有盲点的区域以矩形方式生长</li><li>horizontal部分，只是当前像素所在行，水平部分将当前层已经预测的部分以及垂直部分的输出作为输入</li></ul></li><li>如上图输入分为两部分<ul><li>将<span class="math inline">\(W_f\)</span>和<span class="math inline">\(W_g\)</span>合并在一个单独的mask卷积中以增加并行化（图中蓝色菱形）</li><li>左边是vertical部分，因此维度是 <span class="math inline">\(n\timesn\)</span></li><li>右边输入是 horizontal 部分，因此是一行，即<span class="math inline">\(1\times n\)</span></li><li>p 是feature maps的个数，作者在实验中在 vertical 部分加入了ResidualConnection 也没有什么提升的效果，因此省了</li></ul></li><li>给定一个用潜在向量h表示的高级图像描述，来寻找适合该描述的图像的条件分布p(x|h)建模。在形式上，条件PixelCNN模型的分布如下：<ul><li><span class="math inline">\(p(\mathbf{x})=\prod_{i=1}^{n^{2}}p\left(x_{i} \mid x_{1}, \ldots, x_{i-1},h\right)\)</span></li><li>然后将h添加到非线性激活部分，从而对条件分布进行建模：</li><li><span class="math inline">\(\mathbf{y}=\tanh \left(W_{k, f} *\mathbf{x} + V_{k,f}^T\mathbf{h} \right) \odot \sigma\left(W_{k, g} *\mathbf{x} + V_{k,g}^T\mathbf{h} \right)\)</span></li></ul></li><li>作者在这里提到，h是一个指定类的one-hot编码，仅包含有关图像中应包含的内容而不包含位置的信息<ul><li>为此作者开发了一种条件作用依赖于位置的变体</li><li>在h中嵌入图像特定结构的位置信息，使用反卷积网络m()来将h映射到空间表示s=m(h)（具有与图像相同的宽度和高度，但可以具有任意数量的特征图）</li><li><span class="math inline">\(\mathbf{y}=\tanh \left(W_{k, f} *\mathbf{x} + V_{k,f}^T * \mathbf{s} \right) \odot \sigma\left(W_{k, g} *\mathbf{x} + V_{k,g}^T * \mathbf{s} \right)\)</span><ul><li><span class="math inline">\(V_{k,g}^T * \mathbf{s}\)</span>是一个无mask的1x1卷积</li></ul></li></ul></li><li>自编码器：一个编码器encoder将输入图像x并将其映射为低维表示h，一个解码器decoder尝试重建原始图像<ul><li>作者使用Conditional PixelCNN替换反卷积解码器</li><li>希望网络可以从低维表示h中忽略那些像素统计信息，而去关注更高级的抽象信息</li></ul></li></ul><h3 id="experiments">Experiments</h3><ul><li>实验部分还是蛮有趣的，放几张图看看吧</li><li><img src="/2021/01/23/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-0x09/10.png"></li></ul><h3 id="section-1">😝😜😋</h3><p>这个论文短很多，主要就是在PixelRNN的基础上进行了改进和完善，一个是mask卷积在CNN中出现的盲点问题，然后改用两个卷积来分别处理，再合并；另一个就是条件生成，加入了一个one-hot编码来控制条件生成；同时改进了激活函数部分，使用门控激活单元。</p><p>可能都是很长时间以前的东西了，但现在自己学习起来还是回感觉有很多不熟悉了解的地方，还是需要继续加油啊~</p>]]></content>
    
    
    <categories>
      
      <category>study</category>
      
    </categories>
    
    
    <tags>
      
      <tag>自监督学习</tag>
      
      <tag>底层视觉应用</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>论文阅读-0x08</title>
    <link href="/2020/12/07/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-0x08/"/>
    <url>/2020/12/07/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-0x08/</url>
    
    <content type="html"><![CDATA[<p>深度学习最近突然有一些做图像着色的想法，也有个想法也算是比较成熟，不过看到已经有不少比较成熟的论文了，所以再学习看一下，还是想试着做一下，哪怕做出来效果不好，但也是自己的一次尝试学习嘛，噶油！</p><p>论文的阅读笔记：</p><p>《 Example-Based Colourization Via Dense Encoding Pyramids 》<a href="https://github.com/chufengxiao/Example-based-Colorization-via-Dense-Encoding-pyramids">[code]</a>, Pacific Graphics 2018</p><p>《 Interactive Deep Colorization Using Simultaneous Global and LocalInputs 》， ICASSP 2019</p><span id="more"></span><h2 id="example-based-colourization-via-dense-encoding-pyramids">Example-BasedColourization Via Dense Encoding Pyramids</h2><p>通过密集编码金字塔的基于实例的着色</p><h3 id="abstract">Abstract</h3><ul><li>提出了一种新的基于深度样本的图像着色方法——密集编码金字塔网络</li><li>作者在本文中将颜色化定义为一个多项分类问题</li><li><strong>给定一幅灰度图像和一幅参考图像，该网络利用大规模数据，然后通过分析参考图像的颜色分布来预测灰度图像的颜色</strong></li><li>作者通过扩展网络，提出了一种新的并行残差密集块来有效地提取颜色表示的局部-全局上下文</li><li>不像全自动彩色化产生固定颜色的图像，该网络的参考图像是灵活的;自然图像和简单的调色板都可以用来引导色彩</li></ul><p><img src="/2020/12/07/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-0x08/1.png"></p><ul><li><p>密集编码金字塔能够根据参考图像或调色板的不同颜色产生似是而非的结果。更重要的是，它比其他的更友好，因为用户可以很容易地操纵调色板，从而得到所需的样式</p></li><li><p>现有的图像着色技术可归纳为两大类</p><ul><li>传统的基于用户交互的图像着色技术<ul><li>在用户输入的指导下，一个优化过程被用来传播颜色。然而，优化过程通常需要大量的计算量和时间。此外，在边界开放的区域经常会出现漏色和漂色现象</li><li>一些工作提出了在参考图像的指导下对图像着色，不过一旦参考图像和灰度图像的差异很大，这些方法可能无法保证视觉上合理的结果</li><li>一旦参考图像和灰度图像的差异很大，这些方法可能无法保证视觉上合理的结果</li></ul></li><li>基于学习的图像自动着色技术<ul><li>基于学习的自动方法不能灵活地生成所需的颜色分布的图像</li><li>一些色彩化方法，不过需要用户有足够的艺术感来选择合适的配色方案，特别是对于真实的图像</li></ul></li></ul></li><li><p>本文提出了一种新的基于实例的深度颜色化方法，即密集编码金字塔网络(DenseEncoding Pyramids Network, DEPN)</p><ul><li>关键思想是图像的颜色先验总是来自于图像本身。网络通过将参考图像的颜色分布映射到灰度图像来实现图像着色</li><li>将网络设计成金字塔结构，以利用颜色表示的层次结构的金字塔形状</li><li>提出了一种新的并行残差密集块（PRDB），用于探索网络中更丰富的局部-全局上下文信息</li><li>提出了一种分层解码器滤波器(HDEF)来聚合相邻两层之间的颜色分布结果</li></ul></li><li><p>为了从灰度图像中恢复丢失的信息，传统的着色方法需要用户的指导/用户交互/参考图像</p><ul><li>一些颜色转移方法也可以用于转移颜色到灰度图像<ul><li>它们建立了一个映射函数，将颜色分布从一个映射到另一个</li><li>类似于色彩转换技术，一些方法在参考图像的引导下着色，而不需要用户干预</li><li>通过匹配参考图像和目标图像之间的<strong>亮度和纹理信息</strong>，将颜色从参考图像迁移到目标图像</li></ul></li></ul></li><li><p>基于学习的图像上色方法</p><ul><li>利用大规模彩色图像数据的全自动彩色化网络<ul><li>然而，这些没有任何提示的全自动方法高度依赖于训练数据</li><li>一旦输入图像没有被训练数据覆盖，就可能无法产生一个合理的、视觉上令人愉快的结果</li></ul></li><li>这种用户交互的网络有一个潜在的要求，那就是用户需要有足够的艺术感来选择合适的、和谐的配色方案，特别是对于真实的图像。否则，色彩化的图像可能不自然</li><li>[ZZI*17]提出了一种深度用户引导的着色方法<ul><li>该网络通过融合低级线索和从大规模数据中学习的高级语义信息来传播用户交互</li><li>然而，彩色化的结果在很大程度上取决于参考图像的质量。同时，它也必须尽可能地与目标图像相似</li></ul></li><li>[HCL*18]提出了一种基于实例的带有两个子网的局部着色方法<ul><li>设计了相似子网来寻找参考图像和灰度图像之间的语义对应，然后融合多级抽象特征</li><li>颜色化子网采用相似子网的输出来生成生动的结果</li><li>然而，当输入一个缺乏语义信息的参考图像时，比如一个简单的调色板，由于相似子网无法获取语义和亮度信息，所以整个网络无法产生可信的结果</li></ul></li></ul></li><li><p>颜色迁移</p><ul><li>也可以实现给定参考图像的着色</li><li>[CFL*15]介绍了一种基于颜色的方法来重新着色图像<ul><li>开发了一种加速聚类方法，可以从一张图像生成一个调色板</li><li>通过用户改变调色板来实现颜色转移</li></ul></li><li>[WZL*17]提出了一种包含相似性映射和细节守恒的两阶段方法<ul><li>相似度映射模型采用超像素采样和K-means聚类的方法从输入图像和参考图像中提取RGB空间的中间特征</li><li>在此基础上，提出了一种L0梯度保持算法，通过控制像素在其颜色区域内的梯度来生成迁移结果</li></ul></li></ul></li><li><p>金字塔结构（Pyramid structure）；残差结构；密集结构（densestructures）</p><ul><li>DenseNet来融合各层特征，有效地克服了消失梯度问题，且参数少得多。DenseNet由许多密集的块组成，其中任何层连接到所有随后的层</li></ul></li><li><p>对于图像着色来说，极深的结构可能没有帮助</p><ul><li>要生成语义正确、生动的彩色图像，既需要高水平的知识，也需要低水平的知识</li><li>因此，充分利用多尺度特征比学习更多的冗余特征更重要</li></ul></li></ul><h3 id="method">Method</h3><p><img src="/2020/12/07/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-0x08/2.png"></p><ul><li>目的是将参考图像的颜色分布更语义化地映射到输入的灰度图像上<ul><li>将网络设计为金字塔结构，以便利用颜色表示的固有多尺度金字塔等级</li><li>提出的并行残差密集块（PRDBs）是通过使网络更宽而不是更深来提取丰富的网络特征</li><li>进一步定义了分层解码器滤波器（HDEF），以促进金字塔网络不同层之间的信息传播，利用从其他层提取的信息作为指导，避免潜在的错误积累</li><li>参考图像的颜色分布由编码器和PRDB进行提取，然后注入到每一层，作为着色过程的指导</li></ul></li></ul><h4 id="目标函数">目标函数</h4><ul><li>给定一幅灰度图像X和一幅参考图像Y，网络的目标是学习映射F(X,Y)，以便用Y的颜色分布给X着色</li><li>在实验中，作者使用CIE Lab色彩空间代替RGB，从而满足人的色彩视觉感知一致性</li><li>将 <span class="math inline">\(X \in  \mathbb{R} ^{H \times W \times1}\)</span> 作为L通道，然后输出结果 <span class="math inline">\(\hat X\in  \mathbb{R} ^{H \times W \times 2}\)</span> ，表示图像的ab通道</li><li>类似于[ZIE16]，将颜色化问题定义为多项分类</li><li>作者将ab通道量化为大小为10的网格，从而减少ab通道的颜色对数目，进而减少计算量。最终，在整个色域中将向量Q保留为313个值，表示ab对的数量</li><li>每个网络分支的目标是最小化训练数据集上期望的分类损失<ul><li><span class="math inline">\(\hat{\theta}^{i}=\arg \min_{\theta^{i}}\left(\mathcal{L}_{c l}\left(\mathcal{F}^{i}\left(X^{i},Y^{i} ; \theta^{i}\right), Y^{i}\right)\right)\)</span><ul><li>其中<span class="math inline">\(\theta^{i}\)</span>表示第i层分支网络的参数</li><li><span class="math inline">\(\mathcal{L}_{c l}\)</span>表示交叉熵损失，用于测量参考图像Y的预测颜色分布 <span class="math inline">\(\hat Z\)</span> 与真实颜色分布 <span class="math inline">\(Z \in  \mathbb{R} ^{H \times W \times Q}\)</span>之间的误差</li></ul></li><li><span class="math inline">\(\mathcal{L}_{c l}\left(\hat{Z}^{i},Z^{i}\right)=-\sum_{h, w} \omega^{i}\left(Z_{h, w}^{i}\right) \sum_{q}Z_{h, w, q}^{i} \log \left(\hat{Z}_{h, w, q}^{i}\right)\)</span><ul><li>其中<span class="math inline">\(\omega^{i}\)</span> 表示 <span class="math inline">\(Z_{h, w}^{i}\)</span> 的再平衡权重</li><li>能够校正不平衡前景-背景分布中的ab值</li></ul></li></ul></li></ul><h4 id="密集编码金字塔dense-encoding-pyramids">密集编码金字塔——Denseencoding pyramids</h4><ul><li>输入多尺度图像，从而让来自较低层次的特征以由粗到细的方式引导较高层次的特征</li><li>主要优势是多尺度的上下文信息和递归预测机制（由粗到细）</li><li>尽管参考图像在不同尺度上的分布是相同的，但所分配的颜色取决于图像内容的特定区域，而该区域直接受到网络的输入分辨率和接受域的影响<ul><li>作者这里使用的金字塔结构从四个不同尺度的上下文信息着色图像</li><li>因此作者提出的DEPN可以看作是一种由粗到细的图像着色处理方法</li></ul></li><li>对于每一个网络分支都可以看作一个完整的着色网络<ul><li>对于第i层的 <span class="math inline">\(\mathcal{F}^{i}\)</span>，接收到一个灰度图像 <span class="math inline">\(X^{i}\)</span> 和参考图像 <span class="math inline">\(Y^{i}\)</span>，分别由X和Y进行下采样得到</li><li>首先通过普通卷积来获取浅层特征，然后插入三个PRDBs，提取不同尺度的特征</li><li>参考图像的颜色分布被注入到第一个PRDB中作为颜色提示</li><li>接着经过一个反卷积块、一个卷积块和一个解码器，生成预测 <span class="math inline">\(\hat Y^{i}\)</span> 的ab通道</li></ul></li><li>为了聚合两个相邻分支之间的特征，即 <span class="math inline">\(\mathcal{F}^{i-1}\)</span> 和 <span class="math inline">\(\mathcal{F}^{i}\)</span> ，作者提出了HDEF<ul><li>在相邻两层之间建立联系，将信息从低层传递到高层</li><li><span class="math inline">\(\mathcal{F}(G,X)=\left\{\begin{array}{l}\mathcal{F}^{i}\left(X^{i}, Y^{i}\right), i=1\\ \mathcal{F}^{i}\left(X^{i}, Y^{i},\Theta\left(\mathcal{F}^{i-1}\right)\right),i&gt;1\end{array}\right.\)</span></li><li><span class="math inline">\(\Theta\)</span>表示HDEF，作者采用了逐步训练网络的方式，从而让网络能够更好的适应多尺度的着色任务</li><li>在最后一级 <span class="math inline">\(\mathcal{F}^{i}\)</span> ，从<span class="math inline">\(Z^{i}\)</span> 解码得到的 <span class="math inline">\(\hat X^{i}\)</span>的ab值通过双线性插值上采样恢复到原始输入图像的分辨率大小</li><li><span class="math inline">\(\hat X\)</span>与X的L通道相结合形成最终输出的彩色图像</li></ul></li></ul><h4 id="并行残差密集块parallel-residual-dense-block">并行残差密集块——Parallelresidual dense block</h4><ul><li>主要是为了充分利用 <span class="math inline">\(\mathcal{F}^{i}\)</span>的多种特征，提取图像的丰富特征表示，包括输入图像、参考图像和输出图像</li><li><img src="/2020/12/07/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-0x08/3.png"><ul><li>与原来的链式残差结构做对比</li></ul></li><li>与常规卷积相比，残差和密集的结构有利于记忆底层信息并提取丰富的特征</li><li>作者这里提出说网络过深可能对于图像着色并没有很大的帮助，色彩化同样依赖于低水平和高水平的信息来产生语义上正确和不同颜色的图像</li><li><strong>充分利用多尺度特征比学习更多的冗余特征更重要</strong></li><li>因此作者这里并没有使用链式RDB结构，而是通过PRDB来拓宽网络，在并行密集结构中获取更多的信息</li><li>同时作者认为这种结构缩短了块内的信息流，从而减少了学习歧义</li><li>作者在PRDB的开始部分添加了卷积层来对前面的块进行裁剪，在最后添加了另一个卷积层来进行批处理归一化，以实现信息的集成和快速收敛</li><li>网络具体结构就不写了，和上图一致</li></ul><h4 id="分层编码-解码滤波hierarchical-decoderencoder-filter">分层编码-解码滤波——Hierarchicaldecoder–encoder filter</h4><ul><li>整个网络是一个全局编码-解码结构，编码器向高级特征中注入低级信息，解码器从分类值中恢复最终结果的色彩信息</li><li>不过来自底层的错误会累积，从而增加冗余信息。<ul><li>为此作者定义了HDEF，一种非对称的编码-解码器，作为两个相邻级之间的连接</li><li>编码器被用来映射颜色值到颜色分布</li><li>解码器能够产生每个网络分支的结果 <span class="math inline">\(Y^{i}\)</span></li></ul></li><li>作者采用软编码的方法将密集的颜色信息编码到颜色分布中<ul><li>它使网络在分类问题中比单热点编码更快地提取知识</li><li>该编码器通过搜索由RBF加权的bins的K个最近的邻居，从而将图像映射到颜色分布中</li><li>对第i级参考图像 <span class="math inline">\(Y^{i}\)</span>的颜色分布 <span class="math inline">\(Z^{i}\)</span> 进行编码</li><li><img src="/2020/12/07/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-0x08/4.png"></li><li>emmm没有特别看懂他这部分，感觉这里的K-nearestneighbours和感受野差不多，后面作者也说到在低分辨率下采用较大的 <span class="math inline">\(\left|K^{i}\right|\)</span>，然后较大的分辨率上采用较小的<span class="math inline">\(\left|K^{i}\right|\)</span></li></ul></li><li>解码器对于每一个网络分支，将分类结果 <span class="math inline">\(Z^{i}\)</span> 转换回ab通道<ul><li>利用softmax函数得到每个像素处的概率</li><li><span class="math inline">\(P\left(Z_{h, w}^{i}\right)=\frac{\exp\left(Z_{h, w}^{i}\right)}{\sum_{q=1}^{|Q|} \exp \left(Z_{h, w,q}^{i}\right)}\)</span></li><li>Q中分布的平均值由每个像素的分类概率 <span class="math inline">\(Z_{h, w}^{i}\)</span> 与Q的内积计算得出</li><li><span class="math inline">\(Dec^i \left(Z_{h,w}^{i}\right)=P\left(Z_{h, w}^{i}\right) \bigodot Q\)</span></li></ul></li><li>编码器将彩色图像映射到颜色分布，而解码器则相反<ul><li>不过这两个过程不可逆，这是因为分层编码器利用软编码方案，在牺牲密集的颜色信息的同时形成稀疏矢量</li><li>将两者组合得到</li><li><span class="math inline">\(\Theta(Z^{i})= Enc^i \left(Dec^i\left(Z^{i}\right) \right)\)</span></li><li>HDEF不是直接将颜色分布传递给下一个网络分支，而是先将颜色分布解码为颜色信息，然后再将其编码为密集特征</li><li>从而避免之前级别分支的错误累积。另一方面，Enc的密集输出有助于挖掘深层信息，为下一个网络分支提供指导</li></ul></li><li>网络的具体实现和上面的网络结构图一致</li></ul><h3 id="experiments">Experiments</h3><ul><li>与当前先进的着色方法做对比（定性&amp;定量）</li><li>然后使用各种参考图（调色板）来评估稳健性</li><li>用户研究</li><li>消融实验</li><li><img src="/2020/12/07/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-0x08/5.png"></li><li><img src="/2020/12/07/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-0x08/6.png"></li></ul><h3 id="section">😝😜😋</h3><p>看的第一篇图像上色的论文，感触还是蛮多的，慢慢写点吧，然后以后看的时候也多注意这些方面</p><ul><li>我觉得我现在最大的疑问还是网络结构上吧，这篇论文里面作者也提到了许多关于网络结构选择的想法，比如选择多尺度而不是高深度。不过作者网络结构中还是使用了编码-解码结构，因为在我的感觉里面，这种图像上色任务，更重要的是要保留这种结构信息，所以应该尽可能地保留尺度信息。<ul><li>自己主要没有特别想明白的就是，如果对图像进行编码-解码，那这样的话，就会有分辨率的损失，虽然会通过解码器进行恢复，但仍然感觉不能恢复到原始最佳的分辨率状态，emmm就是感觉那个结构信息很难保存下来，但看作者的效果是很好的，不知道是不是通过多分辨多分支操作的结果，难以理解，，，，</li><li>不过作者在将网络输入到网络之前，也是对RGB进行了处理，转换到了另一个结构空间，然后只处理ab通道，另外的L通道直接保留，然后拼接到最终的输出结果上。可能也是对图像结构信息的一种保留吧，这个可以再学习看一下</li><li>还有一点想吐槽一下的就是，作者论文里面的用到的结构其实都没有特别复杂感觉，但是作者解释完就让人感觉很麻烦，而且有几个公式和几句话我还没看懂，，，但看他的网络结构其实还蛮简单的，但是还是能感觉到图像上色这个任务所应用到的部分还是很多的，不是简单的一个端到端网络就能够轻易解决的</li><li>作者在这个网络中使用的网络结构和思想感觉都还是挺先进的，前几篇看的CVPR2020的几篇论文感觉一些思想上和这个都是很相近的</li></ul></li><li>另外发现图像上色并不像自己之前想的那么简单，作者在实验部分，借鉴一张参考图像来给图像上色的部分中，作者认为网络会学习其语义特征，进而来给灰度图像进行上色，而对于直接使用调色板来给图像上色，是缺少这种语义信息的，所以在上色过程中会出现一些问题，类似下面这种，不能够出现我们所预期想要得到的色彩效果（感觉也是因为在训练过程中没有相关数据集的学习）<ul><li><img src="/2020/12/07/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-0x08/7.png"></li><li>所以就感觉自己想的那种通过调色板来实现非常完美的图像上色还是很麻烦的，感觉很难学习到那种对应区域上色，以及调色板之外色彩的学习生成</li><li>理解上来说，感觉神经网络通过学习应该是学习一些语义特征之类的知识，但对于图像上色，如果没有很丰富的数据集支持，在最后使用的时候就很难达到我们所预期的效果，不过这东西感觉有没有很好的办法，，所以还是有点迷茫</li></ul></li><li>在作者提出的论文基础上，感觉作者在网络中使用的结构和损失函数等应该都算是比较常规的，感觉可以在损失函数和结构设计上再添加一点东西，不过不知道效果咋样了，打算再读读论文，然后试着去跑跑代码，加油！！</li></ul>]]></content>
    
    
    <categories>
      
      <category>study</category>
      
    </categories>
    
    
    <tags>
      
      <tag>图像增强与图像恢复</tag>
      
      <tag>图像着色</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>论文阅读-0x07</title>
    <link href="/2020/12/03/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-0x07/"/>
    <url>/2020/12/03/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-0x07/</url>
    
    <content type="html"><![CDATA[<p>论文的阅读笔记：</p><p>《 Space-Time-Aware Multi-Resolution Video Enhancement 》<a href="https://github.com/alterzero/STARnet">[code]</a> , CVPR 2020</p><p>《 From Fidelity to Perceptual Quality: A Semi-Supervised Approachfor Low-Light Image Enhancement 》，CVPR 2020</p><span id="more"></span><h2 id="space-time-aware-multi-resolution-video-enhancement">Space-Time-AwareMulti-Resolution Video Enhancement</h2><h3 id="abstract">Abstract</h3><ul><li>时空感知多分辨率视频增强（Space-Time-Aware Multi-Resolution VideoEnhancement）</li><li>时空超分辨率（space-time super-resolution，ST-SR）问题<ul><li>其目标是将低帧率的低空间分辨率视频转换为高空间和时间分辨率的视频</li><li>增加视频帧的空间分辨率，同时对帧进行插值以提高帧速率</li><li>现在的方法一次只能处理一方面</li></ul></li><li>作者提出的STARnet能够在空间和时间上同时处理，能够利用时间和空间之间的相互信息关系:更高的分辨率可以提供更多关于运动的详细信息，更高的帧率可以提供更好的像素校准信息</li><li>在ST-SR问题中，该模型中生成潜在的低分辨率和高分辨率表示的组件可用于微调针对空间SR或时间SR的专用机制</li><li><img src="/2020/12/03/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-0x07/1.png"><ul><li>白色和灰色矩形分别表示输入和输出帧。小矩形和大矩形分别表示S-LR和S-HR帧。省略了从图像到特征的特征提取步骤</li><li>(a)和(b)分别为原始的S-SR和T-SR方法</li><li>对于ST-SR， (c)执行T-SR生成中间帧，然后使用S-SR放大帧(如DAIN→RBPN)<ul><li>反过来，(d)执行S-SR，然后SR帧被用于使用T-SR生成中间帧(例如，RBPN→DAIN)</li></ul></li><li>STARnet(e)联合优化了所有任务(S-SR、T-SR和ST-SR)，以便在多个分辨率下相互增强空间和时间特征<ul><li>紫色箭头表示ST-SR从LR到HR的直接连接</li><li>除了上采样之外，还使用下采样将S-HR特性转换回S-LR特性，以便在多种分辨率下实现相互连接</li></ul></li></ul></li><li>现有的SR方法对空间和时间上采样是独立的<ul><li>空间SR(S-SR)采用多输入帧(即多图像SR和视频SR)，通过对相似帧进行空间对齐，将空间低分辨率(S-LR)帧超分辨为空间高分辨率(S-HR)帧（上图a）</li><li>时间SR(T-SR)旨在通过在帧之间进行时间插值，将输入帧的帧率从短时低分辨率(T-LR)帧提高到短时高分辨率(T-HR)帧（上图b）</li><li>可以通过交替地、独立地使用任何基于学习的S-SR和T-SR来执行ST-SR<ul><li>在S-LR上构造中间帧，然后用S-SR生成它们的SR帧（上图c）</li><li>对输入帧进行空间上采样S-SR，然后进行T-SR构造其中间帧（上图d）</li></ul></li></ul></li><li>然而，空间和时间显然是相关的。这种关系允许我们联合使用空间和时间表示来解决人类和机器感知的视觉任务<ul><li>直观地说,更准确的运动可以在更高的空间表现（高分辨率）</li><li>反过来,更高的时间表示(例如,更多的帧的表示内容很相似)可以用来在时间帧中准确地提取更多的空间上下文信息在多图像SR和视频SR</li></ul></li><li>为此作者提出了空间感知的多分辨率网络，即STARnet<ul><li>STARnet通过为ST-SR提供从LR到HR的直接连接，显式的合并了空间和时间表示，以便在LR和HR空间中互相增强S-SR和T-SR（上图e）</li><li>这个网络还提供了可扩展性，可以对同一个网络进行ST-SR、S-SR或T-SR的进一步优化（下图）</li><li><img src="/2020/12/03/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-0x07/2.png"><ul><li>ST-SR、T-SR、S-SR比较(S-SR: 4倍，T-SR:2倍)红色箭头表示其他方法产生的伪影和模糊，而STARnet(我们的)可以构建更好的图像</li></ul></li></ul></li><li>该文的主要贡献<ul><li>新的基于学习的ST-SR方法，它训练一个端到端的深度网络来共同学习空间和时间上下文，从而形成作者所称的时空感知多分辨率网络(STARnet)。该方法优于S-SR和T-SR方法的组合</li><li>在多个分辨率上联合学习，以估计在视频中观察到的大的和细微的运动<ul><li>在S-HR帧上执行T-SR很难估计大尺度运动，而在S-LR帧上对细微的运动进行插值则很困难</li><li>作者的联合学习通过在多个分辨率之间直接横向连接来呈现丰富的多尺度特征，从而解决了这两个问题</li></ul></li><li>一种新的S-SR和T-SR的观点优于直接的S-SR和T-SR<ul><li>与直接的S-SR和TSR方法相比，作者的S-SR和T-SR模型是通过finetuningSTAR获得的</li><li>这种来自STAR的微调允许S-SR和T-SR模型被ST-SR学习扩展<ul><li>通过插入帧和输入帧来增强S-SR</li><li>通过在S-HR中观察到的微妙运动以及在S-LR中观察到的大运动来增强T-SR</li></ul></li></ul></li></ul></li><li>Space SR<ul><li>通过更好的上采样层，残差学习，反投影，递归层和渐进式上采样来拓展帧，增强分辨率</li><li>在视频SR中，时间信息通过帧级联和递归网络保留</li></ul></li><li>Time SR<ul><li>T-SR或视频插值，主要通过合成实现<ul><li>其中T-SR使用一个流运动的形象，不过流图像会遭受模糊和大运动的困扰</li><li>可以通过对输入的S-HR帧进行空间缩减，然后分别在缩减的S-LR和输入的S-HR帧中提取大而细微的运动</li></ul></li></ul></li><li>Space-Time SR<ul><li>ST-SR的第一项工作解决了巨大的线性方程，然后从所有LR帧中创建了一个包含所有时空测量值的向量</li><li>后来，在空间和时间重复的假设下，从单个视频记录中提出了ST-SR</li><li>但仍存在一些问题，例如方程之间的依赖性，对某些参数的敏感性以及需要更长的视频以提取有意义的时空模式</li></ul></li><li>另一种方法是将S-SR和T-SR结合起来，但此方法独立地处理时空的每个上下文。尚未通过联合学习对ST-SR进行研究。如图1（c）和（d）所示</li></ul><h3 id="space-time-aware-multiresolution">Space-Time-AwaremultiResolution</h3><h4 id="formulation">Formulation</h4><ul><li><p>给定两个低分辨率（LR）帧 <span class="math inline">\((I_t^l,I_{t+1}^l )\)</span> ，尺寸为 <span class="math inline">\(M^l\times N^l\)</span></p><ul><li>ST-SR获取时空SR帧 <span class="math inline">\((I_t^{sr},I_{t+n}^{sr} , I_{t+1}^{sr})\)</span> , 尺寸为 <span class="math inline">\(M^{h} \times N^h\)</span><ul><li>其中0&lt;n&lt;1， <span class="math inline">\(M^{h} &gt; M^l , N^{h}&gt; N^l\)</span></li></ul></li><li>ST-SR的目的是从<span class="math inline">\(\left\{I_{1}^{l}\right\}_{t=0}^{T}\)</span>得到<span class="math inline">\(\left\{I_{t}^{sr}\right\}_{t=0}^{T+}\)</span><ul><li>其中T+指超过T数量的帧数</li></ul></li><li>此外，STARnet从 <span class="math inline">\((I_t^l,I_{t+1}^l)\)</span> 计算得到S-LR帧 <span class="math inline">\((I_{t+n}^l)\)</span>，用于在LR和HR上联合学习时空信息</li><li>双向的密度运动流图<ul><li><span class="math inline">\(F_{t-&gt;t+1} and F_{t+1-&gt;t}\)</span> ，从 <span class="math inline">\(I_t^l,I_{t+1}^l\)</span> 中预计算得到</li></ul></li><li><span class="math inline">\(\mathrm{L}_{t} \in \mathbb{R}^{M^{l}\times N^{l} \times c^{l}}\)</span> 和 <span class="math inline">\(\mathrm{H}_{t} \in \mathbb{R}^{M^{h} \times N^{h}\times c^{h}}\)</span> 代表S-LR和S-HR在时间t的特征图<ul><li>cl和ch为通道数</li></ul></li></ul></li><li><p>STARnet主要分为三个阶段</p><ul><li>第一阶段：初始化<ul><li>在LR和HR上实现了S-SR，T-SR和ST-SR的联合学习，其中T-SR和ST-SR在由“ST-SR”指示的同一子网中执行</li><li>四个输入： <span class="math inline">\((I_t^l,I_{t+1}^l)\)</span>及其双向流图像<span class="math inline">\(F_{t-&gt;t+1} and F_{t+1-&gt;t}\)</span></li></ul></li></ul><p><span class="math inline">\(\begin{aligned} \text { S-SR: } &amp;H_{t}=\operatorname{Net}_{S}\left(I_{t}^{l}, I_{t+1}^{l}, F_{t+1\rightarrow t} ; \theta_{s}\right) \\ H_{t+1}&amp;=\operatorname{Net}_{S}\left(I_{t+1}^{l}, I_{t}^{l}, F_{t\rightarrow t+1} ; \theta_{s}\right) \\ L_{t}&amp;=\operatorname{Net}_{D}\left(H_{t} ; \theta_{d}\right) \\ L_{t+1}&amp;=\operatorname{Net}_{D}\left(H_{t+1} ; \theta_{d}\right) \\ \text {Motion: } \quad M &amp;=\operatorname{Net}_{M}\left(F_{t \rightarrowt+1}, F_{t+1 \rightarrow t} ; \theta_{m}\right) \\ \text { ST-SR: }H_{t+n}, L_{t+n} &amp;=\operatorname{Net}_{S T}\left(H_{t}, H_{t+1},L_{t}, L_{t+1}, M ; \theta_{s t}\right) \end{aligned}\)</span></p><ul><li><p>θ表示每个网络中的一组权重</p></li><li><p>通过上采样和下采样以增强SR的特征之后，<span class="math inline">\(Net_D\)</span> 对 <span class="math inline">\(H_tand H_{t+1}\)</span> 进行缩减，用来更新 <span class="math inline">\(L_tand L_{t+1}\)</span></p></li><li><p><span class="math inline">\(Net_M\)</span> 产生运动特征的表示，由双向流计算得到。其输出为流特征图，由CNN学习得到</p></li><li><p>虽然很难直接解释这些特征，但它们旨在帮助 <span class="math inline">\(F_{t-&gt;t+1} and F_{t+1 -&gt;t}\)</span>之间的空间对齐</p></li><li><p>最后结合所有特征，特征空间中的STAR由 <span class="math inline">\(Net_{ST}\)</span> 计算得到</p></li><li><p><span class="math inline">\(Net_{ST}\)</span>可以同时实现LR和HR上集成的T-SR和ST-SR</p></li><li><p><img src="/2020/12/03/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-0x07/3.png"></p><ul><li><span class="math inline">\(Net_{ST}\)</span>：蓝色和紫色箭头部分</li></ul></li><li><p>阶段1的输出是中间帧的HR和LR特征图 <span class="math inline">\(H_{t+n}, L_{t+n}\)</span></p></li><li><p>第二阶段：Refinement（细化改善）</p><ul><li>进一步保持循环一致性，以再次细化特征图</li><li>第一阶段中得到的流特征M，在第二阶段的第一个方程中便使用<ul><li>这使得我们可以得到更可靠的特征图</li><li>进一步完善，在等式中提取残差特征，用于实践特征的精确空间对齐</li></ul></li></ul></li></ul><p><span class="math inline">\(\begin{aligned} \mathrm{t}: H_{t}^{b}&amp;=\operatorname{Net}_{B}\left(L_{t+n}, L_{t}, M ; \theta_{b}\right)\\ L_{t}^{b} &amp;=\operatorname{Net}_{D}\left(H_{t}^{b} ;\theta_{d}\right) \\ \hat{H}_{t}&amp;=H_{t}+\operatorname{ReLU}\left(H_{t}-H_{t}^{b}\right) \\\hat{L}_{t} &amp;=L_{t}+\operatorname{ReLU}\left(L_{t}-L_{t}^{b}\right)\\ \mathrm{t}+1: H_{t+1}^{f} &amp;=\operatorname{Net}_{F}\left(L_{t+n},L_{t+1}, M ; \theta_{f}\right) \\ L_{t+1}^{f}&amp;=\operatorname{Net}_{D}\left(H_{t+1}^{f} ; \theta_{d}\right) \\\hat{H}_{t+1}&amp;=H_{t+1}+\operatorname{ReLU}\left(H_{t+1}-H_{t+1}^{f}\right) \\\hat{L}_{t+1}&amp;=L_{t+1}+\operatorname{ReLU}\left(L_{t+1}-L_{t+1}^{f}\right) \\\mathrm{t}+\mathrm{n}: H_{t+n}^{f}&amp;=\operatorname{Net}_{F}\left(\hat{L}_{t}, L_{t+n}, M ;\theta_{f}\right) \\ L_{t+n}^{f}&amp;=\operatorname{Net}_{D}\left(H_{t+n}^{f} ; \theta_{d}\right) \\H_{t+n}^{b} &amp;=\operatorname{Net}_{B}\left(\hat{L}_{t+1}, L_{t+n}, M; \theta_{b}\right) \\ L_{t+n}^{b}&amp;=\operatorname{Net}_{D}\left(H_{t+n}^{b} ; \theta_{d}\right) \\\hat{H}_{t+n}&amp;=H_{t+n}+\operatorname{ReLU}\left(H_{t+n}-H_{t+n}^{f}\right)+\operatorname{ReLU}\left(H_{t+n}-H_{t+n}^{b}\right)\\ \hat{L}_{t+n}&amp;=L_{t+n}+\operatorname{ReLU}\left(L_{t+n}-L_{t+n}^{f}\right)+\operatorname{ReLU}\left(L_{t+n}-L_{t+n}^{b}\right)\end{aligned}\)</span></p><ul><li>第三阶段：Reconstruction（重建）<ul><li>使用进一个卷积层的 <span class="math inline">\(Net_{rec}\)</span>将四个特征图 <span class="math inline">\(\left(\hat{H}_{t},\hat{H}_{t+n}, \hat{H}_{t+1},\right. \left.\hat{L}_{t+n}\right)\)</span>转换为对应图像<span class="math inline">\(\left(I_{t}^{s r}, I_{t+n}^{sr}, I_{t+1}^{s r}, I_{t+n}^{l}\right)\)</span></li></ul></li></ul></li><li><p>训练目标</p><ul><li>在该训练中<ul><li>SHR图像（作为ground-truth）缩小为SLR图像</li><li>T-HR帧（作为ground-truth）缩略为T-LR帧</li></ul></li><li>包含三个loss<ul><li>空间损失：在 <span class="math inline">\(I_{t}^{sr}\)</span> 和<span class="math inline">\(I_{t+1}^{sr}\)</span> 上</li><li>时间损失：仅在 <span class="math inline">\(I_{t+n}^{l}\)</span>上</li><li>时空损失：仅在 <span class="math inline">\(I_{t+n}^{sr}\)</span>上</li><li>以上每个损失都由 <span class="math inline">\(L_1\)</span> 损失和<span class="math inline">\(L_{vgg}\)</span> 组成<ul><li>L1 是预测的超分辨率帧和真实帧之间每个像素点的损失<ul><li><span class="math inline">\(L_{1}=\sum_{t=0}^{T}\left\|I_{t}^{h}-I_{t}^{sr}\right\|_{1}\)</span></li></ul></li><li><span class="math inline">\(L_{vgg}\)</span>是使用预训练的VGG19网络在特征空间中计算得到的</li><li>为了计算 <span class="math inline">\(L_{vgg}\)</span> ， <span class="math inline">\(I^h\)</span> 和 <span class="math inline">\(I^{sr}\)</span> 都通过与VGG多个最大池化层（m =5）的微分函数fm映射到特征空间中</li><li><span class="math inline">\(L_{v gg}=\sum_{t=0}^{T}\left\|f_{m}\left(I_{t}^{h}\right)-f_{m}\left(I_{t}^{sr}\right)\right\|_{2}^{2}\)</span></li><li>L1 用于满足标准图像质量评估标准（如PSNR），并经过SR验证</li><li><span class="math inline">\(L_{vgg}\)</span> 用于改善视觉感知</li></ul></li></ul></li><li>提供了4种变体<ul><li><img src="/2020/12/03/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-0x07/4.png"></li><li>原始STAR</li><li>STAR-ST：使用了空间损失和时空损失在HR上，在时空超分辨帧 <span class="math inline">\(\left\{I_{t}^{sr}\right\}_{t=0}^{T+}\)</span>上优化网络</li><li>STAR-S：在S-HR上使用空间损失，仅优化了 <span class="math inline">\(\left\{I_{t}^{sr}\right\}_{t=0}^{T}\)</span></li><li>STAR-T：在T-HR上使用时间损失<ul><li>STAR-T可以在两种不同的方案S-LR和S-HR上进行训练</li><li><span class="math inline">\(STAR-T_{HR}\)</span>使用原始帧（S-HR）作为输入帧，而 <span class="math inline">\(STAR-T_{LR}\)</span>使用缩小的帧（S-LR）作为输入帧</li></ul></li></ul></li></ul></li><li><p>流程优化</p><ul><li>上文提到了作者使用的预先计算得到的流图像，不过正如很多文章中说到的，两个时间帧之间的大运动会使得视频插帧变得很困难<ul><li>尽管STARnet不仅在S-HR中而且在S-LR中都通过T-SR抑制了这种不良影响，但很难完全解决此问题</li><li>为了进一步改进，提出了一种简单的解决方案来精炼或去噪流图像，称为流优化（FR）模块</li></ul></li><li>令 <span class="math inline">\(F_{t-&gt;t+1} and F_{t+1-&gt;t}\)</span> 是帧 <span class="math inline">\(I_{t}^{l}\)</span> 和<span class="math inline">\(I_{t+1}^{l}\)</span> 向前和向后的动作</li><li>在训练中，可以从t时刻的输入帧到ground truth来计算 <span class="math inline">\(F_{t-&gt;t+n}\)</span></li></ul><p><span class="math inline">\(\begin{aligned} \mathrm{FR}: \hat{F}_{t\rightarrow t+1} &amp;=\operatorname{Net}_{f l o w}\left(F_{t\rightarrow t+1}, I_{t}, I_{t+1} ; \theta_{f l o w}\right) \\\hat{F}_{t+1 \rightarrow t} &amp;=\operatorname{Net}_{f l ow}\left(F_{t+1 \rightarrow t}, I_{t+1}, I_{t} ; \theta_{f l o w}\right)\end{aligned}\)</span></p><ul><li>为了减少噪声，使用损失<ul><li><span class="math inline">\(\begin{aligned} L_{\text{flow}}=&amp;\left\|\hat{F}_{t \rightarrow t+1}-\left(F_{t \rightarrowt+n}+F_{t+n \rightarrow t+1}\right)\right\|_{2}^{2} \\&amp;+\left\|\hat{F}_{t+1 \rightarrow t}-\left(F_{t+1 \rightarrowt+n}+F_{t+n \rightarrow t}\right)\right\|_{2}^{2}\end{aligned}\)</span></li></ul></li><li>加上该损失，STARnet的损失为<ul><li><span class="math inline">\(\begin{aligned} L_{r} &amp;=w_{1} *L_{1}+w_{2} * L_{f l o w} \\ L_{f} &amp;=L_{r}+w_{3} * L_{v g g}\end{aligned}\)</span></li></ul></li></ul></li></ul><h3 id="experiment">Experiment</h3><p>略过吧，，</p><h3 id="section">😝😜😋</h3><p>这篇论文看的我有点晕，而且是视频处理方面，虽然也能看懂，不过看的也比较费事。上面的很多公式啥的其实也都很简单，也就慢慢读下来了。感觉作者的主要思路就是在视频帧进行超分辨率的时候充分结合了当前时刻和下一刻帧的信息，然后为了减少大运动造成的影响，使用了一种预先计算得到的流图像，从而尽可能地得到物体运动信息，保证插入帧的质量。</p><p>关于网络结构和损失函数设计啥的，都还是比较常规的。</p><p>比较愁我的就是，论文内容里面插着各种公式，我做笔记又是手打公式感觉太麻烦了，，，不过作者各部分都介绍的还算满详细的</p><h2 id="from-fidelity-to-perceptual-quality-a-semi-supervised-approach-for-low-light-image-enhancement">FromFidelity to Perceptual Quality: A Semi-Supervised Approach for Low-LightImage Enhancement</h2><p>从保真到感官质量：低光图像增强的半监督方法</p><h3 id="abstract-1">Abstract</h3><ul><li>曝光不足会导致很多问题：能见度降低、低对比度、噪声强烈和色彩偏置等</li><li>作者提出了一种半监督学习方法用于弱光图像增强——<strong>deep recursiveband network (DRBN)</strong> ，深度递归波段网络<ul><li>用来恢复一个增强的正常光图像和成对的低/正常光图像的线性波段表示</li><li>然后基于感知质量驱动的对抗性学习和不成对数据，通过另一种可学习的线性变换对给定频段进行重组，从而获得改进的频段</li></ul></li><li>弱光环境下<ul><li>使用一些专业设备可以减轻一些退化，但仍然不可以完全避免噪声的出现</li><li>如果没有足够的光到达相机传感器，会导致场景信号被系统噪声掩盖</li><li>如果花费更长的曝光时间来抑制噪声，这将很有帮助，但是这会引入模糊性</li><li>所以通过软件算法等进行处理，为高级计算机视觉任务做铺垫</li></ul></li><li>传统方法<ul><li>直方图均化：通过拉伸动态范围来增强低光图像<ul><li>不过会产生不可预料的光照信息，并且会意外放大噪声信息</li></ul></li><li>基于Retinex理论：分别分解和处理图像的两层（即反射层和照明层）</li></ul></li><li>深度学习<ul><li>现有的损失函数与人的感知没有很好的匹配，没有捕捉到图像的内在信号结构，导致视觉结果不理想，如色彩分布偏和残留噪声</li><li>EnlightenGAN证明了利用非配对数据进行弱光增强的可行性（其数据集只有低/正常光图像，不必要成对）<ul><li>不过由于没有成对的监督，细节无法恢复，强化结果中仍然存在强烈的噪声</li></ul></li><li>全监督方法：在成对监督的情况下进行训练，在训练阶段提供了ground truth<ul><li>用于详细的信号建模，网络更有能力抑制噪声和保留细节</li></ul></li><li>无监督方法：从未配对的低/正常光照图像集中提取学习增强映射的知识<ul><li>能够更自适应地学习恢复光照、颜色和对比度</li></ul></li></ul></li><li>为此，作者希望能够结合以上两种优点的统一体系结构——构建了一种新的用于弱光图像增强的半监督学习框架</li><li>DRBN提供一种波段表示形式，以连接<strong>从成对数据获得的先验信号保真度</strong>和<strong>未配对的高质量数据集提取的先验视觉质量</strong>（其中视觉图像通过平均意见得分选择）</li><li>第一阶段（递归波段学习）：对成对的低/正态光图像进行网络训练<ul><li>通过成对的低/正常光图像进行训练来恢复线性波段表示，其估计在递归过程中是互利的</li><li>然后利用DRBN第一阶段提取的增强图像的频带表示，弥补成对数据的恢复知识与高质量图像数据集提供的感知质量之间的差距</li></ul></li><li>第二阶段（波段重组）<ul><li>通过对抗性学习来学习重构波段表示以适应高质量图像的视觉特性</li></ul></li><li>作者归纳的主要贡献<ul><li>首次尝试提出一种用于弱光图像增强的半监督学习框架<ul><li>通过一个深度递归波段表示将完全监督和非监督框架连接起来，以整合它们的优点</li></ul></li><li>该框架可以提取一系列由粗到细的波段表示<ul><li>通过递归的端到端训练，这些波段表示的估计是互利的，能够去除噪声和纠正细节</li></ul></li><li>在质量导向的对抗学习的知觉指导下重组了深层波段表示<ul><li>基于平均意见得分（MOS）在感知上选择鉴别器的“真实图像”</li></ul></li></ul></li></ul><h3 id="deep-recursive-band-network-for-semisupervised-low-light-enhancement">DeepRecursive Band Network for SemiSupervised Low-Light Enhancement</h3><h4 id="motivation">Motivation</h4><ul><li><strong>弥合信号保真度和感知质量之间的差距</strong></li><li>递归波段学习<ul><li>配对训练数据提供了较强的信号保真度约束来校正细节信息</li><li>在这个过程中，除了得到增强图像 <span class="math inline">\(\hatx\)</span> ，还从y得到了一些列波段表示 <span class="math inline">\(\left\{\Delta \hat{x}_{s_{1}}^{T}, \Delta\hat{x}_{s_{2}}^{T}, \ldots, \Delta \hat{x}_{s_{n}}^{T}\right\}\)</span><ul><li>其中 <span class="math inline">\(\hat{x}=\sum_{i=1}^{n}\hat{x}_{s_{i}}^{T}\)</span> ， <span class="math inline">\(s_i\)</span>表示波段的阶数，<span class="math inline">\(\Delta\hat{x}_{s_{i}}^{T}\)</span>通过全监督学习得到（高阶波段依赖于低阶波段）</li></ul></li></ul></li><li>连接递归波段表示和对抗性学习<ul><li>由于第一阶段信号保真度的限制，无法自然地获得良好的视觉质量</li><li>受最近基于未配对数据集的图像增强方法的启发，在第二阶段，我们重新组合在第一阶段学习得到的波段表示来获取从人类感知的角度来看更好的感知质量的结果</li><li><span class="math inline">\(\hat{x}=\sum_{i=1}^{n}w_{i}\left(y,\left\{\Delta \hat{x}_{s_{1}}^{T}, \Delta\hat{x}_{s_{2}}^{T}, \ldots, \Delta \hat{x}_{s_{n}}^{T}\right\}\right)\Delta \hat{x}_{s_{i}}^{T}(y)\)</span><ul><li>其中 <span class="math inline">\(w_i\)</span> 为权重参数</li><li>其重新组合一张增强图像的波段信号，从几乎无噪声且细节重建良好的图像来得到更优越的光照、对比度和颜色分布的新图像，即从人类视觉的角度来看具有更好的感知质量</li></ul></li></ul></li></ul><h4 id="deep-recursive-band-network">Deep Recursive Band Network</h4><p><img src="/2020/12/03/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-0x07/5.png"></p><p>网络主要分成两个部分</p><ul><li><p>递归波段学习（依赖于配对数据）</p><ul><li>基于低光输入递归恢复正常光图像，充分利用成对数据学习来恢复增强图像的每个波段信号，能很好地恢复细节和抑制噪声</li><li>中间估计是前一个递归的输出，作为下一个递归的引导输入<ul><li>将y与上一次的递归输出 <span class="math inline">\(\hat{x}_{s_{3}}^{t-1}\)</span>的增强结果连接到特征空间中，然后通过几个卷积层（特征的空间分辨率通过步长卷积和反卷积完成下采样和上采样）进行变换</li><li>并且网络中使用到了跳跃了解，从而帮助浅层特征更好地传递到深层部分</li><li>每次BLN在s1= 1/4, s2= 1/2和s3=1三个尺度上产生对应三个不同尺寸的特征，从而提取一系列由粗到细的波段表示，然后合并到增强结果中</li><li>将所有的带估计连接在一起，形成联合估计</li></ul></li><li>用公式直观表示</li></ul><p><span class="math inline">\(\begin{aligned}\left[f_{s_{1}}^{1},f_{s_{2}}^{1}, f_{s_{3}}^{1}\right] &amp;=F_{\mathrm{BLN}_{-} F}^{1}(y)\\ \hat{x}_{s_{1}}^{1} &amp;=F_{\mathrm{R}_{-}s_{1}}^{1}\left(f_{s_{1}}^{1}\right) \\ \hat{x}_{s_{2}}^{1}&amp;=F_{\mathrm{R}_{-}s_{2}}^{1}\left(f_{s_{2}}^{1}\right)+F_{\mathrm{U}}\left(\hat{x}_{s_{1}}^{1}\right)\\ \hat{x}_{s_{3}}^{1} &amp;=F_{\mathrm{R}_{-}s_{3}}^{1}\left(f_{s_{3}}^{1}\right)+F_{\mathrm{U}}\left(\hat{x}_{s_{2}}^{1}\right)\end{aligned}\)</span></p><ul><li>其中 <span class="math inline">\(f_{s_{1}}^{1}, f_{s_{2}}^{1},f_{s_{3}}^{1}\)</span> 分别为y在三个不同尺度下得到的输出</li><li><span class="math inline">\(F_{\mathrm{R}_{-}s_{1}}^{1},F_{\mathrm{R}_{-} s_{2}}^{1},F_{\mathrm{R}_{-}s_{3}}^{1}\)</span> 表示将特征以相应的尺度映射回图像域的处理，<span class="math inline">\(F_{U}(·)\)</span> 表示上采样过程</li><li>首先在最低尺度s1下恢复图像，然后通过两次上采样操作，得到中间输出</li><li>在递归过程中，通过中间输出的指导下，只学习残差特征和图像（即将y和上一次的递归输出拼接作为这一次递归的输入）</li></ul><p><span class="math inline">\(\begin{aligned}\left[f_{s_{1}}^{t},f_{s_{2}}^{t}, f_{s_{3}}^{t}\right] &amp;=F_{\mathrm{BLN}_{-} F}^{t}(y,\hat{x}_{s_{3}}^{t-1}) \\ f_{s_{i}}^{t} &amp;= \Delta f_{s_{i}}^{t} +f_{s_{i}}^{t-1}, i=1,2,3 \\ \hat{x}_{s_{1}}^{t} &amp;=F_{\mathrm{R}_{-}s_{1}}^{t}\left(f_{s_{1}}^{t}\right) \\ \hat{x}_{s_{2}}^{t}&amp;=F_{\mathrm{R}_{-}s_{2}}^{t}\left(f_{s_{2}}^{t}\right)+F_{\mathrm{U}}\left(\hat{x}_{s_{1}}^{t}\right)\\ \hat{x}_{s_{3}}^{t} &amp;=F_{\mathrm{R}_{-}s_{3}}^{t}\left(f_{s_{3}}^{t}\right)+F_{\mathrm{U}}\left(\hat{x}_{s_{2}}^{t}\right)\end{aligned}\)</span></p><ul><li>最后递归T次（作者设为4），重构损失为<ul><li><span class="math inline">\(\begin{aligned}L_{\mathrm{Rect}}=-&amp;\left(\phi\left(\hat{x}_{s_{3}}^{T},x\right)+\lambda_{1} \phi\left(\hat{x}_{s_{2}}^{T}, F_{D}\left(x,s_{2}\right)\right)\right.\\ &amp;\left.+\lambda_{2}\phi\left(\hat{x}_{s_{1}}^{T}, F_{D}\left(x, s_{1}\right)\right)\right)\end{aligned}\)</span><ul><li>其中 <span class="math inline">\(F_D\)</span>为下采样，比例因子为s2</li><li><span class="math inline">\(\phi\)</span> 用来计算SSIM值，<span class="math inline">\(\lambda _1, \lambda _2\)</span> 为权重参数</li></ul></li></ul></li><li>可以将从成对图像中学习到的增强知识与高质量图像的数据先验结合起来</li><li>可以看出，其包含以下几个有点<ul><li>由最后一个递归式推导出的高阶波段将影响该递归式中低阶带的推导<ul><li>因此，低阶带和高阶带之间的连接是双向的</li><li>高阶带也为恢复低阶带提供了有用的指导</li></ul></li><li>递归估计使不同的频带能够学会基于所有波段的先前估计来校正当前自身估计</li><li>递归学习提高了建模能力。后一阶递归只需在前一阶递归估计的指导下，恢复残差信号。因此，可以获得准确的估计，并注意细节</li></ul></li></ul></li><li><p>波段重组（依赖于非配对数据）</p><ul><li><p>信号的保真度并不总是很好地与人类的视觉感知对齐，特别是对于图像的一些全局属性，如光线、颜色分布等</p></li><li><p>让模型通过感知质量引导的对抗性学习，在高质量图像数据集的感知引导下<strong>重新组合</strong>恢复的频带信号</p><ul><li>基于MOS值从美学视觉分析数据集中选择的高质量图像用于表示人类感知的先验知识</li></ul></li><li><p>利用另一个网络对重构过程 <span class="math inline">\(F_{RC}(·)\)</span>进行建模，生成重构频带信号的变换系数（线性地操纵和融合这些波段）</p></li><li><p>公式表示</p><p><span class="math inline">\(\begin{aligned}\left\{w_{1}, w_{2},w_{3}\right\} &amp;=F_{\mathrm{RC}}\left(\left\{\Delta\hat{x}_{s_{1}}^{T}, \Delta \hat{x}_{s_{2}}^{T}, \Delta\hat{x}_{s_{3}}^{T}\right\}\right) \\ \hat{x}_{3}^{F}&amp;=\sum_{i=1}^{3} w_{i} \Delta \hat{x}_{s_{i}}^{T} \\ \Delta\hat{x}_{s_{i}}^{T}&amp;=\hat{x}_{s_{i}}^{T}-F_{\mathrm{U}}\left(\hat{x}_{s_{i-1}}^{T}\right),i=2,3 \\ \Delta \hat{x}_{s_{1}}^{T} &amp;=\hat{x}_{s_{1}}^{T}\end{aligned}\)</span></p></li><li><p>其中 <span class="math inline">\(\hat{x}_{s_{1}}^{F}\)</span>通过三个损失进行训练</p><p><span class="math inline">\(\begin{aligned} L_{\text {Detail }}&amp;=-\phi\left(\hat{x}_{3}^{F}-x\right) \\ L_{\text {Percept }}&amp;=\left\|F_{\mathrm{P}}\left(\hat{x}_{3}^{F}\right)-F_{\mathrm{P}}(x)\right\|_{2}^{2}\\ L_{\text {Quality }} &amp;=-\log D\left(\hat{x}_{3}^{F}\right)\end{aligned}\)</span></p><ul><li>对抗性损失(称为质量损失 <span class="math inline">\(L_{Quality}\)</span>)，用于判断图像是否感知高质量，作为约束条件</li><li>其中D为鉴别器，用于衡量 <span class="math inline">\(\hat{x}_{s_{1}}^{F}\)</span>偏向符合人类视觉的可能性</li><li><span class="math inline">\(F_P\)</span>表示从预先训练好的VGG网络中提取深度特征的过程</li></ul></li><li><p>整体损失函数为</p><ul><li><span class="math inline">\(L_{\mathrm{SBR}}=L_{\mathrm{Percept}}+\lambda_{3}L_{\mathrm{Detail}}+\lambda_{4} L_{\mathrm{Quality}}\)</span></li></ul></li><li><p>从而保证无论从信号保真度还是从人的感知质量来看，总体上都取得了较好的效果</p></li></ul></li><li><p>总结</p><ul><li>首先执行波段表示学习。通过成对数据集的引导，学会了对每个波段信号进行恢复<ul><li>这一阶段确保了信号保真度和细节恢复</li></ul></li><li>利用未配对数据集的感知指导进行波段重组以提高增强图像的视觉质量<ul><li>其中高质量的图像作为人类视觉感知的先验知识</li></ul></li></ul></li></ul><h3 id="experiment-1">Experiment</h3><ul><li>消融实验<ul><li><img src="/2020/12/03/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-0x07/6.png"></li></ul></li><li><img src="/2020/12/03/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-0x07/7.png"></li></ul><h3 id="section-1">😝😜😋</h3><p>这篇论文，刚开始看开头的时候，真的是一脸懵逼，和前面看的几篇弱光增强的论文挺不一样的，感觉波段啊，递归啊啥的好难理解，然后当我看完网络结构部分之后，就感觉真的是，，，原来也就那样，没有那么复杂，不过作者对这个网络中所使用的一些东西的介绍和描述，确实值得学习😋</p><p>其实也挺简单的，就是前面一个普通u-net网络，作者递归了四次，然后学习了多尺度特征，然后充分利用。第二部分，将这些中间输出全部整合起来，通过一个人类感知分值计算来选择比较好的结果，，，</p><p>那个网络结构图和网络介绍部分看一下就行，基本就懂了，前面部分介绍的有点玄乎</p>]]></content>
    
    
    <categories>
      
      <category>study</category>
      
    </categories>
    
    
    <tags>
      
      <tag>图像增强与图像恢复</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>论文阅读-0x06</title>
    <link href="/2020/12/01/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-0x06/"/>
    <url>/2020/12/01/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-0x06/</url>
    
    <content type="html"><![CDATA[<p>论文的阅读笔记：</p><p>《 Zero-Reference Deep Curve Estimation for Low-Light ImageEnhancement 》<a href="https://github.com/Li-Chongyi/Zero-DCE">[code]</a> , CVPR 2020</p><p>《 Learning to Restore Low-Light Images viaDecomposition-and-Enhancement 》，CVPR 2020</p><span id="more"></span><h2 id="zero-reference-deep-curve-estimation-for-low-light-image-enhancement">Zero-ReferenceDeep Curve Estimation for Low-Light Image Enhancement</h2><h3 id="abstract">Abstract</h3><ul><li>本文提出了一个新颖的方法—— 零参考深度曲线估计 (Zero-Reference DeepCurveEstimation，Zero-DCE)，通过深层网络将光增强公式化为特定于图像的曲线估计任务（图像作为输入，曲线作为输出 ）</li><li>训练一个轻量级深度网络DCE-Net来估计像素和高阶曲线，以便对给定图像进行动态范围调整</li><li>Zero-DCE的优点在于它对参考图像的宽松假设，即在训练过程中不需要任何配对或未配对的数据，而是通过一组精心制定的非参考损失函数来实现的，隐式地测量增强质量并驱动网络的学习</li><li>其方法是有效的，因为图像增强可以实现直观和简单的非线性曲线映射</li><li>该方法以一个弱光图像作为输入，生成高阶曲线作为输出。这些曲线然后用于对输入的动态范围进行像素调整，以获得增强的图像</li><li>同时作者发现该曲线是可微的，因此可以通过一个深度卷积神经网络来学习曲线的可调参数</li><li>并且该方法是零参考的，即在训练过程中不需要任何配对甚至是非配对的数据，仅使用CNN和GAN方法，以及一套特别设计的非参考损失函数<ul><li>空间一致性损失、曝光控制损失、色彩稳定性损失和照明平滑损失，所有这些都考虑到光增强的多因素</li></ul></li><li>主要贡献总结如下：<ul><li>提出了第一种不依赖于配对和非配对训练数据的弱光增强网络，避免了过拟合的风险，可以很好地适用于各种照明条件</li><li>设计了一种特定于图像的曲线，它可以迭代地逼近像素和高阶曲线（pixel-wise and higher-order）。这种图像特异性曲线可以有效地在较宽的动态范围内进行映射</li><li>展示了在没有参考图像的情况下，通过任务特异性的非参考损失函数来间接评估增强质量，训练深度图像增强模型的潜力</li></ul></li></ul><p>相关工作</p><ul><li>传统方法<ul><li>HE-based，基于图像直方图分布，在全局和局部对图像的直方图分布进行调整</li><li>基于Retinex 理论的方法，分解一个图像的反射率和光照。反射分量通常被假定在任何光照条件下是一致的</li><li>本文提出的Zero-DCE方法通过图像特定的曲线映射产生增强的结果</li><li>Zero-DCE是一种纯数据驱动的方法，在设计非参考损耗函数时考虑了多个光增强因素，因此具有更好的鲁棒性，更宽的图像动态范围调整，更低的计算量</li></ul></li><li>数据驱动方法<ul><li>大多数基于cnn的解决方案依赖配对数据进行监督训练，因此它们是资源密集型的。通常情况下，配对的数据是通过自动光降解、在数据捕捉期间改变相机的设置、或通过图像润色合成数据来穷尽地收集的</li><li>因此这些基于以上这种配对数据的方法通常是highcost(数据收集)，并且数据集还存在一些虚假/不真实的图片(人工合成)，这样会影响到模型的泛化能力(当模型应用在真实世界的不同光照下，通常会出现色偏等现象)</li><li>基于GAN的无监督方法具有消除配对数据用于训练的优点，不过需要仔细选择未配对的训练数据</li><li>本文提出的Zero-DCE相比于上面的data-driven方法有三个优点</li><li>探索了一种全新的学习策略(<strong>zeroreference</strong>)，消除了对成对和非成对数据的需求</li><li>设置非参考损失函数来对输出图像进行间接的评估</li><li>提出的方法是高效和经济的</li></ul></li></ul><h3 id="methodologydce-net">Methodology——DCE-Net</h3><p><img src="/2020/12/01/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-0x06/1.png"></p><ul><li>DCE-Net：用于估计给定输入图像的一组最佳拟合光增强曲线(LE-curves)</li><li>然后框架迭代应用曲线，对输入图像的RGB通道中所有像素进行映射，从而获得最后的增强图像</li><li>关键组件：LE-curve、DCE-Net和non-reference loss functions</li></ul><h4 id="light-enhancement-curve-le-curve">Light-Enhancement Curve(LE-curve)</h4><ul><li>尝试设计一种曲线，可以自动将弱光图像映射到增强版本，其中自适应曲线参数完全依赖于输入图像<ul><li>增强图像的每个像素值都应在归一化范围内[0,1]，避免溢出截断造成的信息损失</li><li>设计的曲线应该是单调的，从而保留相邻像素间的差异(对比度)</li><li>曲线应该尽可能地简单和可微，使得其在梯度反向传播过程中是可导的</li></ul></li><li>为此设计了一个二次曲线<ul><li><span class="math inline">\(\operatorname{LE}(I(\mathbf{x}) ;\alpha)=I(\mathbf{x})+\alphaI(\mathbf{x})(1-I(\mathbf{x}))\)</span></li><li>x表示像素坐标，<span class="math inline">\(\operatorname{LE}(I(\mathbf{x}) ; \alpha)\)</span>是输入I(x)的增强结果</li><li>α ∈ [−1,1] 是可训练曲线参数，调节LE-curve的大小，控制曝光水平</li><li>每个像素归一化到[0,1]，操作都是在像素上进行，将LE-curve应用到RGB三通道上，从而保留固有色彩，同时避免过拟合</li><li><img src="/2020/12/01/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-0x06/2.png"></li></ul></li><li>可以看出来LE-curve损失符合上述三个目标的。此外，LE-curve能够增大或减小输入图像的动态范围<ul><li>不仅有助于增强弱光区域，也有助于消除过度曝光的伪影</li></ul></li><li>Higher-Order Curve<ul><li>对LE-curve反复使用可以得到其高阶表示形式</li><li><span class="math inline">\(\operatorname{LE_n}(\mathbf{x} )=\operatorname{LE_{n-1}}(\mathbf{x}) +\alpha_n\operatorname{LE_{n-1}}(\mathbf{x})(1-\operatorname{LE_{n-1}}(\mathbf{x}))\)</span><ul><li>n为迭代次数，控制曲率（更大的调节能力，更大的曲率），作者将n的值设为8，可以满足大多数情况下的处理</li></ul></li></ul></li><li>Pixel-Wise Curve<ul><li>高阶曲线可以在比较宽的范围内进行图像调整，由于所有像素都使用了 α，所以仍然相当于进行了全局操作，往往会对局部区域进行过强或过弱的增强</li><li>为此采用逐像素参数的方式来表示出图像的坐标，即给定输入图像的每一个像素都有一个对应的曲线，以最佳拟合的坐标进行动态范围的调整</li><li><span class="math inline">\(L E_{n}(\mathbf{x})=LE_{n-1}(\mathbf{x})+\mathcal{A}_{n}(\mathbf{x}) LE_{n-1}(\mathbf{x})\left(1-L E_{n-1}(\mathbf{x})\right)\)</span><ul><li>其中A为与给定图像大小相同的参数图</li><li>假设一个局部区域的像素具有相同的强度(同样是相同的调整曲线)，这样在输出结果中相邻的像素仍然保持单调的关系</li><li>实验中作者这里使用的迭代次数为8，为了方便理解代码化简一下，大概形式为<span class="math inline">\(x_n = x_{n-1} + A_n*(x^2 - x)\)</span>，然后在这个基础上进行迭代</li></ul></li><li><img src="/2020/12/01/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-0x06/3.png"></li><li>上图中给出了三个通道的估计曲线参数图的例子。从图中可以看出，不同通道的最佳拟合参数映射具有相似的调整趋势，但值不同，说明了低光图像的三个通道之间的相关性和差异性</li></ul></li></ul><h4 id="dce-net">DCE-Net</h4><ul><li>学习输入图像与其最佳拟合曲线参数图之间的映射</li><li>网络结构<ul><li>具有对称连接的七个卷积层。每层由32个大小为3×3的卷积核和stride=1以及ReLU激活</li><li>抛弃了破坏相邻像素关系的下采样和批处理归一化层</li><li>在最后一个卷积层之后是Tanh激活函数，它为<strong>8次迭代(n =8)</strong>生成24个参数图，其中每次迭代需要3个通道的曲线参数图</li></ul></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 代码实现比较简单</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x</span>):<br><br>x1 = self.relu(self.e_conv1(x))<br><span class="hljs-comment"># p1 = self.maxpool(x1)</span><br>x2 = self.relu(self.e_conv2(x1))<br><span class="hljs-comment"># p2 = self.maxpool(x2)</span><br>x3 = self.relu(self.e_conv3(x2))<br><span class="hljs-comment"># p3 = self.maxpool(x3)</span><br>x4 = self.relu(self.e_conv4(x3))<br><br>x5 = self.relu(self.e_conv5(torch.cat([x3,x4],<span class="hljs-number">1</span>)))<br><span class="hljs-comment"># x5 = self.upsample(x5)</span><br>x6 = self.relu(self.e_conv6(torch.cat([x2,x5],<span class="hljs-number">1</span>)))<br><br>x_r = F.tanh(self.e_conv7(torch.cat([x1,x6],<span class="hljs-number">1</span>)))<br><span class="hljs-comment"># 迭代了8次，24个参数层，三个一组，分位8组</span><br>r1,r2,r3,r4,r5,r6,r7,r8 = torch.split(x_r, <span class="hljs-number">3</span>, dim=<span class="hljs-number">1</span>)<br><br><span class="hljs-comment"># 迭代增强部分</span><br>x = x + r1*(torch.<span class="hljs-built_in">pow</span>(x,<span class="hljs-number">2</span>)-x)<br>x = x + r2*(torch.<span class="hljs-built_in">pow</span>(x,<span class="hljs-number">2</span>)-x)<br>x = x + r3*(torch.<span class="hljs-built_in">pow</span>(x,<span class="hljs-number">2</span>)-x)<br>    <span class="hljs-comment"># 取了一个中间的增强结果</span><br>enhance_image_1 = x + r4*(torch.<span class="hljs-built_in">pow</span>(x,<span class="hljs-number">2</span>)-x)<br>x = enhance_image_1 + r5*(torch.<span class="hljs-built_in">pow</span>(enhance_image_1,<span class="hljs-number">2</span>)-enhance_image_1)<br>x = x + r6*(torch.<span class="hljs-built_in">pow</span>(x,<span class="hljs-number">2</span>)-x)<br>x = x + r7*(torch.<span class="hljs-built_in">pow</span>(x,<span class="hljs-number">2</span>)-x)<br>enhance_image = x + r8*(torch.<span class="hljs-built_in">pow</span>(x,<span class="hljs-number">2</span>)-x)<br>r = torch.cat([r1,r2,r3,r4,r5,r6,r7,r8],<span class="hljs-number">1</span>)<br><span class="hljs-keyword">return</span> enhance_image_1,enhance_image,r<br></code></pre></td></tr></table></figure><h4 id="non-reference-loss-functions">Non-Reference Loss Functions</h4><ul><li>为了在DCE-Net中实现零参考学习，提出了一组可区分的非参考损失，可以用来评估增强图像的质量</li><li>Spatial Consistency Loss（空间一致性损失）<ul><li>通过保持输入图像与增强图像相邻区域的差异（对比度）来促进增强图像的空间一致性</li><li><span class="math inline">\(L_{s p a}=\frac{1}{K} \sum_{i=1}^{K}\sum_{j \in\Omega(i)}\left(\left|\left(Y_{i}-Y_{j}\right)\right|-\left|\left(I_{i}-I_{j}\right)\right|\right)^{2}\)</span><ul><li>K为局部区域的数量，Ω(i)是以i为中心的四个相邻区域（上下左右），Y和I分别为增强图像和输入图像的局部区域平均强度值。<strong>这个局部区域的Size经验性地设置为4x4</strong></li></ul></li></ul></li><li>Exposure Control Loss（曝光控制损失）<ul><li>为了抑制曝光不足/过度区域</li><li>衡量局部区域的平均强度值到良好曝光水平E之间的距离。遵循现有的做法，将E设为RGB颜色空间的灰度，在本文中作者将其设为0.6</li><li><span class="math inline">\(L_{e x p}=\frac{1}{M}  \sum_{k = 1}^M\left(\left|\left(Y_{k}-E\right)\right|\right)\)</span><ul><li>M为大小为16×16的不重叠局部区域的个数，Y为增强图像中某个局部区域的平均强度值</li></ul></li></ul></li><li>Color Constancy Loss（色彩恒等损失）<ul><li>根据灰度世界的颜色恒等假设<ul><li>即每个传感器通道的颜色在整个图像上平均为灰度</li></ul></li><li>用于校正增强图像中可能出现的颜色偏差，并建立了三个调整通道之间的关系</li><li><span class="math inline">\(L_{c o l}=\sum_{\forall(p, q) \in\varepsilon}\left(J^{p}-J^{q}\right)^{2}, \varepsilon=\{(R, G),(R,B),(G, B)\}\)</span><ul><li>式中<span class="math inline">\(j^p\)</span>表示增强图像通道p的平均强度值，(p,q)表示一对通道</li></ul></li></ul></li><li>Illumination Smoothness Loss（光照平滑度损失）<ul><li>为了保持相邻像素间的单调关系</li><li><span class="math inline">\(L_{t v_{\mathcal{A}}}=\frac{1}{N}\sum_{n=1}^{N} \sum_{c \in \xi}\left(\left|\nabla_{x}\mathcal{A}_{n}^{c}\right|+\nabla_{y} \mathcal{A}_{n}^{c}\mid\right)^{2}, \xi=\{R, G, B\}\)</span><ul><li>N为迭代次数，<span class="math inline">\(\nabla_{x},\nabla_{y}\)</span>分别代表水平和垂直方向的梯度操作</li></ul></li></ul></li><li>Total Loss<ul><li><span class="math inline">\(L_{total} = L_{spa} + L_{exp} +W_{col}L_{col} + W_{t v_{\mathcal{A}}}L_{tv_{\mathcal{A}}}\)</span></li><li>不过源码中曝光控制损失前面也是有个权重参数</li></ul></li><li>代码实现有点多，不过基本都是上面这些公式的具体实现</li></ul><h3 id="experiment">Experiment</h3><ul><li>batch size= 8，2080Ti，使用(0,0.02)高斯函数初始化权重，bias初始为常量，使用ADAM优化器(lr=1e-4)，<span class="math inline">\(W_{col}\)</span> 为0.5，<span class="math inline">\(W_{t v_{\mathcal{A}}}\)</span>为20，从而平衡loss间尺度差距</li><li>消融实验</li><li><img src="/2020/12/01/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-0x06/4.png"></li><li><img src="/2020/12/01/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-0x06/5.png"><ul><li>L-F-N代表Zero-DCE有L层卷积，每层有F个feature map以及迭代次数为N</li></ul></li><li><img src="/2020/12/01/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-0x06/6.png"><ul><li>使用不同数据集对Zero-DCE进行训练</li><li>原训练集中(2422)的900张low-light图像Zero-DCELow</li><li>DARK FACE中9000张未标注的low-light图像Zero-DCELargeL</li><li>SICE数据集Part 1 andPart2组合的4800张多重曝光图像Zero-DCELargeLH</li><li>在去除过度曝光的训练数据后，使用更多的低光图像，Zero-DCE仍倾向于过度增强光照良好的区域(如人脸)。这些结果表明了在我们的网络训练过程中使用多暴露训练数据的合理性和必要性。此外，当使用更多的多曝光训练数据时，ZeroDCE可以更好地恢复暗区，如图6(e)所示</li></ul></li><li>Benchmark Evaluations（基准评价）<ul><li>在多个数据集(NPE LIME MEF DICMVV以及SICE的Part2)上与目前SOAT的方法进行了对比</li><li><img src="/2020/12/01/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-0x06/7.png"></li><li>在实验数据上面，基本上就是又快又好</li><li><img src="/2020/12/01/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-0x06/1.jpg"><ul><li>用户研究(US)↑/感知指数(PI)↓在图像集(NPE、LIME、MEF、DICM、VV)上得分，US得分越高，说明人主观视觉质量越好，PI值越低，说明感知质量越好。在每种情况下，最好的结果是红色的，而第二好的结果是蓝色的。</li></ul></li><li><img src="/2020/12/01/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-0x06/8.png"></li></ul></li></ul><h3 id="section">😝😜😋</h3><p>以后想着看一篇论文多少都写一点自己的感受，不然感觉没啥吸收内容。</p><p>这篇论文感觉比较精彩的地方就是图像增强的实现方式以及那几个损失函数的组合使用，这样就能达到一种无参考的函数优化，感觉特别新颖，很有创新型。网络结构也不是特别复杂，但是能想到这种方式真的很强，好好记录一下，以后说不定会翻出来再看看。</p><h2 id="learning-to-restore-low-light-images-via-decomposition-and-enhancement">Learningto Restore Low-Light Images via Decomposition-and-Enhancement</h2><h3 id="abstract-1">Abstract</h3><ul><li>弱光图像（Low-light images）存在两个问题<ul><li>能见度低(即像素值小)</li><li>由于低信噪比，噪声十分明显，干扰了图像内容</li></ul></li><li>不过大多数现有的低光图像增强方法都是从可忽略噪声的数据集学习的（像上面那篇）</li><li>这些方法在增强微光图像并去除其噪声是存在问题的，我们观察到噪声在不同的频率层表现出不同的对比度，并且<strong>在低频层比在高频层更容易检测到噪声</strong></li><li>在此基础上提出了一种新的网络，该网络首先在低频层学习恢复目标图像，然后基于恢复的目标图像增加高频细节</li><li>作者还准备了一个新的具有真实噪声的弱光图像数据集</li><li>由于量化的原因，在标准RGB (sRGB,24位/像素)空间中，弱光图像的可见度与人的感知不匹配</li><li>本文解决了弱光sRGB图像增强问题<ul><li>涉及两个问题:图像增强和去噪</li><li>动机基于两个观察<ul><li>图像低频层保存了更多的信息，如物体和颜色，与比图像高频层(图1(d))相比，受噪声的影响较小(图1(c))，这说明对低频图像层进行增强要比直接对整个图像进行增强容易</li><li>图像基元的固有维数非常低，使得神经网络有可能学习图像基元的全部知识</li></ul></li></ul></li><li><img src="/2020/12/01/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-0x06/9.png"><ul><li>在给定24位色深（a）的低光照sRGB图像的情况下，典型的增强方法（这里使用的Hist就是直方图均化）无法产生令人愉悦的图像，并且细节得以恢复并且噪声得到抑制（b，e，f）。为了说明我们的想法，我们应用高斯滤波器将（b）分解为低频层（c）和高频层（d），并观察到低频层保留了足够的信息以恢复对象和颜色，可以用于增强高频细节。这激发我们学习弱光图像的分解和增强方法（h）</li></ul></li><li>给定一个图像基元的低频信息，通过推测相应的高频信息，网络可以重构出整个基元。有了这样的先验，可以从恢复的低频层中学习增强高频细节</li><li>为此作者提出一种新的神经网络，利用 关注上下文编码(Attention toContext Encoding， ACE)模块<ul><li>第一阶段自适应地选择低频信息用于恢复低频层和去除噪声</li><li>第二阶段自适应地选择高频信息用于细节增强</li><li>还提出了一个跨域变换(Cross Domain Transformation，CDT)模块，以利用多尺度基于频率的特征，在两个阶段进行噪声抑制和细节增强</li></ul></li><li>作者归纳的三个贡献点<ul><li>提出了一种新的基于频率的分解和增强模型来增强弱光图像<ul><li>在抑制噪声的同时恢复低频层的图像内容，然后恢复高频图像细节</li></ul></li><li>提出了一种网络，其中包含一个关注上下文编码(ACE)模块，用于分解输入图像以自适应增强高频/低频层，以及一个跨域转换(CDT)模块，用于噪声抑制和细节增强</li><li>准备了一个具有真实噪声的低光图像数据集和相应的groundtruth图像，以方便学习过程</li></ul></li><li>低光增强的传统方法<ul><li>上面那篇论文介绍了一些，就稍微在记录一下</li><li>直方图均化、伽马校正</li><li>基于深度学习去学习一个映射函数；基于retinx的图像增强，将输入图像分解为光照和反射率，然后增强图像</li><li>不过作者认为这些方法都是针对低噪声的低光图像的，不能增强噪声低光图像</li></ul></li><li>然而，简单地通过使用现有的去噪方法进行 前/后处理来去除低光图像中的噪声是有一定复杂度的<ul><li>低像素值使得难以在增强弱光图像之前提供用于检测/去除噪声的足够的上下文信息</li><li>在应用现有的增强方法之后，噪声会被不可预测地放大，从而产生仍然具有低SNR的图像，因此难以进一步去噪</li><li>为了解决这个限制，作者在本文中提出学习一种深度增强模型，以端到端的递归方式增强弱光图像同时去除噪声</li></ul></li></ul><h3 id="net-model">Net Model</h3><p><img src="/2020/12/01/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-0x06/10.png"></p><ul><li>作者提出的方法受到两个观察结果的启发<ul><li>与直接增强整个图像相比，增强噪声低光图像的低频层更容易<ul><li>低频层的噪声更容易检测和抑制。通过分析图像低频层的全局属性，可以正确地估计图像的光照/颜色</li></ul></li><li>自然图像中最原始的部分，例如边缘和角落，具有非常低的固有维数<ul><li>如此低的维数意味着少量的图像示例就足以很好地表示图像原语。因此，给定基元的低频信息，可以推断出相应的高频信息</li></ul></li></ul></li><li>作者提出的模型有两个主要阶段<ul><li>低频图像的增强：提出学习低频<strong>图像增强函数</strong>C(·)，然后学习用于<strong>颜色恢复放大函数</strong>A(·)<ul><li>通过联合建模从C(·)到A(·)的映射，网络不必同时学习全局信息(如光照)和局部信息(如颜色)，增强效果更好</li><li>给定一个弱光sRGB图像I，第一阶段的增强可以写成</li><li><span class="math inline">\(I^{a}=\alpha A(C(I)) \cdot C(I)\)</span><ul><li>其中<span class="math inline">\(I^{a}\)</span> 为放大的低频层</li><li>这里的A与基于retinex方法中的光照图（illumination map）不同，其中α是一个全局增强系数，通过C获得的</li><li>即αA(·)可以被解释为自注意方式增强C的误差图</li></ul></li></ul></li><li>高频图像的增强：于<span class="math inline">\(I^{a}\)</span>学习高频细节增强函数D(·)，而不是直接从有噪声的原始输入图像 I恢复高频细节。然后对D(·)进行残差建模<ul><li><span class="math inline">\(I^{c}= I^{a} + D(I^{a})\)</span></li></ul></li><li>各阶段的可视化图<ul><li><img src="/2020/12/01/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-0x06/11.png"></li></ul></li></ul></li></ul><h4 id="ace-module">ACE Module</h4><p><img src="/2020/12/01/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-0x06/12.png"></p><ul><li>Attention to Context Encoding注意上下文编码ACE</li><li>目标是学习用于图像分解的频率感知特征，为此作者扩展了最初用于编码长程关系的非局部运算（non-local）来选择频率自适应上下文信息</li><li>给定一个输入<span class="math inline">\(x_{i n} \in R^{H \times W\times C}\)</span>，首先使用两组扩张卷积（卷积核大小/扩张率为1/1和3/2），分别记为 <span class="math inline">\(f_{d1}\)</span> 和 <span class="math inline">\(f_{d2}\)</span>，用于提取不同接受域的特征，然后计算这两个特征之间的对比感知注意映射<span class="math inline">\(C_a\)</span><ul><li><span class="math inline">\(C_a = sigmoid (f_{d1}(x_{in}) -f_{d2}(x_{in}))\)</span></li><li><span class="math inline">\(C_a\)</span>表示像素级的相对对比度信息，其中高对比度的像素被认为属于高频层，取反的话就是得到低对比度部分，即低频信息</li><li>然后计算其逆映射<span class="math inline">\(\overline{C_a} = 1 -C_a\)</span> ，从<span class="math inline">\(x_{in}\)</span>中选择特征来表示低频内容<ul><li><span class="math inline">\(x_c = \overline{C_a} \cdotx_{in}\)</span></li></ul></li><li>接着通过最大值池化来进一步缩小所选择的特征<span class="math inline">\(x_{c}\)</span>从而得到 <span class="math inline">\(x_{c}^{\downarrow}\)</span>，从而减少计算量和分部局的像素依赖</li><li>对于给定 <span class="math inline">\(x_{c}^{\downarrow} \inR^{H&#39; \times W&#39; \times C&#39;}\)</span> ，non-local上下文编码为<ul><li><span class="math inline">\(x_{c}^{r}=g\left(x_{c}^{\downarrow}\right)^{\top}\times h\left(x_{c}^{\downarrow}\right) \timesf\left(x_{c}^{\downarrow}\right)^{\top}\)</span><ul><li>其中g，h，f为一组运算（卷积，重塑和矩阵转置）</li><li>首先计算像素之间的关系表 <span class="math inline">\(M \inR^{H&#39;  W&#39; \times H&#39;  W&#39;}\)</span>，然后考虑每个像素与其他所有像素的关系，计算非局部增强特征 <span class="math inline">\(x_c^r\)</span></li></ul></li></ul></li><li>最后我们便得到了频率感知的非局部增强特征<span class="math inline">\(x_{out}\)</span><ul><li><span class="math inline">\(x_{out} = Unpool(x_c^R) +x_c^r\)</span><ul><li>使用残差结构</li></ul></li></ul></li></ul></li><li><img src="/2020/12/01/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-0x06/15.png"></li><li>在整体网络框架中的两个ACE共享权重，不过第二个ACE模块中使用对比度感知映射算法（不是反向感知注意映射算法，即使用高频信息）从代表高频层的特征中学习图像细节</li><li><img src="/2020/12/01/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-0x06/13.png"></li></ul><h4 id="cdt-module">CDT module</h4><ul><li><img src="/2020/12/01/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-0x06/14.png"></li><li>了解低光图像的全局属性有助于恢复光照和图像内容，为此设计了CDT模块，在缩小弱光域和增强域中的特征之间的差距的同时，增加感受野<ul><li>在噪声弱光域提取的频率感知特征与在增强域提取的频率感知特征之间的差异</li></ul></li><li>在第一阶段，首先通过自推导的反对比度感知映射<span class="math inline">\(\overline{C_a}\)</span> 对编码器 <span class="math inline">\(x_{en}\)</span>的噪声特征进行空间重加权，以滤除高对比度信息，然后 <span class="math inline">\(x_{en}\)</span> 与相应解码器的特征 <span class="math inline">\(x_{de}\)</span> 进行拼接</li><li>然后从拼接后的<span class="math inline">\([ x_{en}, x_{de}]\)</span> 中计算全局缩放向量 v，以通道方式自适应地缩放来自不同域的特征</li><li>使用v与拼接后的结果进行相乘得到最后的 <span class="math inline">\(x_{out}\)</span></li><li>在第二阶段，使用对比度感知的注意映射算法（高频）来学习图像细节，而不是使用反映射算法（低频），类似于ACE模块</li></ul><h4 id="proposed-dataset">Proposed Dataset</h4><ul><li>为了便于学习所提出的模型，作者准备了一个新的包含噪声信息的低光数据集：低光图像和对应的groundtruth sRGB图像对</li><li>基于SID数据集准备训练数据，该数据集由原始数据和groundtruth图像对组成，其中ground truth的拍摄时间较长，所以噪声可忽略不计<ul><li>不过线性相机原始数据与非线性sRGB数据有显著的不同，特别是在噪声和图像强度方面</li><li>为此作者考虑了图像生成管道中的几个关键步骤(即曝光补偿Exposurecompensation、白平衡Whitebalance和去线性化De-linearization)，并对它们的操作进行了操作，以模拟来自不同相机的真实噪声低光sRGB图像</li></ul></li></ul><h4 id="training">Training</h4><ul><li>在两阶段的训练过程中，使用L2损失来测量重建的准确性<ul><li>在第一阶段,鼓励网络关注预测输入图像的低频分量，准备相应的groundtruth，用<span class="math inline">\(I_{f}^{gt}\)</span>表示，通过使用引导滤波器过滤高频细节,保持groundtruth图像的主要结构和内容</li><li><span class="math inline">\(L_{a c c}=\lambda_{1}\left\|C-I_{f}^{gt}\right\|_{2}+\lambda_{2}\left\|I^{c}-I^{g t}\right\|_{2}\)</span><ul><li><span class="math inline">\(C,I^c,I_f^{gt},I^{gt}\)</span>分别为重构图像内容、恢复后的图像、低频层的ground truth、恢复图像的groundtruth</li><li>λ1和λ2是平衡参数</li><li>网络结构图中的两个红色箭头（supervision，监督）</li><li>让低频图像和恢复图都接近ground truth</li></ul></li></ul></li><li>通过使用L1损失比较<span class="math inline">\(I^c andI^{gt}\)</span>的VGG特征距离来合并感知损失<ul><li><span class="math inline">\(L_{vgg}=\lambda_{3}\left\| \Phi (I^c) -\Phi (I^{g t}) \right\|_{2}\)</span><ul><li><span class="math inline">\(\Phi\)</span> 为vgg网络</li></ul></li></ul></li></ul><h3 id="section-1">😝😜😋</h3><p>emm没啥写的，不写了</p><ul><li>作者观察到，在不同的频率层中，噪声对图像的影响是不同的。基于此提出了一种新的基于频率的图像分解增强模型，在抑制噪声的同时，自适应增强不同频率层的图像内容和细节</li><li>提出了一个网络，该网络包含了关注上下文编码(ACE)模块，用于自适应增强高频和低频层，以及跨域转换(CDT)模块，用于噪声抑制和细节增强</li><li>准备了一个新的弱光图像数据集</li></ul>]]></content>
    
    
    <categories>
      
      <category>study</category>
      
    </categories>
    
    
    <tags>
      
      <tag>图像增强与图像恢复</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>论文阅读-0x05</title>
    <link href="/2020/11/21/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-0x05/"/>
    <url>/2020/11/21/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-0x05/</url>
    
    <content type="html"><![CDATA[<p>论文的阅读笔记（Wenqi Ren 任文琦老师）：</p><p>《Single image rain removal via a deep decomposition–compositionnetwork》<a href="https://drive.google.com/file/d/1TPu9RX7Q9dAAn5M1ECNbqtRDa9c6_WOt/view">[code]</a>, CVIU 2019</p><p>《Single Image Deraining: A Comprehensive Benchmark Analysis》<a href="https://github.com/lsy17096535/Single-Image-Deraining">[project]</a>，CVPR2019</p><span id="more"></span><h2 id="single-image-rain-removal-via-a-deep-decompositioncomposition-network">Singleimage rain removal via a deep decomposition–composition network</h2><h3 id="abstract">Abstract</h3><ul><li>设计了一种新的端到端多任务学习结构，减少了输入到输出的映射范围，提高了性能</li><li>构建一个分解网，将雨图像分割为干净的背景和雨层</li><li>模型除了包含一个表示所需的干净图像的组件外，还包含一个用于雨层的额外组件</li><li>在训练阶段，进一步采用合成结构将分离出来的干净图像和雨水信息重新生成输入，以提高分解质量</li><li>大气中的雨一般有两种存在方式，即稳定雨和动态雨<ul><li>持续的降雨是由远处遍布整个场景的微小雨滴造成的，这在视觉上类似于背景中的雾/雾/薄雾</li><li>动态的则来自于大颗粒(雨纹)，这些颗粒看起来像随机的和局部的腐败</li><li><img src="/2020/11/21/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-0x05/1.png"></li><li>从形式上看，雨天图像可以看作是两个层的叠加<ul><li><span class="math inline">\(O = \psi(B,P)\)</span></li><li>其中<span class="math inline">\(O\in  \mathbb{R} ^{m\times  n}\)</span>表示观测数据，<span class="math inline">\(R\in  \mathbb{R} ^{m \times  n}\)</span> 和 <span class="math inline">\(B\in \mathbb{R} ^{m \times  n}\)</span>分别表示雨层和期望的干净背景</li><li><span class="math inline">\(\psi\)</span>是一个混合函数</li></ul></li><li>从数学上讲，将一幅图像分解成两层是一个严重的不适定问题(NP难度)，因为需要恢复的未知量是给定测量值的两倍</li></ul></li><li>在自然干净的背景和/或雨层上使用各种先验。采用多幅图像或视频序列来提供丰富的时间信息是一种常用的方案，以降低背景场景恢复的难度<ul><li>这些基于多帧的方法在实践中不太吸引人，因为帧需要以一种良好的控制方式捕获，限制了它们的灵活性和适用性</li><li>输入多幅图像，它们的时间信息可能没有很好的对齐，因此不可靠</li><li>由于可用信息较少，基于单帧的方法更具挑战性，需要额外的先验来规范解决方案</li></ul></li><li>虽然基于深度学习的策略在除雨方面比传统方法有了进一步的进步，但仍存在两个挑战<ul><li>如何提高深层架构的有效性，更好地利用训练数据，获得更准确的恢复结果</li><li>如何提高测试图像的处理效率，以满足真实(实时)任务的高速要求</li></ul></li><li>本文提出了一种新的深度分解合成网络(DDC-Net)，可以有效地去除单一图像在不同条件下的降雨效应<ul><li>所设计的网络由分解网和合成网组成</li><li>建立分解网，将雨天图像分割为干净的背景和雨点层</li><li>模型体积保持小，性能良好，因此，架构的有效性得到了提高</li><li>合成网是将分解网中分离出的两层重新生成输入的雨图像，目的是进一步提高分解的质量</li><li>与以前的深层模型不同，该模型明确地考虑了雨层的恢复精度</li></ul></li><li>从传递性的角度来看，联合考虑分解和合成与CycleGAN提出的循环一致性概念相似</li><li>从技术角度来看，我们的DDC-Net与CycleGAN有很大的不同<ul><li>样式转换vs层分离</li><li>CycleGAN需要一对一的映射输入转换到结果(在训练过程中,没有真是样本)和另一个一对一的逆向映射(包含真实样本)；而DDC-Net从输入层执行一到二的分解，然后接上一个从二到一的合成层，在两个映射方向上都有真实样本信息</li></ul></li><li>根据场景混合模式，作者不是简单的加性混合，而是合成一个包含10400个三部分(rainimage, clean background, raininformation)的训练数据集。所以在测试阶段，只需要分解网部分</li><li>基于多帧图像的去雨方法<ul><li>附加了一个约束来提高降雨条纹探测的准确性;约束表明，不同RGB通道的雨条纹强度变化是高度相关的</li><li>一般来说，当给定的信息具有足够的冗余性时，这种方法可以提供合理的结果</li><li>但在实际应用中经常违反这一条件。例如，一个视频序列被一个(快速或突然)移动的摄像机捕获，这使得时间对准不准确。有时雨图像是由静止相机或来自互联网，甚至没有参考。</li></ul></li><li>基于单帧图像的去雨方法<ul><li>将输入的雨图像分成包含结构的低频分量和包含雨条纹和背景纹理的高频分量。然后根据构建的字典将图像纹理与细节层中的雨纹区分开来，并将其添加回结构层</li><li>一方面，这些基于先验的方法即使有训练有素的词典/GMMs的帮助，仍然无法捕捉到背景和雨层充分明显的特征</li><li>另一方面，它们的计算成本对于实际应用来说太大了</li></ul></li></ul><blockquote><p>As for deraining, Fu et al. proposed a deep detail network (DDN) (Fuet al., 2017), inspired by Kang et al. (2012). It first decomposes arain image into a detail layer and a structure layer. Then the networkfocuses on the high-frequency layer to learn the residual map of rainstreaks. The restored result is formed by adding the extracted detailsback to the structure layer. Yang et al. (2017a) proposed aconvolutional neural network (CNN) based method to jointly detect andremove rain streaks from a single image (JORDER). They used amulti-stream network to capture the rain streak component with differentscales and shapes. The rain information is then fed into the network tofurther learn rain streak intensity. By recurrently doing so, the raineffect can be detected and removed from input images. The work in Zhanget al. (2017a) proposes a single image de-raining method called imagederaining conditional gen- eral adversarial network (ID-CGAN), whichconcerns quantitative, visual and also discriminative issues into theobjective function. In Zhang and Patel (2018), a density-awaremultistream densely connected convolu- tional neural network (DID-MDN)was designed for joint rain density estimation and image deraining. Panet al. proposed a general dual convolutional neural network (DualCNN)for low-level vision problems like deraining and superresolution (Pan etal., 2018). Despite the progress made by deep models, the effectivenessand efficiency of deep architectures still need to be furtherimproved.</p></blockquote><ul><li>基于单帧图像的除雾方法<ul><li>持续降雨导致类似于雾霾的视觉现象，可以通过除雾技术缓解</li><li>大多数现有的这类方法都是通过估计传输转换图来恢复无雾图像</li></ul></li></ul><h3 id="deep-decompositioncomposition-networkddc-net">Deepdecomposition–composition network:DDC-Net</h3><p><img src="/2020/11/21/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-0x05/2.png"></p><h4 id="decomposition-net">decomposition net</h4><ul><li>将雨图像分割为干净的背景和雨层的分解网络主要有两个分支:一个是恢复背景，另一个是恢复雨信息</li><li>基于残差 encoder和decoder架构构建分解分支<ul><li>前两个卷积层采用扩容卷积来扩大接收域<ul><li>stride=1x1, max-pooling</li></ul></li><li>使用两种解码器网络(干净背景分支和辅助雨分支)分别恢复干净背景和雨层</li><li>来自干净背景分支的反卷积模块将连接到辅助降雨分支，以便在上采样阶段（向下箭头）更好地获取降雨信息<ul><li>背景特性希望能够帮助从雨的部分中排除属于背景的纹理</li><li>作者也尝试了将雨水噪声的特性添加到背景分支中，但是并没有明显的性能改进。原因是由于雨的纹理通常比背景纹理更简单和规则</li></ul></li></ul></li><li>对合成图像进行预训练<ul><li>由于是合成图像，所以能直接得到原始干净背景图像和雨线信息以及最后合成的雨图，构成一个图像三元组，从而直接对干净背景分支和雨分支进行训练更新</li><li>为了得到恢复后的图像与原始图像之间的差异，计算结果与目标图像之间的欧氏距离，因此在预训练阶段中，干净背景图像和雨层的损失为：</li><li><span class="math inline">\(\mathcal{L}_{\mathrm{B}}=\frac{1}{N}\sum_{i=1}^{N}\left\|f_{b}\left(\mathbf{O}^{i}\right)-\mathbf{B}^{i}\right\|_{F}^{2}\)</span></li><li><span class="math inline">\(\mathcal{L}_{\mathrm{R}}=\frac{1}{N}\sum_{i=1}^{N}\left\|f_{r}\left(\mathbf{O}^{i}\right)-\mathbf{R}^{i}\right\|_{F}^{2}\)</span><ul><li>N表示图像数量，<span class="math inline">\(\left\|·\right\|_{F}^{2}\)</span>为Frobenius范数（一种矩阵范数），<span class="math inline">\(\mathbf{O}^{i}, \mathbf{B}^{i}\)</span> 和 <span class="math inline">\(\mathbf{R}^{i}\)</span>表示第i个输入、背景图像和雨图，其中 <span class="math inline">\(f_b\)</span> 和 <span class="math inline">\(f_r\)</span> 表示干净背景分支和辅助雨分支</li></ul></li></ul></li><li>在真实图像上进行微调<ul><li>由于合成的雨层在真实场景中无法区分衰减和飞溅的效果，所以使用从Flicker和谷歌中收集的真实无雨和有雨图像来对我们的模型进行微调</li><li>在这个阶段，不再使用图像三元组(对于真实的图像也没有这样的三元组，即没有雨线信息)，只需要雨天的图像和干净的图像。所以该阶段作者只是利用分解网络生成复原的图像，从而欺骗鉴别器，引入生成式对抗网络GAN损失<ul><li>该阶段不再对雨层分支进行训练更新</li><li>作者希望对抗模型能够帮助训练分解网络，分解网络生成增强的图像，该图像可以欺骗鉴别器D，将该增强图像识别为真实图像</li></ul></li><li>对抗损失：</li><li><span class="math inline">\(\mathcal{L}_{\mathrm{ADV}}=\underset{\mathbf{I}\sim p(I)}{\mathbb{E}}[\log D(\mathbf{I})]+\underset{\mathbf{O} \simp(\mathcal{O})}{\mathbb{E}}[\log (1-D(G(\mathbf{O})))]\)</span></li><li>鉴别器D：</li><li><img src="/2020/11/21/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-0x05/3.png"></li></ul></li></ul><h4 id="composition-net">composition net</h4><ul><li>其目的是从分解模型的输出中学习到原始的雨图像，然后利用构建的雨图像作为自监督信息来指导反向传播</li><li>利用分解模型，可以将一个雨图像分解为两个相应的组件<ul><li>第一个组件是从干净背景分支中恢复无雨图像B</li><li>第二个是雨层R，由辅助雨分支学习</li></ul></li><li>因此，可以通过简单的方式直接合成对应的雨图像O<ul><li><span class="math inline">\(O = B + R\)</span></li><li>不过该公式是有限的，因为还有其他因素涉及到雨图像，如雾霾和飞溅</li></ul></li><li>为了缓解这个问题，将干净的背景图像和分解网络中的雨层拼接起来，然后使用一个额外的CNN块来建模真实的雨天图像</li><li>这样所提出的合成网络可以实现更一般的合成过程，并能解释真实图像中的一些未知现象</li><li>然后定义了一个二次训练代价函数来度量重构后的雨图像与原始雨图像的差值</li><li><span class="math inline">\(\mathcal{L}_{\mathrm{O}}=\frac{1}{N}\sum_{i=1}^{N}\left\|f\left(\mathbf{O}\right)-\mathbf{O}\right\|_{F}^{2}\)</span><ul><li>这里f表示整个网络</li></ul></li><li>所以在最终的在测试阶段，只需要干净的背景分支来生成所需的结果</li></ul><h4 id="training-dataset">training dataset</h4><ul><li>作者提倡使用场景混合模式来合成数据，以更好地近似真实情况</li><li><span class="math inline">\(\begin{aligned} \mathbf{O}&amp;=\mathbf{1}-(\mathbf{1}-\mathbf{B}) \circ(\mathbf{1}-\mathbf{R}) \\&amp;=\mathbf{B}+\mathbf{R}-\mathbf{B} \circ \mathbf{R}\end{aligned}\)</span></li><li>其中<span class="math inline">\(\circ\)</span> 表示哈达玛积(一种矩阵运算)，两个图层中的像素值被倒转、相乘，然后再倒转</li><li>干净的背景图像B来自BSD300数据集。降雨部分R是按照不同强度、不同方向和不同重叠的步骤生成的。最后以场景融合的方式将两部分融合。这样合成的雨天图像更有物理意义</li><li>可以得到了一个包含10400个三元组数据集(雨图像，干净的背景，雨层)。在微调阶段，我们只采集15幅真实图像，随机裁剪240个样本，尺寸为224×224</li></ul><h3 id="experiment">Experiment</h3><p><img src="/2020/11/21/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-0x05/4.png"></p><ul><li>本文提出了一种新的用于单幅图像去雨的深度结构，即深度分解-合成网络，它由分解网络和合成网络两个主要子网络组成</li><li>建立分解网络，将雨水图像分割为干净的背景和雨水层</li><li>与以前的架构不同，我们的分解模型除了包含一个表示所需的干净图像的组件外，还包含一个用于雨层的额外组件</li><li>在训练阶段，附加的合成结构用于通过分离的干净图像和雨信息来再现输入，以进一步提高分解质量</li><li>此外，通过合成数据得到的预训练模型通过无配对监督进一步微调，以更好地适应实际情况</li><li>在合成和真实图像上的实验结果显示了我们的设计的有效性，并证明了与其他先进的方法相比，它的明显优势。在运行时间方面，我们的方法明显快于其他的技术，这可以扩大对高速要求的去雨任务的适用性</li></ul><h2 id="single-image-deraining-a-comprehensive-benchmark-analysis">SingleImage Deraining: A Comprehensive Benchmark Analysis</h2><h3 id="abstract-1">Abstract</h3><ul><li>目前提出的很多去雨算法，都是使用特定类型的合成图像，假设一个特定的降雨模型，加上一些真实图像进行评估</li><li>目前还不清楚这些算法将如何在“野外”获取的雨天图像上执行，以及我如何衡量该领域的进展</li><li>本文提出了一个综合研究和评价现有的单一图像去雨算法，使用一个新的大规模基准，包括合成和真实世界的各种类型的雨图像</li><li>这个数据集强调不同降雨模型(雨条纹，雨滴，雨和薄雾),以及丰富多样的评估标准(完全和无参考的客观，主观和特定任务)</li><li>我们的评估和分析表明合成多雨的图像和真实图像之间的性能差距,使我们能够更好地确定每种方法的优势和局限性以及未来的研究方向</li><li>近年来，在单一图像去雨方面取得了重大进展。主要归因于各种自然图像先验和基于深度卷积神经网络(CNN)的模型</li><li>雨图像公式化模型<ul><li>降雨是一个复杂的大气过程，由于雨滴大小、降雨密度和风速等环境因素的影响，降雨会导致多种不同类型的能见度下降</li><li>当拍摄雨天图像时，雨水在数字图像上的视觉效果进一步取决于许多相机参数，如曝光时间、景深和分辨率等</li><li>大多数现有的去雨任务会假设一个降雨模型(通常是降雨条纹)，这可能会过度简化问题</li><li>作者将文献中已有的降雨模型分为三大类:条纹雨、雨滴、雨和雾</li><li>将一幅雨条纹图像<span class="math inline">\(R_s\)</span>建模为清晰背景B和稀疏的线形雨分量S的线性叠加:<ul><li><span class="math inline">\(\mathbf{R}_{s}=\mathbf{B}+\mathbf{S}\)</span></li><li>整个场景中积累的降雨条纹会降低背景B的能见度。这是大多数去雨算法最常见的假设模型</li></ul></li><li>附着在相机镜头或窗玻璃上的雨滴会阻碍和/或模糊背景场景。因此雨滴退化图像<span class="math inline">\(R_d\)</span>可以建模为干净背景B和雨滴D在散射的小尺度局部相干区域的模糊或阻碍效应的组合<ul><li><span class="math inline">\(\mathbf{R}_{d}=(1-\mathbf{M}) \odot\mathbf{B}+\mathbf{D}\)</span></li><li>M是二进制掩码，而<span class="math inline">\(\odot\)</span>表示逐元素乘法。在蒙版中，如果M（x）= 1，则像素x是雨滴区域的一部分，否则属于背景</li></ul></li><li>在真实情况下，雨天图像经常同时包含雨和雾，而远处的雨条纹在整个场景中累积，以一种更类似于雾的方式降低能见度，在图像背景中创造一个类似雾的现象，定义捕获图像<span class="math inline">\(R_m\)</span>的雨和雾模型基于雨条纹模式和大气散射雾模式的合成<ul><li><span class="math inline">\(\mathbf{R}_{m}=\mathbf{B} \odot t +A(1-t) + \mathbf{S}\)</span></li><li>S为雨条纹分量，t和A分别是确定雾/雾成分的透射图和大气光</li></ul></li></ul></li><li>不过作者认为这些算法仍然存在许多不足之处<ul><li>对雨的建模过于简化，即每种方法只考虑和评估一种类型的雨</li><li>在合成图像上显示出许多定量结果，但这些图像通常无法捕获真实降雨的复杂性和特征</li><li>由于最后一点的原因，对于图像恢复，评价指标大多局限于(完全参考)PSNR和SSIM。当涉及到其他任务目的时，比如人类感知质量或计算机视觉效用，它们之间的联系可能不够紧密</li></ul></li><li>作者构造了一个大型基准，称为多用途图像去雨(MPID)，MPID数据集涵盖了更广泛的雨模型(雨条纹、雨滴、雨和雾)，包括用于评估的合成和真实世界的图像，并具有多种内容和2个来源(对于真实雨图像)</li><li>此外，作为图像去雨处理方面的首次尝试，作者为自动任务和视频监控场景分别注释了两组带有目标边界框的雨天图像，用于特定任务的评估</li><li>去雨方法概述：<ul><li>基于多帧图像的方法</li><li>基于先验的方法<ul><li>利用干净图像或降雨类型先验来去除降雨</li><li>不过都依赖于良好(且相对简单)的预先设计。因此，在场景复杂、形态复杂的真实图像上，它们的表现往往不尽人意</li></ul></li><li>数据驱动的CNN模型<ul><li>尽管基于深度学习的除雨方法与基于先验的除雨方法相比取得了进步，但它们的性能取决于合成的训练数据，如果真实的雨天图像出现域不匹配，则可能会出现问题</li></ul></li></ul></li><li>数据集<ul><li>现有的数据集要么规模太小，局限于一种降雨类型(条纹或下降)</li><li>要么缺乏足够的真实世界图像来进行不同的评估</li><li>此外，它们没有任何语义注释，也没有考虑任何后续任务性能</li></ul></li></ul><h3 id="new-benchmark-multi-purpose-image-deraining-mpid">New Benchmark:Multi-Purpose Image Deraining (MPID)</h3><ul><li>评估角度从传统的PSNR/SSIM，到无参考感知驱动指标和人的主观质量，再到“任务驱动指标”，这些指标表明了目标计算机视觉任务在图像去雨任务上的执行情况</li><li>作者从合成和现实世界的资源中大规模地生成/收集图像，覆盖不同的现实场景，并在需要时注释它们</li></ul><p><img src="/2020/11/21/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-0x05/5.png"></p><ul><li><p>训练集：包含三个合成模型</p><ul><li>Rain streak(T)：包含的2400对干净-雨图像，其中雨图由干净图像生成（清晰背景B和稀疏的线形雨分量S的线性叠加）</li><li>Rain drop (T)：由861对干净的和雨滴图像组成，来自借用</li><li>Rain and mist (T)：使用大气散射模型添加雾，700对</li></ul></li><li><p>测试集：从合成图像到真实图像</p><ul><li>利用上面相同的方式来得到三个综合测试集：Rain streak (S) 200对, Raindrop (S) 149对 和 Rain and mist (S) 70对</li><li>在每个测试集上，使用经典的PSNR和SSIM指标来评估去雨算法的恢复性能</li><li>为了预测对人类观察者的感知质量，引入了三种无参考的IQA模型以补充PSNR/SSIM的不足<ul><li>自然图像质量评估器(NIQE)：用于指示图像的感知“自然”程度：得分越小，表示感知质量越好</li><li>空间光谱熵的质量(SSEQ)</li><li>使用DCT统计的盲图像完整性标记器(BLIINDS-II)<ul><li>SSEQ和BLIINDS-II得分范围从0（最差）到100（最佳）</li></ul></li></ul></li><li>作者还收集了三组真实世界的图像：Rain streak (R), Raindrop (R), andRain and mist (R)<ul><li>它们分别属于三种定义的降雨类别，以评估失量化算法在真实世界中的泛化效果</li></ul></li></ul></li><li><p>任务驱动的评估集</p><ul><li>高级计算机视觉任务的性能，如目标检测和识别，在各种感官和环境恶化的情况下会恶化。去雨算法可以作为多雨条件下计算机视觉任务的预处理</li><li>为此收集了两组数据<ul><li>雨天驾车时采集的车载摄像头采集的雨中行车集(RID)<ul><li>雨效应最接近于相机镜头上的“雨滴”</li></ul></li><li>雨天网络交通监控摄像头采集的雨中行车集(RIS)<ul><li>雨效应是最接近于“雨和雾”(许多相机在雨中有雾凝结，低分辨率也会导致更多的雾效应)</li></ul></li></ul></li></ul></li></ul><h3 id="experimental-comparison">Experimental Comparison</h3><ul><li>作者在MPID山评估了六种算法</li><li>高斯混合模型先验（GMM），联合雨滴检测和去除（JORDER），深层细节网络（DDN），条件生成对抗网络（CGAN），使用多流密集网络的密度感知图像去雨方法（DIDMDN）和DeRaindrop。除GMM之外，其他所有算法都是基于CNN的最新算法</li></ul><p>实验部分感觉就是做了各种分析，表明了各种网络在不同类型雨图下的性能表现</p><p>作者在最后提出了对未来方向的一些看法：</p><ul><li>降雨类型多样，需要专门的模型。某些模型或组件被发现对特定的雨类型很有前途，例如，雨探测/注意，GANs，以及像补丁级GMM这样的先验。我们还提倡将适当的先验和数据驱动方法相结合</li><li>对于所有的雨类型，没有一个最佳的解算算法。为了处理真实的复杂多变的降雨，可能需要考虑专家的混合模型。另一个实际有用的方向是发展特定场景的减量化，例如交通视图</li><li>在所有指标下，也没有一种最佳的去雨算法。在设计去雨算法时，需要清楚它的最终目的。此外，经典的感知指标本身在评估去雨时可能存在问题。开发新的指标可能和新算法一样重要</li><li>在合成配对数据上训练的算法可能很难推广到真实数据上，特别是在复杂的雨类型上，如雨和雾。对所有真实数据进行不配对的训练可能会很有趣。</li><li>现有的方法似乎不能直接帮助检测。这可能会鼓励社区开发新的健壮的算法，以解释在真实的雨天图像中出现的高水平视觉问题。另一方面，为了实现雨中检测的鲁棒性，不需要采用雨中预处理</li></ul>]]></content>
    
    
    <categories>
      
      <category>study</category>
      
    </categories>
    
    
    <tags>
      
      <tag>图像去雾去云相关</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>论文阅读-0x04</title>
    <link href="/2020/11/16/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-0x04/"/>
    <url>/2020/11/16/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-0x04/</url>
    
    <content type="html"><![CDATA[<p>论文的阅读笔记（Wenqi Ren 任文琦老师）：《Gated Fusion Network forSingle Image Dehazing》<a href="https://github.com/rwenqi/GFN-dehazing">[code]</a> , CVPR 2018</p><span id="more"></span><h2 id="gated-fusion-network-for-single-image-dehazing">Gated FusionNetwork for Single Image Dehazing</h2><h3 id="introduction">Introduction</h3><ul><li>基于一个端到端可训练的神经网络，该网络由 encoder and a decoder组成</li><li>该网络采用融合策略，通过白平衡、对比度增强和伽马校正，从原始的模糊图像中提取3个输入<ul><li>White Balance (WB), Contrast Enhancing (CE), and Gamma Correction(GC)</li></ul></li><li>基于这些不同输入之间的差异计算像素级置信图，以混合派生输入的信息，并保持良好的区域可见度</li><li>最终的去噪图像是通过限制输入的重要特征而得到的</li><li>同时为了训练网络，引入了一种多尺度方法来避免晕轮效应</li><li>早期的方法专注于根据清晰图像的统计数据来开发手工制作的特征<ul><li>暗通道先验和局部最大对比度（dark channel prior and local maxcontrast）</li></ul></li><li>在去雾化文献中，去雾化过程通常被描述为<ul><li><span class="math inline">\(I(x) = J(x)t(x) + A(1-t(x))\)</span><ul><li>I(x) 观测到的雾霾图像; J(x)是无雾场景</li><li>A是全局大气光照</li><li>t(x)是描述未散射并到达摄像机传感器的那部分光的场景传输率</li></ul></li><li>不过实际上，光的传播和大气光是未知的。给定一个模糊的图像，大多数去雾方法试图估计透射t(x)和大气光A</li></ul></li><li>作者认为从模糊图像中估计传输是一个严重病态问题<ul><li>一些方法尝试使用视觉线索来捕获模糊图像的确定性和统计特性</li><li>显然这些传输近似是不准确的，特别是在场景中，物体的颜色本质上与大气光的颜色相似的情况下</li><li>所以这种错误的传输估计就会直接影响到恢复质量</li></ul></li><li>为此作者提出了一种新的端到端的可训练神经网络，它不显式估计传输和大气光，因此可以避免由传输估计误差引起的伪影</li><li>模糊图像中有两个主要的因素需要处理：<ul><li>一是由大气光引入的颜色投射</li><li>二是由于衰减造成的能见度不足</li></ul></li><li><img src="/2020/11/16/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-0x04/1.png"><ul><li>第一个输入通过消除大气光引起的色散确保了输出的自然再现(图1(b))</li><li>第二种对比度增强的输入产生了更好的全局可见性，但主要是在厚的模糊区域(如图1(c)中的后墙)</li><li>然而，对比度增强后的图像在光线模糊的区域太暗。因此，为了恢复轻度模糊区域，我们发现gamma校正后的图像很好地恢复了轻度模糊区域的信息(如图1(d)中的前草坪)</li></ul></li><li>主要贡献<ul><li>提出了一种深度端到端可训练的中立网络，它可以恢复清晰的图像（无需对场景传输和大气光线进行任何限制）</li><li>证明了GFN在单一图像去雾的效用和有效性，利用从原始模糊图像的导出输入</li><li>用多尺度的方法训练所提出的模型，以消除影响图像去雾的光环伪影</li></ul></li><li>图像去雾的方法主要有三种:基于多图像的方法、基于手工先验的方法和数据驱动的方法<ul><li>基于多图像的方法：这些方法都做了相同的假设，即在同一场景中使用多幅图像</li><li>基于手工先验的方法：强烈依赖于假设图像先验的准确性，因此当假设先验不足以描述真实图像时，可能会表现不佳。因此，这些方法往往会引入不需要的人工现象，如颜色失真。</li></ul></li></ul><h3 id="gated-fusion-network---gfn">Gated Fusion Network---GFN</h3><ul><li><img src="/2020/11/16/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-0x04/2.png"></li><li>使用原始的模糊图像和三个衍生图像作为输入</li><li>其核心思想是学习置信度映射，通过保留图像中最重要的特征，将多个输入图像合并为一个单一的图像<ul><li>首先，由于大气光的影响，模糊图像的颜色经常发生变化；二是遥远地区由于散射和衰减现象造成的能见度不足</li><li>基于这些观察，我们生成三个输入，从最初的模糊图像中恢复整个图像的颜色和能见度<ul><li>白平衡：对模糊输入进行估计，恢复图像潜色，消除大气色造成的色差<ul><li>灰色世界假设技术：给定一幅颜色变化量足够大的图像，图像中红、绿、蓝分量的平均值应该平均为一个普通灰度值</li><li>这一假设在任何给定的现实场景中基本上都是有效的，因为颜色的变化是随机的和独立的。可以说，在样本数量较大的情况下，平均值应该趋向于收敛于均值，均值是灰色的</li><li>虽然白平衡可以消除大气光引起的色移，但结果仍然是低对比度</li></ul></li><li>然后利用对比度增强和伽马校正来获取更好的全局信息<ul><li>Ancuti and Ancuti 从模糊输入中减去整幅图像<span class="math inline">\(I\)</span>的平均亮度值<span class="math inline">\(\widetilde I\)</span>，得到对比度增强图像</li><li>然后利用一个因子<span class="math inline">\(\mu\)</span>来对恢复的模糊区域的亮度进行线性增加</li><li><span class="math inline">\(\mathbf{I}_{ce}=\mu(\mathbf{I}-\tilde{I})\)</span></li><li>其中<span class="math inline">\(\mu=2(0.5+\tilde{I})\)</span></li><li>不过在雾霾比较密集的地区。主要原因是随着图像亮度指标<span class="math inline">\(\widetilde I\)</span>的增加，<span class="math inline">\((\mathbf{I}-\tilde{I})\)</span>的负值可能会主导对比度增强输入，导致图像的暗区往往是黑色的</li></ul></li><li>伽马校正<ul><li>为了克服上述情况，使用伽马校正来创建另一种对比度增强</li><li><span class="math inline">\(\mathbf{I}_{g c}=\alpha\mathbf{I}^{\gamma}\)</span></li><li>伽玛校正是一种非线性操作，用于对图像内容中的亮度或三色值进行编码(<span class="math inline">\(\gamma\)</span>&lt; 1)和解码(<span class="math inline">\(\gamma\)</span>&gt; 1)</li><li>本文中，作者使用了<span class="math inline">\(\alpha = 1,\gamma =2.5\)</span>，能有效地解决上述情况</li></ul></li></ul></li></ul></li><li>网络使用了一个encoder-decoder network model，并且带有残差结构以及跳跃连接<ul><li>在输入层中保留场景颜色的主要信息，同时消除输入中不重要的颜色</li><li>然后将反卷积层的输出组合起来，以恢复三个导出输入的权重图<ul><li>即反卷积层的输出是衍生输入图像<span class="math inline">\(I_{wb},I_{ce}\)</span> 和 <span class="math inline">\(I_{gc}\)</span>的置信度映射（权重图）</li></ul></li><li>将反卷积得到的置信度映射与三个衍生输入相乘，得到每个尺度下的最终去雾图像</li><li><span class="math inline">\(J=C_{w b} \circ \mathbf{I}_{w b}+C_{c e}\circ \mathbf{I}_{c e}+C_{g c} \circ \mathbf{I}_{g c}\)</span></li></ul></li><li>为了在不丢失局部细节的情况下利用更多的上下文，使用扩展网络来扩大卷积层中的接受域。在每个卷积层或反卷积层之后添加校正层</li><li><img src="/2020/11/16/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-0x04/3.png"></li><li>多尺度结构<ul><li>如图所示，首先在粗尺度上进行去雨去雾操作，然后将结果上采样再进一步操作</li><li>卷积层以连接的形式从前一阶段获取清晰的图像以及它自己的模糊图像和派生的输入</li><li>每个输入大小是其粗尺度网络大小的两倍</li><li>在下一阶段之前有一个上采样层。在更精细的尺度上，恢复原始的高分辨率图像</li><li>通过encoder-decoder结构，构建了一个金字塔结构网络，适用MSE准则</li><li><span class="math inline">\(\mathcal{L}_{\text {cont}}(\Theta,k)=\frac{1}{N} \sum_{i=1}^{N}\left\|\mathcal{F}\left(\mathbf{I}_{i, k},\Theta, k\right)-\mathbf{J}_{i, k}\right\|^{2}\)</span></li></ul></li><li>对抗损失<ul><li>建立一个鉴别器，以最精细尺度或ground-truth锐化图像的输出作为输入</li><li><span class="math inline">\(\begin{aligned} \mathcal{L}_{a dv}=&amp; \underset{\mathbf{J} \sim p_{\text {clear}}(\mathbf{J})}{\mathbb{E}}[\logD(\mathbf{J})]  &amp;+\underset{\mathbf{I} \sim p_{\text {hazy}}(\mathbf{I})}{\mathbb{E}}[\log (1-D(\mathcal{F}(\mathbf{I})))]\end{aligned}\)</span></li><li>其中F为上图中的多尺度网络，D为鉴别器</li><li><span class="math inline">\(\mathcal{L}_{\text{total}}=\mathcal{L}_{\text {cont}}+0.001 \mathcal{L}_{\text{adv}}\)</span></li></ul></li></ul><h3 id="experiments">Experiments</h3><ul><li>在合成数据集和真实的模糊图像上定量地评估了所提出的算法</li><li>给定一个清晰的图像 J ,随机大气光A∈(0.8,1.0)和真实深度d,我们使用<span class="math inline">\(t(x)=e^{−βd  (x)}\)</span>合成变化映射，然后利用物理模型生成模糊图像。对于散射系数β∈(0.5,1.5)</li><li>对每一张干净的图像使用7种不同的校正器，这样就可以为每一张输入图像合成不同的雾霾浓度图像</li><li>此外，在每个模糊输入中加入1%高斯噪声，增强训练后网络的鲁棒性</li><li>结构相似性(SSIM)和峰值信噪比(PSNR)</li><li><img src="/2020/11/16/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-0x04/4.png"></li><li>图像融合是一种保留最有用的特征，将多幅图像融合为一幅图像的方法<ul><li>通过计算相应的置信度图来过滤重要的输入信息，从而有效地融合了输入信息</li><li>因此，在GFN中，衍生的输入通过三个像素级置信映射进行权重控制，以保持区域的良好可视性</li><li>GFN有两个优点<ul><li>通过单像素的操作可以减少基于块的伪影(例如黑信道优先)</li><li>消除传输和大气光估计造成的影响</li></ul></li></ul></li><li><img src="/2020/11/16/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-0x04/5.png"><ul><li>多尺度方法的有效性。第一和第二行分别是单尺度网络和多尺度网络的结果</li></ul></li><li><img src="/2020/11/16/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-0x04/6.png"><ul><li>训练了一个没有融合过程的端到端网络。该网络与DFN具有相同的结构，只是输入是模糊图像，输出是不需要置信映射学习的去雾结果</li><li>没有门控的方法会产生图8( b )中的非常暗的图像</li><li>没有融合策略的方法会产生颜色失真和暗区域的结果，如图8( c )</li></ul></li><li>局限性是DFN不能处理有很大雾的已损坏图像。由于重度雾霾严重地干扰了大气光(大气光不是恒定的)，雾霾模型不适用于此类例子</li></ul>]]></content>
    
    
    <categories>
      
      <category>study</category>
      
    </categories>
    
    
    <tags>
      
      <tag>图像去雾去云相关</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>论文阅读-0x03</title>
    <link href="/2020/10/25/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-0x03/"/>
    <url>/2020/10/25/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-0x03/</url>
    
    <content type="html"><![CDATA[<p>两篇论文的阅读笔记：《Rotate to Attend: Convolutional TripletAttention Module》<a href="https://github.com/LandskapeAI/triplet-attention">[code]</a>&amp;《GINet: Graph Interaction Network for Scene Parsing》,ECCV2020</p><span id="more"></span><h2 id="rotate-to-attend-convolutional-triplet-attention-module">Rotateto Attend: Convolutional Triplet Attention Module</h2><h3 id="introduction">Introduction</h3><ul><li><p>作者提出了一种轻量且有效的注意力机制——Triplet Attetion</p></li><li><p>通过使用Triplet Branch结构来<strong>捕获跨维度交互（capturingcross dimension interaction）</strong>。对于输入张量，TripletAttention通过旋转操作和残差变换建立维度间的依存关系，并以可忽略的计算开销对通道和空间信息进行编码</p></li><li><p>关于attention机制的使用，已经有很多具有代表性的工作，不够在这方面也只是看过SENet，在此基础上改进的工作并没有太多了解，这里列举出来，然后找时间再看看</p><ul class="task-list"><li><label><input type="checkbox" checked><strong>SENet</strong>(Squeeze and Excitemodule)</label></li><li><label><input type="checkbox" checked><strong>CBAM</strong>(Convolutional Block AttentionModule)</label></li><li><label><input type="checkbox"><strong>BAM</strong>(BottleneckAttention Module)</label></li><li><label><input type="checkbox"><span class="math inline">\(A^2\)</span><strong>-Nets</strong>(DoubleAttention Networks)</label></li><li><label><input type="checkbox" checked><strong>NL</strong>(Non-Local blocks)</label></li><li><label><input type="checkbox"><strong>GSoP-Net</strong>(GlobalSecond order Pooling Networks)</label></li><li><label><input type="checkbox"><strong>GC-Net</strong>(GlobalContext Networks)</label></li><li><label><input type="checkbox"><strong>CC-Net</strong>(Criss-CrossNetworks)</label></li><li><label><input type="checkbox"><strong>SPNet</strong></label></li></ul></li><li><p><img src="/2020/10/25/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-0x03/1.png"></p><ul><li>（a）Squeeze Excitation (SE) Module</li><li>（b）Convolutional Block Attention Module (CBAM)</li><li>（c）Global Context (GC) Module</li><li>（d）triplet attention</li><li><span class="math inline">\(\bigotimes\)</span> 代表矩阵乘法，<span class="math inline">\(\bigodot\)</span> 代表对应位置元素相乘，<span class="math inline">\(\bigoplus\)</span> 代表对应位置元素相加</li></ul></li><li><p>不过作者说上面这些方法都存在缺陷，没有去捕获跨纬度交互信息，并且作者提出的方法几乎无参数</p></li><li><p>与之前的方法进行对比，有两个优点：</p><ul><li>以微不足道的计算开销捕获丰富的鉴别特征表示</li><li>强调跨维交互的重要性而不降低维数，从而消除了通道和权值之间的间接对应关系</li></ul></li></ul><h3 id="triplet-attention-module">Triplet attention module</h3><h4 id="cbam">CBAM</h4><p><img src="/2020/10/25/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-0x03/1.jpg"></p><ul><li>对于输入 <span class="math inline">\(\chi \in \mathbb{R}^{C \times H\timesW}\)</span>，CBAM的注意力机制先计算其通道注意力，然后再计算空间注意力，比较简单，其计算公式为<ul><li><span class="math inline">\(\omega=\sigma\left(f_{\left(\mathbf{W}_{0},\mathbf{W}_{1}\right)}(g(\chi))+f_{\left(\mathbf{W}_{0},\mathbf{W}_{1}\right)}(\delta(\chi))\right)\)</span></li><li><span class="math inline">\(g(\chi)\)</span> 代表全局平均池(globalaverage pooling，GAP)函数<ul><li><span class="math inline">\(g(\chi)=\frac{1}{W \times H}\sum_{i=1}^{H} \sum_{j=1}^{W} \chi_{i, j}\)</span></li></ul></li><li><span class="math inline">\(\delta(\chi)\)</span>代表全局最大值池(global maxpooling，GMP)函数<ul><li><span class="math inline">\(\delta(\chi)=\max _{H,W}(\chi)\)</span></li></ul></li><li><span class="math inline">\(\sigma\)</span> 代表sigmoid激活函数</li><li><span class="math inline">\(f_{\left(\mathbf{W}_{0},\mathbf{W}_{1}\right)}(g(\chi))\)</span> 和 <span class="math inline">\(f_{\left(\mathbf{W}_{0},\mathbf{W}_{1}\right)}(\delta(\chi))\)</span>代表两种变换，进一步转换有：<ul><li><span class="math inline">\(\omega=\sigma\left(\mathbf{W}_{1}\operatorname{ReLU}\left(\mathbf{W}_{0} g(\chi)\right)+\mathbf{W}_{1}\operatorname{ReLU}\left(\mathbf{W}_{0}\delta(\chi)\right)\right)\)</span></li></ul></li><li>其中<span class="math inline">\(W_0\)</span> 和 <span class="math inline">\(W_1\)</span> 代表权重矩阵，其大小定义为 <span class="math inline">\(C \times \frac{C}{r}\)</span> 和 <span class="math inline">\(\frac{C}{r} \times C\)</span><ul><li>这里的 r 代表降维率，r 越大的话，计算复杂度越低</li><li>需要注意的是 MLP: <span class="math inline">\(W_0\)</span> 和 <span class="math inline">\(W_1\)</span> 的权重在CBAM中对于两个输入 <span class="math inline">\(g(\chi)\)</span> 和 <span class="math inline">\(\delta(\chi)\)</span> 共享</li><li>不过这样由于存在降维操作，会造成通道间关系的损失</li></ul></li></ul></li></ul><h4 id="triplet-attention">Triplet Attention</h4><ul><li>所以作者针对CBAM中存在的缺陷，提出了一种不涉及降维操作的高效轻量（几乎无参数）的注意力机制，并且关注跨维度交互信息</li><li><img src="/2020/10/25/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-0x03/2.png"></li><li>Triplet attention由三个分支组成，给定一个输入tensor(C x H x W)<ul><li>其中两个分支负责<strong>捕获通道 C 和空间维度 H 或 W之间的跨维交互特征</strong><ul><li>将输入进入每个分支中，通过 Z-pool，然后是核大小为k×k的卷积层</li><li>attention权值由sigmoid激活生成，然后应用于已排列好的输入张量，然后将其恢复到原始的输入形状</li></ul></li><li>另一个分支与CBAM类似，用于构建空间的attention</li><li>三个分支的输出都使用简单的平均进行聚合</li></ul></li><li>跨纬度交互特征（Cross-Dimension Interaction）<ul><li>作者认为一个channel池化到一个像素值，会导致空间信息的大量丢失，并且没有考虑到维度和空间之间的相互依赖关系<ul><li>空间注意力（spatial attention）：关注维度的哪些部分</li><li>维度/通道注意力（channel attention）：关注哪些维度通道</li></ul></li><li>所以作者提出的Tripletattention同时捕捉空间维度和输入张量通道维度之间的交互作用</li></ul></li><li>Z-pool<ul><li>将C维度的Tensor缩减到2维，将该维上的平均池化特征和最大池化特征连接起来。这使得该层能够保留实际张量的丰富表示，同时缩小其深度以使进一步的计算量更轻</li><li><span class="math inline">\(Z-pool(\chi)=\left[\operatorname{MaxPool}_{0d}(\chi), \operatorname{AvgPool}_{0 d}(\chi)\right]\)</span><ul><li>0d 表示第零维：<span class="math inline">\((C \times W \times H)==&gt; (2 \times W \times H)\)</span></li></ul></li></ul></li><li><img src="/2020/10/25/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-0x03/3.png"><ul><li>在H维度和C维度之间建立交互</li><li>输入张量 <span class="math inline">\(\chi\)</span>沿H轴逆时针旋转90° ，得到 <span class="math inline">\(\hat{\chi}_{1}\)</span> （ W×H×C ）</li><li>经过Z-pool得到 <span class="math inline">\(\hat{\chi}_{1}^{*}\)</span> （ 2×H×C ）</li><li><span class="math inline">\(\hat{\chi}_{1}^{*}\)</span>通过内核大小为k×k的标准卷积层，再通过批处理归一化层，得到 (1×H×C)</li><li>然后通过sigmoid来生成的注意力权值，在最后输出是沿着H轴进行顺时针旋转90°保持和输入的shape一致</li></ul></li><li><img src="/2020/10/25/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-0x03/4.png"></li><li>在H维度和W维度之间建立交互，基本同上</li><li>输入张量 <span class="math inline">\(\chi\)</span>沿W轴逆时针旋转90° ，得到 <span class="math inline">\(\hat{\chi}_{1}\)</span> （ H×C×W ）</li><li>经过Z-pool得到 <span class="math inline">\(\hat{\chi}_{1}^{*}\)</span> （ 2×C×W ）</li><li><span class="math inline">\(\hat{\chi}_{1}^{*}\)</span>通过内核大小为k×k的标准卷积层，再通过批处理归一化层，得到 (1×C×W)</li><li>然后通过sigmoid来生成的注意力权值，在最后输出是沿着H轴进行顺时针旋转90°保持和输入的shape一致</li><li><img src="/2020/10/25/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-0x03/5.png"></li><li>建立空间内部的交互</li><li>直接进行Z-pool得到（ 2×H×W ）</li><li>通过内核大小为k×k的标准卷积层，再通过批处理归一化层，得到(1×H×W)</li><li>然后通过sigmoid来生成的注意力权值</li><li>综上所述，最终的输出y为<ul><li><span class="math inline">\(y=\frac{1}{3}\left(\overline{\hat{\chi_{1}}\sigma\left(\psi_{1}\left(\hat{\chi_{1}}^{*}\right)\right)}+\overline{\hat{\chi_{2}}\sigma\left(\psi_{2}\left(\hat{\chi_{2}^{*}}\right)\right)}+\chi\sigma\left(\psi_{3}\left(\hat{\chi_{3}}\right)\right)\right)\)</span><ul><li>其中<span class="math inline">\(\psi_{1}, \psi_{2} ,\psi_{3}\)</span> 代表卷积核大小为k的标准二维卷积层</li></ul></li><li>进一步简化得到：</li><li><span class="math inline">\(y=\frac{1}{3}\left(\overline{\hat{\chi_{1}}\omega_{1}}+\overline{\hat{\chi_{2}} \omega_{2}}+\chi\omega_{3}\right)=\frac{1}{3}\left(\overline{y_{1}}+\overline{y_{2}}+y_{3}\right)\)</span></li></ul></li><li>复杂度分析（Complexity Analysis）<ul><li><img src="/2020/10/25/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-0x03/6.png"></li><li>C为该层的输入通道数，r为MLP在计算通道注意力时降维使用的缩减比，用于2D卷积的核大小用k表示，k&lt;&lt;&lt;C。</li></ul></li></ul><h3 id="experiments">Experiments</h3><ul><li>实验部分，作者证明其有效性和高效性，比较了性能和参数量水平</li><li>ImageNet数据集上的分类实验<ul><li><img src="/2020/10/25/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-0x03/7.png"></li><li>Single-crop error rate (%) 单次剪裁错误率(%)，complexity comparisonsin terms of network parameters (in millions) 网络参数(以百万计) andfloating point operations per second (FLOPs)每秒浮点运算</li></ul></li><li>目标检测实验<ul><li><img src="/2020/10/25/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-0x03/8.png"></li><li>Object detection mAP(%) on the MS COCO validation set</li><li><img src="/2020/10/25/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-0x03/9.png"></li><li>Object detection mAP(%) on the PASCAL VOC 2012 test set</li></ul></li><li>消融实验（Ablation Study on Branches）</li></ul><p><img src="/2020/10/25/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-0x03/10.png"></p><h2 id="ginet-graph-interaction-network-for-scene-parsing">GINet: GraphInteraction Network for Scene Parsing</h2><h3 id="introduction-1">Introduction</h3><ul><li><p>作者提出了一种图形交互单元（GIunit）和语义上下文损失（SC-loss），利用语义上下文信息来促进基于图像区域的推理。图形交互单元能够增强卷积网络的特征表示，同时能自适应地为每个样本学习语义一致性</p></li><li><blockquote><p>具体地，基于数据集的语义知识首先被纳入图交互单元来促进视觉图的上下文推理，然后演化的视觉图特征被反投影到每个局部特征来增强其可区分力。图交互单元进一步被语义上下文损失改善其生成基于样本语义图的能力。</p></blockquote></li><li><p>上下文推理：</p><ul><li>从特征图中学习图像的一种表示，<strong>图中的顶点定义像素簇(“区域”)，而边表示这些区域在特征空间中的相似性或关系</strong></li><li>通过这种方法，可以在交互图空间中进行上下文推理，然后将演化后的图映射回原始空间，增强场景解析的局部表示。</li></ul></li><li><p><img src="/2020/10/25/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-0x03/11.png"></p><ul><li>作者不仅仅是在输入图像（特征图）和图表示之间的推理（上面部分）</li><li>作者将语义信息（外部知识）与之进行关联，提出了图像交互单元（GIunit），将基于数据集的语言知识整合到视觉图的特征表示中，然后将视觉图的演化表示重新投影回每个位置表示中，以增强判别能力（下面部分）。为此作者也提出了一种语义上下文损失，让其更好自适应地表示样本（在场景中出现的类别被强调，而没有出现的类别被抑制）</li></ul></li><li><p>论文地主要贡献：</p><ul><li>提出了一种新的图形交互单元(GI单元)用于上下文建模，该单元融合了基于数据集的语言知识，可以促进可视化图形上的上下文推理。此外，它还学习了基于范例的语义图</li><li>提出了语义上下文丢失(SC-loss)来规范训练过程，它强调出现在场景中的类别，抑制没有出现在场景中的类别</li></ul></li><li><p>场景解析的上下文建模根据是否考虑图推理，主要可分为两类</p><ul><li><p>FCN</p><ul><li><img src="/2020/10/25/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-0x03/2.jpg"></li></ul></li><li><p>PSPNet利用多尺度信息，提出了金字塔池模块；DeepLab v2和DeepLabv3利用空洞卷积和空间金字塔池捕获上下文信息，该信息由不同扩张率的空洞卷积组成</p><ul><li><img src="/2020/10/25/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-0x03/13.png"></li><li>不过这种方法都是手工设置尺度参数，不能自适应的去学习，最好的方式就是每个像素都有它自己的独特的连接信息</li></ul></li><li><p>使用Non-local非局部卷积的方式来获取全局的上下文信息，针对每一个像素，去计算和其他像素之间的依赖关系</p><ul><li><img src="/2020/10/25/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-0x03/14.png"></li></ul></li><li><p>近期的一些工作会对Non-local进行精简，因为并不需要对所有的像素之间进行依赖关系计算</p></li><li><p>GCN用在语义分割中</p><ul><li><img src="/2020/10/25/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-0x03/15.png"></li><li>首先进行投射，从特征空间投射到图空间，然后对图空间进行GCN图推理，然后再反投射回特征空间</li></ul></li></ul></li></ul><h3 id="ginet">GINet</h3><ul><li><p>GI单元的目标是纳入基于数据集的语言信息，以促进局部表示</p><ul><li>GI单元以视觉和语义表示为输入，通过生成视觉图和语义图进行上下文推理</li><li>以视觉节点和语义节点的相似性为指导，在两个图之间进行图交互，演进节点特征</li></ul></li><li><p>对于输入的二维图现象，GINet使用resnet作为主干网络提取特征信息</p></li><li><p>将基于数据集的语义知识以分类实体(类)的形式提取出来，并将分类实体(类)输入到word embedding 单词嵌入 (如GloVe)中，实现语义表示</p><ul><li>wordembedding，就是找到一个映射或者函数，生成在一个新的空间上的表达</li><li>GloVe（ Global Vectors for Word Representation）：进行词的向量化表示，使得向量之间尽可能多地蕴含语义和语法的信息</li><li>输入：语料库；输出：词向量</li><li>可以把一个单词表达成一个由实数组成的向量，这些向量捕捉到了单词之间一些语义特性，比如相似性（similarity）、类比性（analogy）等。我们通过对向量的运算，比如欧几里得距离或者cosine相似度，可以计算出两个单词之间的语义相似性</li></ul></li><li><p><img src="/2020/10/25/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-0x03/12.png"></p></li><li><p>GI unit中通过图的投影（graphprojection）操作传递视觉特征和语义嵌入表示，分别构造出两个图</p><ul><li><p>GI unit的目标是纳入基于数据集的语言信息，以促进局部表示</p></li><li><p>GI unit以视觉表示和语义表示为输入，通过图的投影生成视觉图（visualgraph）和语义图（semantic graph）进行上下文推理</p><ul><li>视觉图（visual graph）<ul><li>建立在视觉特征之上的，其中节点表示视觉区域，而边表示这些区域之间的相似性或关系</li><li>对于给定的视觉特征输入 <span class="math inline">\(X \in\mathbb{R}^{L \times C}\)</span>， 其中 <span class="math inline">\(L=H\times W\)</span> ，期望构造一个视觉图表示<span class="math inline">\(P\in \mathbb{R}^{N \timesD}\)</span>，其中N是节点数目，D是每个节点的所期望的特征维数</li><li>作者引入了一个变换矩阵<span class="math inline">\(Z \in\mathbb{R}^{N \times L}\)</span>，从而通过X进而构造得到P</li><li><span class="math inline">\(P=Z X W\)</span><ul><li><span class="math inline">\(W \in \mathbb{R}^{C \timesD}\)</span>是一个可以训练矩阵，Z自适应将局部特征聚合到可视化图中的一个节点</li></ul></li></ul></li><li>语义图（semantic graph）<ul><li>依赖于数据的类别上(用wordembedding表示)，它表示了语言相关性和标签相关性</li><li>建立一个语义图表示<span class="math inline">\(S \in \mathbb{R}^{M\timesD}\)</span>，其中M表示节点的数量，等于数据集中分类实体(类)的数量，D为语义图中每个节点的特征维数</li><li>首先使用现成的词向量对每个类别得到语义表达<span class="math inline">\(l_{i} \in \mathbb{R}^{K}\)</span> ，<span class="math inline">\(i \in\{0,1, \ldots, M-1\}, K=300\)</span></li><li>然后，再使用两层MLP层对语言嵌入进行调整，使之适应视觉图的交互</li><li><span class="math inline">\(S_{i}=M L P\left(l_{i}\right), i\in\{0,1, \ldots, M-1\}\)</span></li><li><span class="math inline">\(S_{i}\)</span>代表每个类别的节点特征</li></ul></li></ul></li><li><p>以视觉节点和语义节点的相似性为指导，在两个图之间进行图交互，推断节点特征</p><ul><li>Visual to Semantic (V2S)<ul><li>同下</li><li><span class="math inline">\(\widetilde{P}=f\left(\left(A_{v}+I\right) PW_{v}\right)\)</span></li><li>语义图中的信息从wordembedding中抽取出来的时候是比较通用的，然后通过变换后的VisG的交互后，就变成了针对当前图片的一种语义表征。然后针对这些语义表征利用SC-loss去进一步约束每个有别的有无（可以有效的提高对小物体的识别能力）</li><li>同下，计算引导矩阵</li><li><span class="math inline">\(G_{i, j}^{v 2 s}=\frac{\exp \left(W_{s}\widetilde{s}_{i} \cdot W_{p} \tilde{p}_{j}\right)}{\sum_{n=1}^{N} \exp\left(W_{s} \tilde{s}_{i} \cdot W_{p}\widetilde{p_{n}}\right)}\)</span></li><li>得到引导矩阵侯，对SemG图进行更新，以生成基于范例的SemG</li><li><span class="math inline">\(S_{o}=\beta_{v 2 s} \widetilde{S}+G^{v 2s} \widetilde{P} W_{v 2 s}\)</span></li></ul></li><li>Semantic to Visual (S2V)<ul><li>首先对SemG进行图卷积，从而得到适合与VisG进行交互的图表示<span class="math inline">\(\widetilde{S}\)</span></li><li><span class="math inline">\(\widetilde{S}=f\left(\left(A_{s}+I\right) SW_{s}\right)\)</span></li><li><span class="math inline">\(A_{s} \in \mathbb{R}^{M \timesM}\)</span>是一个可学习的邻接矩阵或共现矩阵，表示语义相关或标签依赖之间的联系，过梯度下降随机初始化更新，<span class="math inline">\(I\)</span> 是单位矩阵，<span class="math inline">\(W_{s} \in \mathbb{R}^{D \times D}\)</span>是可训练矩阵参数，f 是非线性激活函数<ul><li>因为邻接矩阵的对角都是0，和特征矩阵内积相当于将邻接矩阵做了加权和，节点特征的值成为了邻接矩阵的权，自身的特征被忽略。为避免这种情况，可以给A加上一个单位矩阵</li></ul></li><li>利用变换得到的SemG来促进基于VisGeP的上下文推理</li><li>为了得到两个节点之间的关系，计算其特征相似度作为引导矩阵<span class="math inline">\(G^{s 2 v} \in \mathbb{R}^{N \timesM}\)</span></li><li>对于来自 <span class="math inline">\(\operatorname{VisG}\widetilde{P}\)</span> 的一个节点 <span class="math inline">\(\widetilde{p_{i}} \in \mathbb{R}^{D}\)</span>和来自SemG中的节点 <span class="math inline">\(\widetilde{s_{j}} \in\mathbb{R}^{D}\)</span> ，我们可以计算 <span class="math inline">\(\widetilde{s_{i}}\)</span> 对节点 <span class="math inline">\(\widetilde{p_{i}}\)</span> 的赋权值的引导信息</li><li><span class="math inline">\(G_{i, j}^{s 2 v}=\frac{\exp \left(W_{p}\tilde{p}_{i} \cdot W_{s} \tilde{s}_{j}\right)}{\sum_{m=1}^{M} \exp\left(W_{p} \tilde{p}_{i} \cdot W_{s}\widetilde{s_{m}}\right)}\)</span></li><li>其中<span class="math inline">\(W_{p} \in \mathbb{R}^{D / 2 \timesD}\)</span> and <span class="math inline">\(W_{s} \in \mathbb{R}^{D / 2\timesD}\)</span>是可学习的权重矩阵用来降维；得到该引导矩阵后，可以从SemG中提取信息来增强VisG的表示</li><li><span class="math inline">\(P_{o}=\widetilde{P}+\beta_{s 2 v} G^{s 2v} \widetilde{S} W_{s 2 v}\)</span></li><li>其中<span class="math inline">\(W_{s 2 v} \in \mathbb{R}^{D \timesD}\)</span> 是可学习的权重矩阵，<span class="math inline">\(\beta_{s 2v} \in \mathbb{R}^{N}\)</span>是初始化为零的可学习向量，可以通过一个标准梯度来更新</li><li>借助引导矩阵 <span class="math inline">\(G_{s 2v}\)</span>，有效地构建了视觉区域与语义概念之间的相关性，并将相应的语义特征纳入到视觉节点表示中</li></ul></li></ul></li></ul></li><li><p>在GI单元中进行图交互操作，其中语义图被用来促进视觉图的上下文推理，视觉图指导提取基于范例的语义图</p></li><li><p>Gi unit输出：基于范例的SemG；语义信息增强的VisG</p></li><li><p>重用投影矩阵Z来反向投影 <span class="math inline">\(P_0\)</span>得到二维像素特征</p><ul><li>给定VisG中的节点 <span class="math inline">\(P_{o} \in \mathbb{R}^{N\times D}\)</span>，反投影：</li><li><span class="math inline">\(\tilde{X}=Z^{T} P_{o}W_{o}+X\)</span></li><li><span class="math inline">\(W_{o} \in \mathbb{R}^{D \timesC}\)</span> 是一个可训练矩阵，<span class="math inline">\(Z^{T} \in\mathbb{R}^{L \times N}\)</span> 是Z的转置矩阵，使用残差结构</li></ul></li><li><p>将GIunit生成的交互后的视觉图通过图重投影操作来增强对每个局部可视化表示的识别能力，同时在训练阶段通过语义上下文丢失来更新和约束语义图</p><ul><li>强调场景中出现的类别，抑制没有出现的类别，使GINet能够自适应地增强每个样本的外部语义知识，改进基于范例的SemG的生成</li><li>首先对每一个类定义一个可学习的语义中心 <span class="math inline">\(c_{i} \in \mathbb{R}^D\)</span> ，然后对于 <span class="math inline">\(\operatorname{SemG} \widetilde{S_0}\)</span>中的每个语义节点 <span class="math inline">\(\widetilde{s_{i}} \in\mathbb{R}^{D}\)</span> ，在 <span class="math inline">\(\widetilde{s_{i}}\)</span> 和 <span class="math inline">\(c_i\)</span>上通过一个简单的点积运算和sigmoid激活函数来计算分数 <span class="math inline">\(v_i\)</span> (0-1)，用BCE损失进行训练</li><li>SC-loss使语义图中的节点特征与不存在类别的语义质心之间的相似性最小化，并使与存在类别之间的相似性最大化</li><li><span class="math inline">\(Loss _{s c}=-\frac{1}{M}\sum_{i=1}^{M}\left(y_{i} \cdot \log v_{i}\right)+\left(1-y_{i}\right)\log \left(1-v_{i}\right)\)</span><ul><li>其中<span class="math inline">\(y_i \in \{0,1\}\)</span>表示每个类别是否存在</li></ul></li><li>在主干的Res4上添加了一个完整的卷积分割头来获得分割结果，于是GINet由SC-loss、辅助损失（auxiliaryloss）和交叉熵损失组成</li><li><span class="math inline">\(Loss =\lambda \operatorname{Loss}_{sc}+\alpha \operatorname{Loss}_{\text {aux}}+\operatorname{Loss}_{ce}\)</span><ul><li>其中 <span class="math inline">\(\alpha\)</span> 和 <span class="math inline">\(\lambda\)</span>都是超参数；与之前的方法相似，将辅助损失的超参数 <span class="math inline">\(\alpha\)</span> 设为0.4</li></ul></li></ul></li><li><p>最后使用一个1×1的Conv和一个简单的双线性上采样来获得解析结果</p></li></ul><h3 id="experiments-1">Experiments</h3><ul><li>消融实验<ul><li><img src="/2020/10/25/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-0x03/16.png"><ul><li>将GINet与基于VisG的上下文推理方法GCU、GloRe进行了比较</li></ul></li><li><img src="/2020/10/25/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-0x03/17.png"></li><li><img src="/2020/10/25/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-0x03/18.png"><ul><li>通过引入语义图来促进可视化图上的推理，GINet获得了更准确的解析结果</li></ul></li></ul></li><li><img src="/2020/10/25/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-0x03/19.png"></li></ul>]]></content>
    
    
    <categories>
      
      <category>study</category>
      
    </categories>
    
    
    <tags>
      
      <tag>注意力机制</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>晚安~</title>
    <link href="/2020/10/20/%E6%99%9A%E5%AE%89/"/>
    <url>/2020/10/20/%E6%99%9A%E5%AE%89/</url>
    
    <content type="html"><![CDATA[<h1 id="晚安">晚安~🌙</h1><p>今天刷抖音的时候看到了最新一期Running Man的一个剪辑视频，宋智孝的一句晚安，真的说的我心都酥了~</p><p>也是有段时间没有再看RM了，但真心喜欢他们这一群人，晚安~🌙</p><span id="more"></span><video id="video" controls preload="none"><source id="mp4" src="https://www.hyzs1220.ga/晚安.mp4" type="video/mp4"></video>]]></content>
    
    
    <categories>
      
      <category>life</category>
      
    </categories>
    
    
    <tags>
      
      <tag>生活的美好</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>论文阅读-0x02</title>
    <link href="/2020/10/16/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-0x02/"/>
    <url>/2020/10/16/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-0x02/</url>
    
    <content type="html"><![CDATA[<p>两篇论文的阅读笔记：《Classification of Hyperspectral and LiDAR DataUsing Coupled CNNs》,IEEE TGRS 2020 &amp;《Hybrid noise removal inhyperspectral imagery with spatial-spectral gradient network》,IEEE TGRS2019 <a href="https://github.com/WHUQZhang">[代码]</a></p><span id="more"></span><h2 id="classification-of-hyperspectral-and-lidar-data-using-coupled-cnns">Classificationof Hyperspectral and LiDAR Data Using Coupled CNNs</h2><h3 id="introduction">Introduction</h3><ul><li><p>Hyperspectral and LiDAR Data：高光谱(HSI)和激光雷达数据</p></li><li><p>HSI图像具有丰富的光谱信息，但是对于同一材质的物体区分性较弱</p></li><li><p>LiDAR图像则具有丰富的深度信息</p><ul><li>例如：区域中存在混凝土结构的楼房和道路，HSI图像很难区分二者之间的差别（因为它们的光谱响应是相似的），但是LiDAR图像则可以通过深度信息区分出楼房和道路。</li><li>相反，激光雷达数据无法区分由不同材料(如沥青和混凝土)组成的具有相同高度的两条不同的道路。</li><li>论文使用两个耦合卷积神经网络（CNN）</li><li>一个用于从高光谱数据中学习光谱空间特征</li><li>另一个CNN用于从激光雷达数据中获取高程信息</li></ul></li><li><p>两个CNN网络都由三个卷积层组成，最后两个卷积层通过参数共享策略耦合在一起</p></li><li><p>在这方面，主要有两种相关方法在使用</p><ul><li>基于特征级融合：主要是一些特征图层次上的直接叠加，不过作者也说到会出现休斯现象，特别是在训练样本数量相对较少的情况下</li><li>为了解决这个问题，使用主成分分析(PCA)来降低维数</li><li>同时与本文相似，可以设计许多次空间相关模型来融合提取的光谱、空间和高程特征</li><li>基于决策级融合：主要是在最终的结果输出阶段进行融合，主要是分类器的选择和使用</li></ul></li><li><p>在这之前有很多相关工作：有人直接将激光雷达数据作为高光谱数据的另一个光谱波段，然后将连接起来的数据输入到CNN中进行特征学习和分类；或者多个CNN网络来训练等</p></li><li><p>论文主要贡献：</p><ul><li>设计了两个耦合的CNN，耦合的卷积层可以减少参数的数量，同时引导两个CNN相互学习，从而促进特征融合过程</li><li>在融合阶段，同时使用特征级和决策级融合策略。对于特征级融合，除了广泛采用的级联方法外，还提出了求和和最大化融合方法。为了增强学习功能的判别能力，分别向CNN添加了两个输出层。最后，这三个输出结果通过加权求和方法组合在一起，其权重由训练数据上每个输出的分类精度确定</li><li>使用标准训练和测试集在两个数据集上测试了该模型的有效性</li></ul></li></ul><h3 id="网络模型">网络模型</h3><p><img src="/2020/10/16/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-0x02/1.gif"></p><ul><li>网络主要包含两个部分：用于光谱空间特征学习的HS网络；用于高程特征学习的LiDAR网络。</li><li>每个模块包括一个输入模块、一个特征学习模块和一个融合模块</li><li>HS网络<ul><li>利用PCA减少原始高光谱数据的冗余信息，然后在给定像素周围提取一个小立方体</li></ul></li><li>LiDAR网络<ul><li>直接提取与高光谱数据空间位置相同的图像斑块</li></ul></li><li>可以看到，在网络的featurelearning模块中，使用了三层卷积层，最后两层共享参数。在融合模块中，构造了三个分类器。每个CNN都有一个输出层，融合后的特征也被送入一个输出层。</li></ul><p><img src="/2020/10/16/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-0x02/1.png"></p><ul><li>在第一层卷积之前，都先进行PAC操作来减少冗余信息</li><li>HS网络和LiDAR网络的第一层卷积是分别独立的</li><li>第二卷积层，我们让HS网络和激光雷达网络共享参数，使用耦合策略<ul><li>减少两次参数的数量，对于少量的训练样本是非常有用的</li><li>它可以让这两个网络互相学习<ul><li>在没有权值共享的情况下，各网络的训练参数将利用各自的损失函数独立优化</li><li>这一层的反向传播梯度将由两个网络的损耗函数决定，即一个网络中的信息将直接影响另一个网络</li><li>第三卷积层，也使用耦合策略，这可以进一步提高从第二卷积层学习到的表示的区分能力</li></ul></li></ul></li><li>数据融合<ul><li><img src="/2020/10/16/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-0x02/2.png"></li><li>在这里作者提出了一种基于特征层和决策层融合的新组合策略<ul><li>如图所示，首先结合两个特征生成一种新的特征表示</li><li>将这三个特征分别输入到输出层中</li><li>最后，所有的输出层集成在一起，以产生最终的结果</li></ul></li><li><span class="math inline">\(O = D[ f_1(R_h;W_1), f_2(R_l;W_2),f_3(F(R_h,R_l);W_3);U]\)</span></li><li>其中特征融合可以采用逐元素相加或者Max函数<ul><li><span class="math inline">\(F_3 = F_1 + F_2\)</span> 或者 <span class="math inline">\(F_3 = max(F_1 , F_2)\)</span></li></ul></li><li>对于决策级融合，采用加权求和的方法<ul><li><span class="math inline">\(O = D(\hat y_1, \hat y_2,\hat y_3;U) =u_1 \odot \hat y_1+ u_2 \odot \hat y_2+ u_3 \odot \hat y_3\)</span></li><li><span class="math inline">\(\odot\)</span>为加权操作</li></ul></li></ul></li><li>损失函数<ul><li>然后<span class="math inline">\(L_1\)</span>表示HSI图像 <span class="math inline">\(y_1\)</span> 的交叉熵损失，。所 <span class="math inline">\(L_2\)</span> 表示 LiDAR 图像 <span class="math inline">\(y_2\)</span> 的交叉熵损失，<span class="math inline">\(L_3\)</span>表示融合信息O的交叉熵损失，所以最终的损失函数为：<ul><li><span class="math inline">\(L = \lambda_1 L_1 + \lambda_2 L_2 +L_3\)</span></li><li>在实验中，权重参数根据经验将其设置为0.01</li></ul></li></ul></li></ul><h3 id="实验">实验</h3><ul><li>训练参数<ul><li>batch size = 64</li><li>learning rate = 0.001</li><li>epochs = 200</li></ul></li><li><img src="/2020/10/16/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-0x02/3.png"></li><li>休斯顿数据集上，使用不同模型下的准确率情况</li></ul><h2 id="hybrid-noise-removal-in-hyperspectral-imagery-with-spatial-spectral-gradient-network">Hybridnoise removal in hyperspectral imagery with spatial-spectral gradientnetwork</h2><h3 id="introduction-1">Introduction</h3><ul><li>本文提出了一种用于HSI中混合噪声去除的空间光谱梯度网络(SSGN)</li><li>考虑到稀疏噪声特有的空间结构方向性和光谱差异，加上补充信息，采用空间-光谱梯度学习策略，有效地提取了HSI的内在和深层特征</li><li>SSGN基于全级联的多尺度卷积网络，利用同一模型可以同时处理不同HSI或频谱中的不同类型的噪声</li><li>由于传感器的不稳定性和大气的干扰，HSI经常受到多种类型的噪声，如高斯噪声、条纹噪声、脉冲噪声、死线噪声和混合噪声</li><li>当前高光谱图像去噪：<ul><li>手动参数必须针对不同的HSI数据进行适当、仔细的调整，这对于不同的场景和HSI传感器造成了不方便、不通用、耗时的缺点</li><li>HSI的噪声同时存在于空间域和光谱域，其类型不同，水平也不同</li></ul></li><li>克服上述去噪方法的不足，充分利用DCNNs的优点，考虑高斯噪声、条纹噪声、脉冲噪声、死线及其混合的噪声类型，提出了一种用于混合噪声去噪的空间光谱梯度网络(SSGN)。主要的创新可以概括如下<ul><li>提出了一种用于HSI去噪的空间频谱卷积网络SSGN，将空间数据和相邻的光谱数据同时使用在全级联的多尺度卷积神经网络块中</li><li>该模型<strong>将空间梯度和光谱梯度结合在一起</strong><ul><li>利用空间梯度提取稀疏噪声在水平和垂直方向上的独特结构方向性</li><li>利用光谱梯度获取光谱附加互补信息用于噪声去除</li><li>同时在空间光谱损失函数中加入了光谱梯度，在整体框架内减小了光谱失真。</li></ul></li><li>实验结果表明，该方法能够通过单一模型有效地处理不同HSIs或光谱中的高斯噪声、条带噪声和混合噪声。与其他最新的HSI去噪算法相比，在不同混合噪声场景下，SSGN算法在评估指标、视觉评估和时间消耗方面都有较好的表现</li></ul></li></ul><h3 id="关于高光谱图像去噪">关于高光谱图像去噪</h3><ul><li>现有的HSI去噪方法大致可以分为两类<ul><li>基于过滤器的方法<ul><li>通过傅立叶变换、小波变换或非局部均值(NLM)滤波器等滤波操作从噪声信号中分离出干净的信号</li><li>作者举例了一些相关方法，不过指出这些滤波方法的存在缺点<ul><li>使用了手工制作的固定小波，对变换函数的选择很敏感，没有考虑杂波等高频干扰几何特性的差异</li></ul></li></ul></li><li>基于模型优化的方法<ul><li>有很多已经提出并使用的方法，如总变差、稀疏表示、低秩矩阵和张量模型<ul><li>都考虑了HSI数据的合理假设或先验。这种方法可以将有噪声的HSI映射到清晰的HSI，以保持空间和光谱特征</li></ul></li><li>对于HSI，相邻波段之间的高光谱相关性和一个波段内的高空间相似性都可以揭示HSI的低秩特性或张量结构<ul><li>为了充分利用3-D张量高光谱图像的光谱空间结构特性，人们提出了基于低阶张量的高光谱图像去噪方法，但是这些方法往往以较高的计算时间消耗为代价来获得更好的性能。</li></ul></li></ul></li><li>不同HSI去噪方法对混合噪声的不适应性和低效率问题仍然制约着HSI去噪的应用</li></ul></li></ul><h3 id="网络模型-1">网络模型</h3><ul><li>SSGN方法考虑了噪声结构特征、各频带的空间特性和频谱冗余<ul><li>通过在含噪声的HSI patch和去噪后的HSI patch之间进行端到端的学习</li><li>同时采用模拟的第k个噪声频带及其水平/垂直空间梯度和相邻谱梯度(Cube)作为输入数据，输出第k个噪声频带的残余噪声</li></ul></li></ul><p><img src="/2020/10/16/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-0x02/4.png"></p><ul><li>网络输入：<ul><li><ol type="a"><li>噪声下的当前输入空间谱带</li></ol></li><li><ol start="2" type="a"><li>输入空间谱带的水平和垂直梯度</li></ol></li><li><ol start="3" type="a"><li>前空间谱带相邻K个谱带的差异（Cube）</li></ol></li><li>其变换得到方式如下👇</li></ul></li><li>空间和光谱的联合梯度信息：<ul><li>空间波段的梯度信息由于其独特的结构方向性，在一定程度上可以有效地突出稀疏噪声，特别是稀疏分布的条带噪声</li><li>由于HSI数据包含数百个波段的丰富光谱信息，每个波段的噪声水平和类型通常不同。这些差异提供了额外的补充信息，有利于消除混合噪声</li><li>因此作者认为利用空间和光谱的联合梯度信息，可以从空间和光谱域去除HSI中的混合噪声，包括密集噪声和空闲噪声</li><li><img src="/2020/10/16/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-0x02/5.png"></li><li>其计算方式如下：</li><li><span class="math inline">\(\mathbf{G}_x (m,n,k) =\mathbf{Y}(m+1,n,k) - {\mathbf{Y}}(m,n,k)\)</span></li><li><span class="math inline">\(\mathbf{G}_y (m,n,k) ={\mathbf{Y}}(m,n+1,k)-{\mathbf{Y}}(m,n,k)\)</span></li><li><span class="math inline">\(\mathbf{G}_z (m,n,k) ={\mathbf{Y}}(m,n,k+1)-{\mathbf{Y}}(m,n,k)\)</span></li><li>其中 <span class="math inline">\(G_x,G_y,G_z\)</span>分别表示当前波段 <span class="math inline">\(Y_k\)</span>的<strong>水平空间梯度、垂直空间梯度和光谱梯度</strong></li><li>网络最终使用残差学习策略</li></ul></li><li>多尺度卷积块：<ul><li>通过不同尺寸的卷积核大小能偶获得不同的感受野大小。特别是在条带噪声和坏线的场景下效果十分明显</li><li><img src="/2020/10/16/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-0x02/6.png"></li><li>SSGN还采用了完全级联的多尺度卷积块来提取更多具有不同接收域大小的特征图。</li><li>随着层深的增加，不同块的结果逐渐接近最终的残差混合噪声，包括高斯噪声等稠密噪声和稀疏条纹噪声、死线等稀疏噪声</li></ul></li><li>三个输入经过多尺度卷积模块获得最终的输出，SSGN使用 光谱-空间损失函数进行优化</li><li>传统的图像去噪、超分辨任务都是使用欧几里得损失函数，但是这仅仅考虑了空间信息</li><li>为了在保持空间结构信息的同时抑制光谱失真，本文提出了一种新的损失函数</li><li><span class="math inline">\(\xi (\Theta)=(1-\alpha)\cdot \xi_{\mathrm spatial} +\alpha \cdot \xi _{\mathrm spectral}\)</span><ul><li><span class="math inline">\(\alpha  = 0.00\)</span>是权衡空间和光谱项之间的惩罚参数， 可以看出光谱损失占的权重很小</li></ul></li><li>其中 <span class="math inline">\(ξ _{spatial}\)</span> （空间） 和<span class="math inline">\(ξ _{spectral term}\)</span>（光谱）分别定义为<ul><li><span class="math inline">\(\xi _{\mathrm spatial} = \frac 1{2T}\sum _{i=1}^T {\big \| \boldsymbol{ Res _k^i -({\mathbf Y}_k^i-{\mathbf X}_k^i)} \big \|_2^2}\)</span></li><li><span class="math inline">\(\xi _{\mathrm spectral} = \frac 1{2T}\sum _{i=1}^T {\sum _{z=k-\frac K 2,z\ne k}^{k+\frac K 2} {\big \|{\Phi _z^i -{\mathbf G}_z^i} \big \|_2^2}}\)</span></li></ul></li></ul><h3 id="实验-1">实验</h3><ul><li>去噪前，将各HSI波段的灰度值独立归一化为[0-1]</li><li>模拟实验采用平均峰值信噪比(MPSNR)、平均结构相似度指数(MSSIM)和平均谱角(MSA)作为评价指标</li><li>一般来说，在模拟实验中，较高的MPSNR和MSSIM值和较低的MSA值可以反映出较好的HSI去噪效果</li><li>整个网络学习率被初始化为0.001。每过10个epoch，通过乘以0.5的下降因子来降低学习率</li><li>采用Adam优化算法，动量参数分别为0.9、0.999和<span class="math inline">\(10^{−8}\)</span></li><li>后面作者对实验结果进行了很充分的分析，而且比较了加入各种手工混合噪声对实验结构的影响，而且时间效率很高。</li></ul><p><img src="/2020/10/16/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-0x02/7.png"></p>]]></content>
    
    
    <categories>
      
      <category>study</category>
      
    </categories>
    
    
    <tags>
      
      <tag>深度学习</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>CBDNt&amp;卷积上采样</title>
    <link href="/2020/10/15/CBDNet-%E5%8D%B7%E7%A7%AF%E4%B8%8A%E9%87%87%E6%A0%B7/"/>
    <url>/2020/10/15/CBDNet-%E5%8D%B7%E7%A7%AF%E4%B8%8A%E9%87%87%E6%A0%B7/</url>
    
    <content type="html"><![CDATA[<p>这篇论文其实自己之前有学习过，不过没有整理，所以在记录整理一下，并且在看实现代码的过程中，对上采样也有了新的学习了解，并做一下记录。</p><p>《 Toward Convolutional Blind Denoising of Real Photographs 》，<em>CVPR</em> 2019</p><span id="more"></span><h2 id="cbdnet">CBDNet</h2><p>论文主要在以下四个部分做出了创新改进：</p><ul><li>论文不再单纯针对高斯噪声进行去噪，而是主要针对真实噪声，考虑了信号依赖噪声和ISP流程对噪声的影响，展示了图像噪声模型在真实噪声图像中起着关键作用，使用高斯-泊松模型来进行噪声模拟。</li><li>提出了CBDNet模型，其包括了一个噪声估计子网络和一个非盲去噪子网络，可以实现图像的盲去噪（即未知噪声水平）</li><li>提出了非对称学习（asymmetriclearning）的损失函数，并允许用户交互式调整去噪结果，增强了去噪结果的鲁棒性</li><li>将合成噪声图像与真实噪声图像一起用于网络的训练，提升网络的去噪效果和泛化能力</li></ul><h3 id="真实噪声模型">真实噪声模型</h3><ul><li><p>真实噪声分布可以用异方差高斯近似 <span class="math inline">\(n(L)∼N(0,σ^2(L))\)</span></p><ul><li><span class="math inline">\({\sigma ^2}({\mathbf{L}}) = {\mathbf{L}}\cdot \sigma _s^2 + \sigma _c^2\)</span></li></ul></li><li><p>其中L为辐照图像。<span class="math inline">\(n(L) = n_s(L) +n_c\)</span>, 包含了信号依赖噪声部分 <span class="math inline">\(n_s\)</span> 和平稳噪声部分 <span class="math inline">\(n_c\)</span> 。其中 <span class="math inline">\(n_c\)</span>常常建模为方差为 <span class="math inline">\(\sigma _s^2\)</span> 的高斯白噪声，而 <span class="math inline">\(n_s\)</span> 常常与图像亮度有关</p></li><li><p>再次基础上进一步考虑ISP流程</p><ul><li><span class="math inline">\(y = f( { \mathbf{DM} } ( {\mathbf{L}} +n({\mathbf{L} )} ) )\)</span></li></ul></li><li><p>y 就是我们需要的合成噪声图像， f(⋅) 代表了相机响应函数（CRF）, $L = Mf^{−1}(x) $ 从干净图像生成辐照图像， M(⋅)表示将sRGB图像转化为Bayer图像 ， DM(•) 实现了去除马赛克功能。</p></li><li><p>去马赛克函数中的线性插值运算涉及到了不同颜色通道的像素，所以合成的噪声是通道依赖的</p></li><li><p>再考虑JPEG压缩</p><ul><li><span class="math inline">\(y = JPEG( f({\mathbf{DM}}( {\mathbf{L}}+ {\mathbf{n}}({\mathbf{L}})) ) )\)</span></li></ul></li></ul><h3 id="网络模型">网络模型</h3><p><img src="/2020/10/15/CBDNet-%E5%8D%B7%E7%A7%AF%E4%B8%8A%E9%87%87%E6%A0%B7/1.jpg"></p><ul><li>CBDNet包含了两个子网络：噪声估计子网络和非盲去噪子网络</li><li>噪声估计子网<ul><li>将噪声观测图像y转换为估计的噪声水平图 <span class="math inline">\(\hatσ(y)\)</span></li><li>使用五层全卷积网络，卷积核为3×3×32，使用了1个像素的padding来保证尺寸一致,并且不进行pooling和batchnormalization</li></ul></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">FCN</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-built_in">super</span>(FCN, self).__init__()<br><br>        <span class="hljs-comment"># 3 ==&gt; 32 的输入卷积</span><br>        self.inc = nn.Sequential(<br>            nn.Conv2d(<span class="hljs-number">3</span>, <span class="hljs-number">32</span>, <span class="hljs-number">3</span>, padding=<span class="hljs-number">1</span>),<br>            nn.ReLU(inplace=<span class="hljs-literal">True</span>))<br>    <br>        <span class="hljs-comment"># 32 ==&gt; 32 的中间卷积</span><br>        self.conv = nn.Sequential(<br>            nn.Conv2d(<span class="hljs-number">32</span>, <span class="hljs-number">32</span>, <span class="hljs-number">3</span>, padding=<span class="hljs-number">1</span>),<br>            nn.ReLU(inplace=<span class="hljs-literal">True</span>)<br>        )<br>    <br>        <span class="hljs-comment"># 32 ==&gt; 3 的输出卷积 </span><br>        self.outc = nn.Sequential(<br>            nn.Conv2d(<span class="hljs-number">32</span>, <span class="hljs-number">3</span>, <span class="hljs-number">3</span>, padding=<span class="hljs-number">1</span>),<br>            nn.ReLU(inplace=<span class="hljs-literal">True</span>)<br>        )<br>  <br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x</span>):<br>        <span class="hljs-comment"># 第 1 次卷积</span><br>        conv1 = self.inc(x)<br>        <span class="hljs-comment"># 第 2 次卷积</span><br>        conv2 = self.conv(conv1)<br>        <span class="hljs-comment"># 第 3 次卷积</span><br>        conv3 = self.conv(conv2)<br>        <span class="hljs-comment"># 第 4 次卷积</span><br>        conv4 = self.conv(conv3)<br>        <span class="hljs-comment"># 第 5 次卷积</span><br>        conv5 = self.outc(conv4)<br>        <span class="hljs-keyword">return</span> conv5<br></code></pre></td></tr></table></figure><ul><li>非盲去噪子网<ul><li>将 y 和 <span class="math inline">\(\hatσ(y)\)</span>作为输入得到最终的去噪结果 <span class="math inline">\(\hatx\)</span></li><li>除此之外，噪声估计子网络允许用户在估计的噪声水平图 <span class="math inline">\(\hatσ(y)\)</span>输入到非盲去噪子网络之前对应进行调整</li><li>使用16层的U-Net结构，且使用残差学习的方式学习残差映射，从而得到干净的图像</li><li>并且实验发现，<strong>BN对真实噪声的去除帮助微乎其微</strong>。可能的原因是真实噪声非高斯分布。</li></ul></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br></pre></td><td class="code"><pre><code class="hljs python"><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">single_conv</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, in_ch, out_ch</span>):<br>        <span class="hljs-built_in">super</span>(single_conv, self).__init__()<br>        self.conv = nn.Sequential(<br>            nn.Conv2d(in_ch, out_ch, <span class="hljs-number">3</span>, padding=<span class="hljs-number">1</span>),<br>            nn.ReLU(inplace=<span class="hljs-literal">True</span>))<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x</span>):<br>        x = self.conv(x)<br>        <span class="hljs-keyword">return</span> x<br>  <br><span class="hljs-comment"># 上采样</span><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">up</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, in_ch</span>):<br>        <span class="hljs-built_in">super</span>(up, self).__init__()<br>        self.up = nn.ConvTranspose2d(in_ch, in_ch//<span class="hljs-number">2</span>, <span class="hljs-number">2</span>, stride=<span class="hljs-number">2</span>)<br><br>    <span class="hljs-comment"># forward 需要两个输入，x1 是需要上采样的小尺寸 feature map</span><br>    <span class="hljs-comment"># x2 是以前的大尺寸 feature map，因为中间的 pooling 可能损失了边缘像素，</span><br>    <span class="hljs-comment"># 所以上采样以后的 x1 可能会比 x2 尺寸小</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x1, x2</span>):<br>        <span class="hljs-comment"># x1 上采样</span><br>        x1 = self.up(x1)<br>    <br>        <span class="hljs-comment"># 输入数据是四维的，第一个维度是样本数，剩下的三个维度是 CHW</span><br>        <span class="hljs-comment"># 所以 Y 方向上的悄寸差别在 [2],  X 方向上的尺寸差别在 [3] </span><br>        diffY = x2.size()[<span class="hljs-number">2</span>] - x1.size()[<span class="hljs-number">2</span>]<br>        diffX = x2.size()[<span class="hljs-number">3</span>] - x1.size()[<span class="hljs-number">3</span>]<br>        <span class="hljs-comment"># 给 x1 进行 padding 操作</span><br>        x1 = F.pad(x1, (diffX // <span class="hljs-number">2</span>, diffX - diffX//<span class="hljs-number">2</span>,<br>                        diffY // <span class="hljs-number">2</span>, diffY - diffY//<span class="hljs-number">2</span>))<br>        <span class="hljs-comment"># 把 x2 加到反卷积后的 feature map</span><br>        x = x2 + x1<br>        <span class="hljs-keyword">return</span> x<br>  <br><span class="hljs-keyword">class</span> <span class="hljs-title class_">outconv</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, in_ch, out_ch</span>):<br>        <span class="hljs-built_in">super</span>(outconv, self).__init__()<br>        self.conv = nn.Conv2d(in_ch, out_ch, <span class="hljs-number">1</span>)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x</span>):<br>        x = self.conv(x)<br>        <span class="hljs-keyword">return</span> x<br>  <br>  <br><span class="hljs-keyword">class</span> <span class="hljs-title class_">UNet</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-built_in">super</span>(UNet, self).__init__()<br><br>        self.inc = nn.Sequential(<br>            single_conv(<span class="hljs-number">6</span>, <span class="hljs-number">64</span>),<br>            single_conv(<span class="hljs-number">64</span>, <span class="hljs-number">64</span>))<br><br>        self.down1 = nn.AvgPool2d(<span class="hljs-number">2</span>)<br>        self.conv1 = nn.Sequential(<br>            single_conv(<span class="hljs-number">64</span>, <span class="hljs-number">128</span>),<br>            single_conv(<span class="hljs-number">128</span>, <span class="hljs-number">128</span>),<br>            single_conv(<span class="hljs-number">128</span>, <span class="hljs-number">128</span>))<br><br>        self.down2 = nn.AvgPool2d(<span class="hljs-number">2</span>)<br>        self.conv2 = nn.Sequential(<br>            single_conv(<span class="hljs-number">128</span>, <span class="hljs-number">256</span>),<br>            single_conv(<span class="hljs-number">256</span>, <span class="hljs-number">256</span>),<br>            single_conv(<span class="hljs-number">256</span>, <span class="hljs-number">256</span>),<br>            single_conv(<span class="hljs-number">256</span>, <span class="hljs-number">256</span>),<br>            single_conv(<span class="hljs-number">256</span>, <span class="hljs-number">256</span>),<br>            single_conv(<span class="hljs-number">256</span>, <span class="hljs-number">256</span>))<br><br>        self.up1 = up(<span class="hljs-number">256</span>)<br>        self.conv3 = nn.Sequential(<br>            single_conv(<span class="hljs-number">128</span>, <span class="hljs-number">128</span>),<br>            single_conv(<span class="hljs-number">128</span>, <span class="hljs-number">128</span>),<br>            single_conv(<span class="hljs-number">128</span>, <span class="hljs-number">128</span>))<br><br>        self.up2 = up(<span class="hljs-number">128</span>)<br>        self.conv4 = nn.Sequential(<br>            single_conv(<span class="hljs-number">64</span>, <span class="hljs-number">64</span>),<br>            single_conv(<span class="hljs-number">64</span>, <span class="hljs-number">64</span>))<br><br>        self.outc = outconv(<span class="hljs-number">64</span>, <span class="hljs-number">3</span>)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x</span>):<br>        <span class="hljs-comment"># input conv : 6 ==&gt; 64 ==&gt; 64</span><br>        inx = self.inc(x)<br><br>        <span class="hljs-comment"># 均值 pooling, 然后 conv1 : 64 ==&gt; 128 ==&gt; 128 ==&gt; 128</span><br>        down1 = self.down1(inx)<br>        conv1 = self.conv1(down1)<br><br>        <span class="hljs-comment"># 均值 pooling，然后 conv2 : 128 ==&gt; 256 ==&gt; 256 ==&gt; 256 ==&gt; 256 ==&gt; 256 ==&gt; 256</span><br>        down2 = self.down2(conv1)<br>        conv2 = self.conv2(down2)<br><br>        <span class="hljs-comment"># up1 : conv2 反卷积，和 conv1 的结果相加，输入256，输出128</span><br>        up1 = self.up1(conv2, conv1)<br>        <span class="hljs-comment"># conv3 : 128 ==&gt; 128 ==&gt; 128 ==&gt; 128</span><br>        conv3 = self.conv3(up1)<br><br>        <span class="hljs-comment"># up2 : conv3 反卷积，和 input conv 的结果相加，输入128，输出64</span><br>        up2 = self.up2(conv3, inx)<br>        <span class="hljs-comment"># conv4 : 64 ==&gt; 64 ==&gt; 64</span><br>        conv4 = self.conv4(up2)<br><br>        <span class="hljs-comment"># output conv: 65 ==&gt; 3，用1x1的卷积降维，得到降噪结果</span><br>        out = self.outc(conv4)<br>        <span class="hljs-keyword">return</span> out<br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">CBDNet</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-built_in">super</span>(CBDNet, self).__init__()<br>        self.fcn = FCN()<br>        self.unet = UNet()<br>  <br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x</span>):<br>        noise_level = self.fcn(x)<br>        <span class="hljs-comment"># 将 3通道的原图像，和 noise_level 拼接在一起，作为 UNet 的输入</span><br>        concat_img = torch.cat([x, noise_level], dim=<span class="hljs-number">1</span>)<br>        out = self.unet(concat_img) + x<br>        <span class="hljs-keyword">return</span> noise_level, out<br></code></pre></td></tr></table></figure><h3 id="非对称学习asymmetric-learning">非对称学习（AsymmetricLearning）</h3><ul><li><p>作者观察到非盲去噪方法（如BM3D、FFDNet等）对噪声估计的误差具有非对称敏感性，即非盲去噪方法对低估误差比较敏感，而对高估的误差比较鲁棒</p><ul><li>当输入噪声的标准差与真实噪声的标准差一致时，去噪效果最好</li><li>当输入噪声标准差低于真实值时，去噪结果包含可察觉的噪声</li><li>而当输入噪声标准差高于真实值时，去噪结果仍能保持较好的结果，虽然也平滑了部分低对比度的纹理。</li></ul></li><li><p>为了消除这种非对称敏感性，设计了非对称损失函数。</p><ul><li>给定像素 <span class="math inline">\(i\)</span> 的估计噪声水平 <span class="math inline">\(\hatσ(y_i)\)</span> 和真实值 <span class="math inline">\(σ(y_i)\)</span> ，当 <span class="math inline">\(\hatσ(y_i) &lt; σ(y_i)\)</span>时，应该对其MSE引入更多的惩罚</li><li><span class="math inline">\({\mathcal{L} _{asymm}} = \sum\limits_i{| {\alpha - {\mathbb{I}_{( {\hat \sigma ( {y_i} ) - \sigma ( {y_i} )} )&lt; 0}}} | \cdot {( {\hat \sigma ( {y_i} ) - \sigma ( {y_i} )})^2}}\)</span></li><li>当 e&lt;0 时，<span class="math inline">\(\mathbb{I}_e\)</span> =1，否则为0。通过设定0&lt;α&lt;0.5，我们可以对低估误差引入更多的惩罚</li></ul></li><li><p>引入全变分（TV）正则项约束 <span class="math inline">\(σ(y_i)\)</span> 的平滑性</p><ul><li><span class="math inline">\({\mathcal{L}_{\text{TV}}} = \| {\nabla_h\hat \sigma (\mathbf{y})} \|_2^2 + \| {\nabla _v\hat \sigma(\mathbf{y})} \|_2^2\)</span></li><li><span class="math inline">\(\nabla _h\)</span>为其中沿水平(垂直)方向的梯度算子</li><li>对于去噪网络输入的 <span class="math inline">\(\hatx\)</span>，定义重建误差</li><li><span class="math inline">\({\mathcal{L}_{rec}} = \| { \mathbf{\hatx} - \mathbf{x}} \|_2^2\)</span></li><li>综上所述，整个CBDNet的目标损失函数为</li><li><span class="math inline">\(\mathcal{L} = \mathcal{L}_{rec}  +\lambda _{asymm} \mathcal{L} _{asymm} + \lambda _{TV} \mathcal{L}_{TV}\)</span></li></ul></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">fixed_loss</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-built_in">super</span>().__init__()<br>    <br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, out_image, gt_image, est_noise, gt_noise, if_asym</span>):<br>        <span class="hljs-comment"># 分别得到图像的高度和宽度</span><br>        h_x = est_noise.size()[<span class="hljs-number">2</span>]<br>        w_x = est_noise.size()[<span class="hljs-number">3</span>]<br>        <span class="hljs-comment"># 每个样本为 CHW ，把 H 方向第一行的数据去掉，统计一下一共多少元素</span><br>        count_h = self._tensor_size(est_noise[:, :, <span class="hljs-number">1</span>:, :])<br>        <span class="hljs-comment"># 每个样本为 CHW ，把 W 方向第一列的数据去掉，统计一下一共多少元素</span><br>        count_w = self._tensor_size(est_noise[:, :, : ,<span class="hljs-number">1</span>:])<br>        <span class="hljs-comment"># H 方向，第一行去掉得后的矩阵，减去最后一行去掉后的矩阵，即下方像素减去上方像素，平方，然后求和</span><br>        h_tv = torch.<span class="hljs-built_in">pow</span>((est_noise[:, :, <span class="hljs-number">1</span>:, :] - est_noise[:, :, :h_x-<span class="hljs-number">1</span>, :]), <span class="hljs-number">2</span>).<span class="hljs-built_in">sum</span>()<br>        <span class="hljs-comment"># W 方向，第一列去掉得后的矩阵，减去最后一列去掉后的矩阵，即右方像素减去左方像素，平方，然后求和</span><br>        w_tv = torch.<span class="hljs-built_in">pow</span>((est_noise[:, :, :, <span class="hljs-number">1</span>:] - est_noise[:, :, :, :w_x-<span class="hljs-number">1</span>]), <span class="hljs-number">2</span>).<span class="hljs-built_in">sum</span>()<br>        <span class="hljs-comment"># 求平均，得到平均每个像素上的 tvloss</span><br>        tvloss = h_tv / count_h + w_tv / count_w<br><br>        loss = torch.mean( \<br>                <span class="hljs-comment"># 第三部分：重建损失</span><br>                torch.<span class="hljs-built_in">pow</span>((out_image - gt_image), <span class="hljs-number">2</span>)) + \<br>                <span class="hljs-comment"># 第一部分：对比损失</span><br>                if_asym * <span class="hljs-number">0.5</span> * torch.mean(torch.mul(torch.<span class="hljs-built_in">abs</span>(<span class="hljs-number">0.3</span> - F.relu(gt_noise - est_noise)), torch.<span class="hljs-built_in">pow</span>(est_noise - gt_noise, <span class="hljs-number">2</span>))) + \<br>                <span class="hljs-comment"># 第二部分：起平滑作用的 tvloss</span><br>                <span class="hljs-number">0.05</span> * tvloss<br>        <span class="hljs-keyword">return</span> loss<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">_tensor_size</span>(<span class="hljs-params">self,t</span>):<br>        <span class="hljs-keyword">return</span> t.size()[<span class="hljs-number">1</span>]*t.size()[<span class="hljs-number">2</span>]*t.size()[<span class="hljs-number">3</span>]<br></code></pre></td></tr></table></figure><h3 id="实验">实验</h3><ul><li>数据集： NC12、DND和·Nam</li><li>损失函数：<span class="math inline">\(α=0.3 , λ_1=0.5 ,λ_2=0.05\)</span></li><li>使用ADAM算法训练网络：<span class="math inline">\(β_1=0.9\)</span></li><li>最小batch的大小为32，图像块大小为128×128</li><li>所有模型训练 40epochs，前20epochs使用 <span class="math inline">\(10^{−3}\)</span> 学习率，然后使用 <span class="math inline">\(5 \times 10^{−4}\)</span> 的学习率finetune模型</li></ul><h2 id="卷积上采样">卷积上采样</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">torch</span>.nn.ConvTranspose2d(in_channels, out_channels, kernel_size, stride=<span class="hljs-number">1</span>, padding=<span class="hljs-number">0</span>, output_padding=<span class="hljs-number">0</span>, groups=<span class="hljs-number">1</span>, bias=<span class="hljs-literal">True</span>, dilation=<span class="hljs-number">1</span>)<br><br><span class="hljs-comment"># in_channels(int) – 输入信号的通道数</span><br><span class="hljs-comment"># out_channels(int) – 卷积产生的通道数</span><br><span class="hljs-comment"># kerner_size(int or tuple) - 卷积核的大小</span><br><span class="hljs-comment"># stride(int or tuple,optional) - 卷积步长，即要将输入扩大的倍数。</span><br><span class="hljs-comment"># padding(int or tuple, optional) - 输入的每一条边补充0的层数，高宽都增加2*padding</span><br><span class="hljs-comment"># output_padding(int or tuple, optional) - 输出边补充0的层数，高宽都增加padding</span><br><span class="hljs-comment"># groups(int, optional) – 从输入通道到输出通道的阻塞连接数</span><br><span class="hljs-comment"># bias(bool, optional) - 如果bias=True，添加偏置</span><br><span class="hljs-comment"># dilation(int or tuple, optional) – 卷积核元素之间的间距</span><br></code></pre></td></tr></table></figure><ul><li>卷积操作<ul><li><img src="/2020/10/15/CBDNet-%E5%8D%B7%E7%A7%AF%E4%B8%8A%E9%87%87%E6%A0%B7/2.png"></li><li><strong>计算公式</strong><ul><li><span class="math inline">\(out = (in - kernel + 2 \times padding) /stride + 1\)</span></li></ul></li></ul></li><li>反卷积<ul><li><p><strong>计算公式</strong></p><ul><li><span class="math inline">\(H_{out} = (H_{in} - 1) \times stride - 2\times padding + kernelsize - 2 \times padding\)</span></li></ul></li><li><p>当stride=1时，就不会进行插值操作，只会进行padding</p></li><li><p><img src="/2020/10/15/CBDNet-%E5%8D%B7%E7%A7%AF%E4%B8%8A%E9%87%87%E6%A0%B7/3.png"></p></li><li><p>首先正向卷积计算，4x4 输入，卷积核 3x3，stride=1，得到 2x2。所以padding=0</p></li><li><p>计算可以得到输出为 <span class="math inline">\((2-1) \times 1 -2\times 0 + 3 +0 = 4\)</span></p></li><li><p>可以看到上面在进行反卷积的时候进行了padding操作</p></li><li><p><span class="math inline">\(padding_{new} = kernel\\_size -padding -1 = 3 -0 -1 = 2\)</span></p></li><li><p>所以下方大小变为 <span class="math inline">\(H_{in} + 2 \timespadding_{new} = 2+2 \times 2=6\)</span></p></li><li><p>当stride=2时，进行插值和padding操作</p></li><li><p><img src="/2020/10/15/CBDNet-%E5%8D%B7%E7%A7%AF%E4%B8%8A%E9%87%87%E6%A0%B7/4.png"></p></li><li><p>正向计算，5x5输入，卷积核 3x3，stride=2，得到 3x3。所以padding=1</p></li><li><p>计算可以得到输出为 <span class="math inline">\((3-1) \times 2 -2\times 1 + 3 +0 = 5\)</span></p></li><li><p><span class="math inline">\(H_{in\_new} = H_{in} + (stride-1)\times (H_{in}-1) = 3 + (2-1) \times (3-1) = 5\)</span></p></li><li><p><span class="math inline">\(padding_{new} = kernel\\_size -padding -1 = 3 -1 -1 = 1\)</span></p></li><li><p>所以下方大小变为 <span class="math inline">\(H_{in\_ new} + 2\times padding_{new} = 5+2 \times 1=7\)</span></p></li></ul></li><li>其实没有特别理解，不过大概知道是怎么计算了，也是记录一下，以后可以翻出来再看看</li><li>主要还是需要记住计算公式！！</li></ul>]]></content>
    
    
    <categories>
      
      <category>study</category>
      
    </categories>
    
    
    <tags>
      
      <tag>深度学习</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>论文阅读-0x01</title>
    <link href="/2020/09/28/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-0x01/"/>
    <url>/2020/09/28/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-0x01/</url>
    
    <content type="html"><![CDATA[<p>两篇论文的阅读笔记：《Non-local Neutral Networks》&amp;《Learning inthe Frequency Domain》</p><span id="more"></span><h2 id="non-local-neutral-networks">Non-local Neutral Networks</h2><h3 id="abstract">Abstract</h3><ul><li>传统卷积操作和循环操作都是针对一个局部图像邻域进行操作，以卷积操作为例，它的感受野大小就是卷积核大小，只考虑局部区域</li><li>所以作者在本文中提出了一种非局部的方式来捕获远程依赖关系。<ul><li>将某个位置的响应由所有位置的特征的<strong>加权平均</strong>计算得到</li><li>对于顺序数据（语音，文本）：使用循环操作来建立远程依赖模式</li><li>对于图像数据：通过深层卷积运算形成接收场来模拟长距离依赖</li><li>上述两种处理模式：都是通过不断重复，通过数据传播信号，捕获远程依赖关系</li></ul></li><li>作者提出的非局部运算是计算机视觉中经典的非局部均值运算（Non-localmeans）的一种概括</li><li>可以将其添加到深度神经网络的较早部分</li><li>优点：<ul><li>可以直接计算任意两个位置之间的相互作用来捕获远程依赖关系</li><li>即使只有很少的层数，依然有效，并能达到最佳的效果</li><li>可以保持可变的输入大小，并且可以轻松的与其他操作结合使用</li></ul></li></ul><h3 id="non-local-neutral-networks-1">Non-local Neutral Networks</h3><ul><li>公式定义：<ul><li><span class="math inline">\(y_i = \frac{1}{C(x)}\sum\limits_{\forall j} f(x_i, y_j)g(x_j)\)</span></li><li>i为要计算其相应输出位置的索引，j是枚举所有可能位置的索引</li><li>x为输入信号（文本、图像或特征图等），y是与x大小相同的输出信号</li><li>成对函数f，计算i与所有j之间的标量（表示关联程度，相似度）</li><li>g计算位置j处输入信号的表示形式（线性映射，实际网络中用一个1x1卷积实现）</li><li>C(x)进行归一化</li><li>和全连接不同，fc仅是使用学习的权重，而不是使用函数；另外该公式支持可变大小的输入，并在输出中保持相应的大小，而fc需要固定大小，并且会丢失相应位置的对应关系（例如位置i处<span class="math inline">\(x_i\)</span>到<span class="math inline">\(y_i\)</span>）</li><li><img src="/2020/09/28/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-0x01/3.jpg"><ul><li>类似attention的实现，为了计算输出层的一个点，需要将输入的每个点都考虑一遍</li></ul></li><li><img src="/2020/09/28/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-0x01/1.png"><ul><li>论文实验结果表明，f的不同选择对Non-local的结果没有太大影响，表明Non-local这种行为本身才是提升模型的关键因素</li><li>其中g进行了一个矩阵相乘，也就是一个空间编码</li><li>f则是遵循非局部均值和双边滤波</li></ul></li></ul></li><li>Non-local Block<ul><li><img src="/2020/09/28/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-0x01/2.png"></li><li><span class="math inline">\(y_i\)</span>是上面的Non-local操作，然后给一个权重矩阵<span class="math inline">\(W_z\)</span>，再加上残差结构<span class="math inline">\(x_i\)</span></li><li>Non-loca可以转化为矩阵运算乘法+卷积运算，对比for 循环提升效率</li></ul></li><li>与全连接层的关系<ul><li>Non-local Block利用两个点的相似性对每个位置的特征做加权</li><li>全连接层则是利用position-related的weight对每个位置做加权</li><li>任意两点的相似性仅跟两点的位置索引有关，即 <img src="https://www.zhihu.com/equation?tex=f%28%5Cmathbb%7Bx%7D_i%2C+%5Cmathbb%7Bx%7D_j%29%3Dw_%7Bij%7D" alt="[公式]"></li><li>g是identity函数， <img src="https://www.zhihu.com/equation?tex=g%28%5Cmathbb%7Bx%7D_i%29%3D%5Cmathbb%7Bx%7D_i" alt="[公式]"></li><li>归一化系数为1。归一化系数跟输入无关，全连接层不能处理任意尺寸的输入</li></ul></li></ul><h2 id="learning-in-the-frequency-domain">Learning in the FrequencyDomain</h2><h3 id="abstract-1">Abstract</h3><ul><li>传统CNN网络在空间域上进行操作，受到显存限制，输入图像不能过大。不然会在预处理阶段使用下采样，损失很多图像信息</li><li>论文提出了一种方法：将RGB图像在CPU上进行预处理，首先转换到YCbCr颜色空间，然后转换为频域（DCT，离散余弦变换）。这样同一频率的所有分量都被分组到一个channel中，生成了多个channel，而且中某些channel的权重更大，因此在其中进行筛选。</li><li><img src="/2020/09/28/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-0x01/4.jpg"></li><li>论文证明了频域中的输入特征可以以最小的修改应用于空间域中开发的所有现有CNN模型</li><li>只需要删除CNN的输入层部分，然后保留网络后面部分的残差block。</li><li>将第一个残差层作为输入层的下一部分，并且需要修改输入通道的数量以适合DCT系数输入的尺寸。‘</li><li>这样，修改后的模型可以保持与原始模型相似的参数计数和计算复杂度。</li><li><img src="/2020/09/28/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-0x01/5.jpg"></li><li>移除了传统ResNet-50中的三个输入层（灰色虚线框），以接受56×56×64DCT输入</li></ul><h3 id="pre--treatment">pre- treatment</h3><ul><li>在预处理中，传统的预处理流程和增强流程依旧可以正常使用（包括图像大小调整，裁剪和翻转 等）</li><li>DCT transform：然后将图像转换为YCbCr颜色空间，也就是频域</li><li>DCTreshape：之后，将相同频率的二维DCT系数分组到一个channel，以形成三维DCT立方体</li><li>DCT channelselect：通过<strong>通道选择</strong>，选择了影响较大的频道的子集</li><li>该功能模块叫dynamic gatemodel，实际上就是<strong>SE-Net中提出的SE-Block</strong></li><li>DCT concatenate ：YCbCr颜色空间中的选定channel被concat在一起以形成一个张量</li><li>DCT normalize：最后，通过从训练数据集计算出的均值和方差对每个channel进行归一化</li><li><img src="/2020/09/28/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-0x01/6.jpg"></li></ul><h3 id="experiments">Experiments</h3><ul><li>与高频频道（索引较大的框）相比，低频频道（索引较小的框）的选择频率更高。这表明对于视觉推理任务而言，低频通道通常比高频通道更具信息性</li><li>与色度分量Cb和Cr中的频道相比，模型更频繁地选择亮度分量Y中的频道。这表明亮度分量对于视觉推理任务更具参考价值</li><li>热图在分类和分割任务之间共享一个公共模式。这表明上述两个观察结果并非特定于一项任务，很可能对更高层次的视觉任务具有普遍性</li><li>这些观察结果暗示CNN模型可能确实表现出与人类视觉类似的特征，并且针对人眼的图像压缩标准（例如JPEG）也可能适用于CNN模型。</li></ul>]]></content>
    
    
    <categories>
      
      <category>study</category>
      
    </categories>
    
    
    <tags>
      
      <tag>深度学习</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>SENet&amp;语义分割相关知识学习</title>
    <link href="/2020/08/10/SENet-%E8%AF%AD%E4%B9%89%E5%88%86%E5%89%B2%E7%9B%B8%E5%85%B3%E7%9F%A5%E8%AF%86%E5%AD%A6%E4%B9%A0/"/>
    <url>/2020/08/10/SENet-%E8%AF%AD%E4%B9%89%E5%88%86%E5%89%B2%E7%9B%B8%E5%85%B3%E7%9F%A5%E8%AF%86%E5%AD%A6%E4%B9%A0/</url>
    
    <content type="html"><![CDATA[<p>对上一次学习的 HybridSN高光谱分类网络进行优化改进；SENet网络学习和实现；学习视频北京大学李夏的<a href="https://www.bilibili.com/video/BV11E411y7Dr">《语义分割中的自注意力机制和低秩重重建》</a>， 南开大学程明明教授的<a href="https://www.bilibili.com/video/BV1Qz4y197Sj">《图像语义分割前沿进展》</a></p><span id="more"></span><h2 id="hybridsn-高光谱分类网络的优化改进">HybridSN高光谱分类网络的优化改进</h2><h3 id="关于dropout使用">关于DropOut使用</h3><ul><li>在上一次的实验代码中，因为使用了DropOut，所以需要对应使用net.train()和net.eval()函数<ul><li>model.train() 让model变成训练模式，此时 dropout和batchnormalization的操作在训练过程中发挥作用，防止网络过拟合的问题</li><li>net.eval()： 把BN和DropOut固定住，不会取平均，而是用训练好的值<ul><li>这样的话，在测试过程中，由于网络参数都已经固定，所以每次的测试结果也都会保持一致</li></ul></li></ul></li><li>准确率在95.5%左右</li></ul><h3 id="模型改进先使用二位卷积在使用三位卷积">模型改进——先使用二位卷积，在使用三位卷积</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 模型改进——先使用二位卷积，在使用三位卷积</span><br>class_num = <span class="hljs-number">16</span><br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">HybridSN</span>(nn.Module):<br><br>  <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self</span>):<br>    <span class="hljs-built_in">super</span>(HybridSN, self).__init__()<br><span class="hljs-comment"># 二维卷积：原始输入（30, 25, 25） 64个 3x3x30 的卷积核，得到 （64, 23, 23）</span><br>    self.conv4_2d = nn.Sequential(<br>        nn.Conv2d(<span class="hljs-number">30</span>,<span class="hljs-number">64</span>,(<span class="hljs-number">3</span>,<span class="hljs-number">3</span>)),<br>        nn.BatchNorm2d(<span class="hljs-number">64</span>),<br>        nn.ReLU()<br>    )<br>    <span class="hljs-comment"># 三个三维卷积</span><br>    <span class="hljs-comment"># conv1：（1, 64, 23, 23）， 8个 7x3x3 的卷积核 ==&gt; （8, 58, 21, 21）</span><br>    self.conv1_3d = nn.Sequential(<br>        nn.Conv3d(<span class="hljs-number">1</span>,<span class="hljs-number">8</span>,(<span class="hljs-number">7</span>,<span class="hljs-number">3</span>,<span class="hljs-number">3</span>)),<br>        nn.BatchNorm3d(<span class="hljs-number">8</span>),<br>        nn.ReLU()<br>    )<br>    <span class="hljs-comment"># conv2：（8, 58, 21, 21）， 16个 5x3x3 的卷积核 ==&gt; （16, 54, 19, 19）</span><br>    self.conv2_3d = nn.Sequential(<br>        nn.Conv3d(<span class="hljs-number">8</span>,<span class="hljs-number">16</span>,(<span class="hljs-number">5</span>,<span class="hljs-number">3</span>,<span class="hljs-number">3</span>)),<br>        nn.BatchNorm3d(<span class="hljs-number">16</span>),<br>        nn.ReLU()<br>    )<br>    <span class="hljs-comment"># conv3：（16, 54, 19, 19）， 32个 5x3x3 的卷积核 ==&gt; （32, 52, 17, 17）</span><br>    self.conv3_3d = nn.Sequential(<br>        nn.Conv3d(<span class="hljs-number">16</span>,<span class="hljs-number">32</span>,(<span class="hljs-number">3</span>,<span class="hljs-number">3</span>,<span class="hljs-number">3</span>)),<br>        nn.BatchNorm3d(<span class="hljs-number">32</span>),<br>        nn.ReLU()<br>    )<br><br>    self.fn1 = nn.Linear(<span class="hljs-number">480896</span>,<span class="hljs-number">256</span>)<span class="hljs-comment"># 32*52*17*17，这里可以运行一下，print一下out.size()</span><br>    self.fn2 = nn.Linear(<span class="hljs-number">256</span>,<span class="hljs-number">128</span>)<br><br>    self.fn_out = nn.Linear(<span class="hljs-number">128</span>,class_num)<br><br>    self.drop = nn.Dropout(p = <span class="hljs-number">0.4</span>)<br>    <span class="hljs-comment"># emm我在这里使用了softmax之后，网络在训练过程中loss就不再下降了，不知道具体是为啥，很奇怪，，</span><br>    <span class="hljs-comment"># self.soft = nn.Softmax(dim = 1)</span><br><br>  <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x</span>):<br>    <span class="hljs-comment"># 先降到二维</span><br>    out = x.view(x.shape[<span class="hljs-number">0</span>],x.shape[<span class="hljs-number">2</span>],x.shape[<span class="hljs-number">3</span>],x.shape[<span class="hljs-number">4</span>])<br>    out = self.conv4_2d(out)<br>    <span class="hljs-comment"># 升维（64, 23, 23）--&gt;（1,64, 23, 23）</span><br>    out = out.view(out.shape[<span class="hljs-number">0</span>],<span class="hljs-number">1</span>,out.shape[<span class="hljs-number">1</span>],out.shape[<span class="hljs-number">2</span>],out.shape[<span class="hljs-number">3</span>])<br><br>    out = self.conv1_3d(out)<br>    out = self.conv2_3d(out)<br>    out = self.conv3_3d(out)<br>    <span class="hljs-comment"># 进行重组，以b行，d列的形式存放（d自动计算）</span><br>    out = out.view(out.shape[<span class="hljs-number">0</span>],-<span class="hljs-number">1</span>)<br><br>    out = self.fn1(out)<br>    out = self.drop(out)<br>    out = self.fn2(out)<br>    out = self.drop(out)<br><br>    out = self.fn_out(out)<br><br>    <span class="hljs-keyword">return</span> out<br><br><span class="hljs-comment"># 随机输入，测试网络结构是否通</span><br>x = torch.randn(<span class="hljs-number">1</span>,<span class="hljs-number">1</span>, <span class="hljs-number">30</span>, <span class="hljs-number">25</span>, <span class="hljs-number">25</span>)<br>net = HybridSN()<br>y = net(x)<br><span class="hljs-built_in">print</span>(y.shape)<br><span class="hljs-built_in">print</span>(y)<br></code></pre></td></tr></table></figure><ul><li>由于先使用二维卷积，原始输入（30, 25, 25） 经过64个 3x3x30的卷积核，得到 （64, 23,23），在进行三维卷积，可以明显看到参数量的增加，所以整个网络模型的训练时间也会相应变长，不过也是可以看到准确率的提升</li><li>准确率在97.3%左右</li></ul><h3 id="引入注意力机制">引入注意力机制</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 引入注意力机制</span><br>class_num = <span class="hljs-number">16</span><br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">Attention_Block</span>(nn.Module):<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, planes, size</span>):<br>        <span class="hljs-built_in">super</span>(Attention_Block, self).__init__()<br><br>        self.globalAvgPool = nn.AvgPool2d(size, stride=<span class="hljs-number">1</span>)<br><br>        self.fc1 = nn.Linear(planes, <span class="hljs-built_in">round</span>(planes / <span class="hljs-number">16</span>))<br>        self.relu = nn.ReLU()<br>        self.fc2 = nn.Linear(<span class="hljs-built_in">round</span>(planes / <span class="hljs-number">16</span>), planes)<br>        self.sigmoid = nn.Sigmoid()<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x</span>):<br>        residual = x<br><br>        out = self.globalAvgPool(x)<br>        out = out.view(out.shape[<span class="hljs-number">0</span>], out.shape[<span class="hljs-number">1</span>])<br>        out = self.fc1(out)<br>        out = self.relu(out)<br>        out = self.fc2(out)<br>        out = self.sigmoid(out)<br>    <br>        out = out.view(out.shape[<span class="hljs-number">0</span>], out.shape[<span class="hljs-number">1</span>], <span class="hljs-number">1</span>, <span class="hljs-number">1</span>)<br>        out = out * residual<br><br>        <span class="hljs-keyword">return</span> out<br><br><br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">HybridSN</span>(nn.Module):<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-built_in">super</span>(HybridSN, self).__init__()<br>        <span class="hljs-comment"># 3个3D卷积</span><br>        <span class="hljs-comment"># conv1：（1, 30, 25, 25）， 8个 7x3x3 的卷积核 ==&gt; （8, 24, 23, 23）</span><br>        self.conv1_3d = nn.Sequential(<br>            nn.Conv3d(<span class="hljs-number">1</span>,<span class="hljs-number">8</span>,(<span class="hljs-number">7</span>,<span class="hljs-number">3</span>,<span class="hljs-number">3</span>)),<br>            nn.BatchNorm3d(<span class="hljs-number">8</span>),<br>            nn.ReLU()<br>        )<br>        <span class="hljs-comment"># conv2：（8, 24, 23, 23）， 16个 5x3x3 的卷积核 ==&gt;（16, 20, 21, 21）</span><br>        self.conv2_3d = nn.Sequential(<br>            nn.Conv3d(<span class="hljs-number">8</span>,<span class="hljs-number">16</span>,(<span class="hljs-number">5</span>,<span class="hljs-number">3</span>,<span class="hljs-number">3</span>)),<br>            nn.BatchNorm3d(<span class="hljs-number">16</span>),<br>            nn.ReLU()<br>        )<br>        <span class="hljs-comment"># conv3：（16, 20, 21, 21），32个 3x3x3 的卷积核 ==&gt;（32, 18, 19, 19）</span><br>        self.conv3_3d = nn.Sequential(<br>            nn.Conv3d(<span class="hljs-number">16</span>,<span class="hljs-number">32</span>,(<span class="hljs-number">3</span>,<span class="hljs-number">3</span>,<span class="hljs-number">3</span>)),<br>            nn.BatchNorm3d(<span class="hljs-number">32</span>),<br>            nn.ReLU()<br>        )<br>        <span class="hljs-comment"># 二维卷积：（576, 19, 19） 64个 3x3 的卷积核，得到 （64, 17, 17）</span><br>        self.conv4_2d = nn.Sequential(<br>            nn.Conv2d(<span class="hljs-number">576</span>,<span class="hljs-number">64</span>,(<span class="hljs-number">3</span>,<span class="hljs-number">3</span>)),<br>            nn.BatchNorm2d(<span class="hljs-number">64</span>),<br>            nn.ReLU()<br>        )<br>        <span class="hljs-comment"># 注意力机制部分</span><br>        self.layer1 = self.make_layer(Attention_Block,planes = <span class="hljs-number">576</span>, size = <span class="hljs-number">19</span>)<br>        self.layer2 = self.make_layer(Attention_Block,planes = <span class="hljs-number">64</span>, size = <span class="hljs-number">17</span>)<br><br>        <span class="hljs-comment"># 接下来依次为256，128节点的全连接层，都使用比例为0.1的 Dropout</span><br>        self.fn1 = nn.Linear(<span class="hljs-number">18496</span>,<span class="hljs-number">256</span>)<br>        self.fn2 = nn.Linear(<span class="hljs-number">256</span>,<span class="hljs-number">128</span>)<br><br>        self.fn_out = nn.Linear(<span class="hljs-number">128</span>,class_num)<br><br>        self.drop = nn.Dropout(p = <span class="hljs-number">0.1</span>)<br>        <span class="hljs-comment"># emm我在这里使用了softmax之后，网络在训练过程中loss就不再下降了，不知道具体是为啥，很奇怪，，</span><br>        <span class="hljs-comment"># self.soft = nn.Softmax(dim = 1)</span><br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">make_layer</span>(<span class="hljs-params">self, block, planes, size</span>):<br>        layers = []<br>        layers.append(block(planes, size))<br>        <span class="hljs-keyword">return</span> nn.Sequential(*layers)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x</span>):<br>        out = self.conv1_3d(x)<br>        out = self.conv2_3d(out)<br>        out = self.conv3_3d(out)<br>        <span class="hljs-comment"># 进行二维卷积，因此把前面的 32*18 reshape 一下，得到 （576, 19, 19）</span><br>        out = out.view(out.shape[<span class="hljs-number">0</span>],out.shape[<span class="hljs-number">1</span>]*out.shape[<span class="hljs-number">2</span>],out.shape[<span class="hljs-number">3</span>],out.shape[<span class="hljs-number">4</span>])<br><br>        <span class="hljs-comment"># 在二维卷积部分引入注意力机制</span><br>        out = self.layer1(out)<br>        out = self.conv4_2d(out)<br>        out = self.layer2(out)<br>        <span class="hljs-comment"># 接下来是一个 flatten 操作，变为 18496 维的向量</span><br>        <span class="hljs-comment"># 进行重组，以b行，d列的形式存放（d自动计算）</span><br>        out = out.view(out.shape[<span class="hljs-number">0</span>],-<span class="hljs-number">1</span>)<br><br>        out = self.fn1(out)<br>        out = self.drop(out)<br>        out = self.fn2(out)<br>        out = self.drop(out)<br><br>        out = self.fn_out(out)<br><br>        <span class="hljs-comment"># out = self.soft(out)</span><br><br>        <span class="hljs-keyword">return</span> out<br><br><span class="hljs-comment"># 随机输入，测试网络结构是否通</span><br>x = torch.randn(<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">30</span>, <span class="hljs-number">25</span>, <span class="hljs-number">25</span>)<br>net = HybridSN()<br>y = net(x)<br><span class="hljs-built_in">print</span>(y.shape)<br><span class="hljs-built_in">print</span>(y)<br></code></pre></td></tr></table></figure><ul><li>可以明显感觉到网络在训练过程中能够很快的收敛，并且整个网络的训练过程也十分稳定，最终测试结果可以达到99%左右</li><li><img src="/2020/08/10/SENet-%E8%AF%AD%E4%B9%89%E5%88%86%E5%89%B2%E7%9B%B8%E5%85%B3%E7%9F%A5%E8%AF%86%E5%AD%A6%E4%B9%A0/4.png"></li></ul><h2 id="senet">SENet</h2><p><strong>其中心思想：对当前的输入特征图的每一个channel，进行一个Squeeze操作得到一个权重值，然后将这个权重值与对应channel进行乘积操作，对每个channel进行加权操作，从而得到新的特征图</strong></p><p><img src="/2020/08/10/SENet-%E8%AF%AD%E4%B9%89%E5%88%86%E5%89%B2%E7%9B%B8%E5%85%B3%E7%9F%A5%E8%AF%86%E5%AD%A6%E4%B9%A0/1.jpg"></p><h3 id="网络结构">网络结构</h3><ul><li><span class="math inline">\(X--&gt; U\)</span><ul><li><span class="math inline">\(F_{tr}\)</span>是传统的卷积操作</li></ul></li><li><span class="math inline">\(U--&gt; \widetilde X\)</span><ul><li>Squeeze --<span class="math inline">\(F_{sq}(·)\)</span></li><li>先对U中的每一个channel做一个 Global Average Pooling操作，然后可以得到一个1x1xC的数据<ul><li>将整个通道上的值进行平均化操作，便能够基于通道的整体信息来计算scale</li><li>因为这里作者是想要得到各channel之间的分布关联，所以这里虽然屏蔽了每个channel中空间分布中的相关性，但无关大雅</li></ul></li><li>用来表明该层C个feature map的数值分布情况</li><li>Excitation --<span class="math inline">\(F_{ex}(·,W)\)</span></li><li><span class="math inline">\(s = F_{es}(z,W) = \sigma(g(z,W)) =\sigma(W_2\delta(W_1z) )\)</span></li><li>将得到的1x1xC数据先进行一个全连接层操作， 其中<span class="math inline">\(W_1\)</span>的维度是C * C/r<ul><li>这个r是一个缩放参数，在文中取的是16，这个参数的目的是为了减少channel个数从而降低计算量</li><li>这里使用全连接层是为了充分利用通道间的相关性来得到需要的一个权重参数</li></ul></li><li>然后经过一个ReLU层</li><li>接着在经过一个全连接层操作，其中<span class="math inline">\(W_2\)</span>的维度是C/r * C</li><li>最后通过sigmoid 将最终权重限制到[0，1]的范围</li><li>最后将这个值s作为scale乘到U的每个channel上</li></ul></li><li><strong>通过控制scale的大小，把重要的特征增强，不重要的特征减弱，从而让提取的特征指向性更强</strong></li><li>作者还给出了两种实际应用的例子</li><li><img src="/2020/08/10/SENet-%E8%AF%AD%E4%B9%89%E5%88%86%E5%89%B2%E7%9B%B8%E5%85%B3%E7%9F%A5%E8%AF%86%E5%AD%A6%E4%B9%A0/3.png"></li></ul><h3 id="代码实现">代码实现</h3><p>其实现代码来自<a href="https://github.com/miraclewkf/SENet-PyTorch/blob/master/se_resnet.py">链接</a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch.nn <span class="hljs-keyword">as</span> nn<br><span class="hljs-keyword">import</span> math<br><span class="hljs-keyword">import</span> torch.utils.model_zoo <span class="hljs-keyword">as</span> model_zoo<br><br>__all__ = [<span class="hljs-string">&#x27;SENet&#x27;</span>, <span class="hljs-string">&#x27;se_resnet_18&#x27;</span>, <span class="hljs-string">&#x27;se_resnet_34&#x27;</span>, <span class="hljs-string">&#x27;se_resnet_50&#x27;</span>, <span class="hljs-string">&#x27;se_resnet_101&#x27;</span>,<br>           <span class="hljs-string">&#x27;se_resnet_152&#x27;</span>]<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">conv3x3</span>(<span class="hljs-params">in_planes, out_planes, stride=<span class="hljs-number">1</span></span>):<br>    <span class="hljs-string">&quot;&quot;&quot;3x3 convolution with padding&quot;&quot;&quot;</span><br>    <span class="hljs-keyword">return</span> nn.Conv2d(in_planes, out_planes, kernel_size=<span class="hljs-number">3</span>, stride=stride,<br>                     padding=<span class="hljs-number">1</span>, bias=<span class="hljs-literal">False</span>)<br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">BasicBlock</span>(nn.Module):<br>    expansion = <span class="hljs-number">1</span><br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, inplanes, planes, stride=<span class="hljs-number">1</span>, downsample=<span class="hljs-literal">None</span></span>):<br>        <span class="hljs-built_in">super</span>(BasicBlock, self).__init__()<br>        self.conv1 = conv3x3(inplanes, planes, stride)<br>        self.bn1 = nn.BatchNorm2d(planes)<br>        self.relu = nn.ReLU(inplace=<span class="hljs-literal">True</span>)<br>        self.conv2 = conv3x3(planes, planes)<br>        self.bn2 = nn.BatchNorm2d(planes)<br>        self.downsample = downsample<br>        self.stride = stride<br><br>        <span class="hljs-keyword">if</span> planes == <span class="hljs-number">64</span>:<br>            self.globalAvgPool = nn.AvgPool2d(<span class="hljs-number">56</span>, stride=<span class="hljs-number">1</span>)<br>        <span class="hljs-keyword">elif</span> planes == <span class="hljs-number">128</span>:<br>            self.globalAvgPool = nn.AvgPool2d(<span class="hljs-number">28</span>, stride=<span class="hljs-number">1</span>)<br>        <span class="hljs-keyword">elif</span> planes == <span class="hljs-number">256</span>:<br>            self.globalAvgPool = nn.AvgPool2d(<span class="hljs-number">14</span>, stride=<span class="hljs-number">1</span>)<br>        <span class="hljs-keyword">elif</span> planes == <span class="hljs-number">512</span>:<br>            self.globalAvgPool = nn.AvgPool2d(<span class="hljs-number">7</span>, stride=<span class="hljs-number">1</span>)<br>        self.fc1 = nn.Linear(in_features=planes, out_features=<span class="hljs-built_in">round</span>(planes / <span class="hljs-number">16</span>))<br>        self.fc2 = nn.Linear(in_features=<span class="hljs-built_in">round</span>(planes / <span class="hljs-number">16</span>), out_features=planes)<br>        self.sigmoid = nn.Sigmoid()<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x</span>):<br>        residual = x<br><br>        out = self.conv1(x)<br>        out = self.bn1(out)<br>        out = self.relu(out)<br><br>        out = self.conv2(out)<br>        out = self.bn2(out)<br><br>        <span class="hljs-keyword">if</span> self.downsample <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>            residual = self.downsample(x)<br><br>        original_out = out<br>        out = self.globalAvgPool(out)<br>        out = out.view(out.size(<span class="hljs-number">0</span>), -<span class="hljs-number">1</span>)<br>        out = self.fc1(out)<br>        out = self.relu(out)<br>        out = self.fc2(out)<br>        out = self.sigmoid(out)<br>        out = out.view(out.size(<span class="hljs-number">0</span>), out.size(<span class="hljs-number">1</span>), <span class="hljs-number">1</span>, <span class="hljs-number">1</span>)<br>        out = out * original_out<br><br>        out += residual<br>        out = self.relu(out)<br><br>        <span class="hljs-keyword">return</span> out<br><br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">Bottleneck</span>(nn.Module):<br>    expansion = <span class="hljs-number">4</span><br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, inplanes, planes, stride=<span class="hljs-number">1</span>, downsample=<span class="hljs-literal">None</span></span>):<br>        <span class="hljs-built_in">super</span>(Bottleneck, self).__init__()<br>        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=<span class="hljs-number">1</span>, bias=<span class="hljs-literal">False</span>)<br>        self.bn1 = nn.BatchNorm2d(planes)<br>        self.conv2 = nn.Conv2d(planes, planes, kernel_size=<span class="hljs-number">3</span>, stride=stride,<br>                               padding=<span class="hljs-number">1</span>, bias=<span class="hljs-literal">False</span>)<br>        self.bn2 = nn.BatchNorm2d(planes)<br>        self.conv3 = nn.Conv2d(planes, planes * <span class="hljs-number">4</span>, kernel_size=<span class="hljs-number">1</span>, bias=<span class="hljs-literal">False</span>)<br>        self.bn3 = nn.BatchNorm2d(planes * <span class="hljs-number">4</span>)<br>        self.relu = nn.ReLU(inplace=<span class="hljs-literal">True</span>)<br>        <span class="hljs-keyword">if</span> planes == <span class="hljs-number">64</span>:<br>            self.globalAvgPool = nn.AvgPool2d(<span class="hljs-number">56</span>, stride=<span class="hljs-number">1</span>)<br>        <span class="hljs-keyword">elif</span> planes == <span class="hljs-number">128</span>:<br>            self.globalAvgPool = nn.AvgPool2d(<span class="hljs-number">28</span>, stride=<span class="hljs-number">1</span>)<br>        <span class="hljs-keyword">elif</span> planes == <span class="hljs-number">256</span>:<br>            self.globalAvgPool = nn.AvgPool2d(<span class="hljs-number">14</span>, stride=<span class="hljs-number">1</span>)<br>        <span class="hljs-keyword">elif</span> planes == <span class="hljs-number">512</span>:<br>            self.globalAvgPool = nn.AvgPool2d(<span class="hljs-number">7</span>, stride=<span class="hljs-number">1</span>)<br>        self.fc1 = nn.Linear(in_features=planes * <span class="hljs-number">4</span>, out_features=<span class="hljs-built_in">round</span>(planes / <span class="hljs-number">4</span>))<br>        self.fc2 = nn.Linear(in_features=<span class="hljs-built_in">round</span>(planes / <span class="hljs-number">4</span>), out_features=planes * <span class="hljs-number">4</span>)<br>        self.sigmoid = nn.Sigmoid()<br>        self.downsample = downsample<br>        self.stride = stride<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x</span>):<br>        residual = x<br><br>        out = self.conv1(x)<br>        out = self.bn1(out)<br>        out = self.relu(out)<br><br>        out = self.conv2(out)<br>        out = self.bn2(out)<br>        out = self.relu(out)<br><br>        out = self.conv3(out)<br>        out = self.bn3(out)<br><br>        <span class="hljs-keyword">if</span> self.downsample <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>            residual = self.downsample(x)<br><br>        original_out = out<br>        out = self.globalAvgPool(out)<br>        out = out.view(out.size(<span class="hljs-number">0</span>), -<span class="hljs-number">1</span>)<br>        out = self.fc1(out)<br>        out = self.relu(out)<br>        out = self.fc2(out)<br>        out = self.sigmoid(out)<br>        out = out.view(out.size(<span class="hljs-number">0</span>),out.size(<span class="hljs-number">1</span>),<span class="hljs-number">1</span>,<span class="hljs-number">1</span>)<br>        out = out * original_out<br><br>        out += residual<br>        out = self.relu(out)<br><br>        <span class="hljs-keyword">return</span> out<br><br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">SENet</span>(nn.Module):<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, block, layers, num_classes=<span class="hljs-number">1000</span></span>):<br>        self.inplanes = <span class="hljs-number">64</span><br>        <span class="hljs-built_in">super</span>(SENet, self).__init__()<br>        self.conv1 = nn.Conv2d(<span class="hljs-number">3</span>, <span class="hljs-number">64</span>, kernel_size=<span class="hljs-number">7</span>, stride=<span class="hljs-number">2</span>, padding=<span class="hljs-number">3</span>,<br>                               bias=<span class="hljs-literal">False</span>)<br>        self.bn1 = nn.BatchNorm2d(<span class="hljs-number">64</span>)<br>        self.relu = nn.ReLU(inplace=<span class="hljs-literal">True</span>)<br>        self.maxpool = nn.MaxPool2d(kernel_size=<span class="hljs-number">3</span>, stride=<span class="hljs-number">2</span>, padding=<span class="hljs-number">1</span>)<br>        self.layer1 = self._make_layer(block, <span class="hljs-number">64</span>, layers[<span class="hljs-number">0</span>])<br>        self.layer2 = self._make_layer(block, <span class="hljs-number">128</span>, layers[<span class="hljs-number">1</span>], stride=<span class="hljs-number">2</span>)<br>        self.layer3 = self._make_layer(block, <span class="hljs-number">256</span>, layers[<span class="hljs-number">2</span>], stride=<span class="hljs-number">2</span>)<br>        self.layer4 = self._make_layer(block, <span class="hljs-number">512</span>, layers[<span class="hljs-number">3</span>], stride=<span class="hljs-number">2</span>)<br>        self.avgpool = nn.AvgPool2d(<span class="hljs-number">7</span>, stride=<span class="hljs-number">1</span>)<br>        self.fc = nn.Linear(<span class="hljs-number">512</span> * block.expansion, num_classes)<br><br>        <span class="hljs-keyword">for</span> m <span class="hljs-keyword">in</span> self.modules():<br>            <span class="hljs-keyword">if</span> <span class="hljs-built_in">isinstance</span>(m, nn.Conv2d):<br>                n = m.kernel_size[<span class="hljs-number">0</span>] * m.kernel_size[<span class="hljs-number">1</span>] * m.out_channels<br>                m.weight.data.normal_(<span class="hljs-number">0</span>, math.sqrt(<span class="hljs-number">2.</span> / n))<br>            <span class="hljs-keyword">elif</span> <span class="hljs-built_in">isinstance</span>(m, nn.BatchNorm2d):<br>                m.weight.data.fill_(<span class="hljs-number">1</span>)<br>                m.bias.data.zero_()<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">_make_layer</span>(<span class="hljs-params">self, block, planes, blocks, stride=<span class="hljs-number">1</span></span>):<br>        downsample = <span class="hljs-literal">None</span><br>        <span class="hljs-keyword">if</span> stride != <span class="hljs-number">1</span> <span class="hljs-keyword">or</span> self.inplanes != planes * block.expansion:<br>            downsample = nn.Sequential(<br>                nn.Conv2d(self.inplanes, planes * block.expansion,<br>                          kernel_size=<span class="hljs-number">1</span>, stride=stride, bias=<span class="hljs-literal">False</span>),<br>                nn.BatchNorm2d(planes * block.expansion),<br>            )<br><br>        layers = []<br>        layers.append(block(self.inplanes, planes, stride, downsample))<br>        self.inplanes = planes * block.expansion<br>        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">1</span>, blocks):<br>            layers.append(block(self.inplanes, planes))<br><br>        <span class="hljs-keyword">return</span> nn.Sequential(*layers)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x</span>):<br>        x = self.conv1(x)<br>        x = self.bn1(x)<br>        x = self.relu(x)<br>        x = self.maxpool(x)<br><br>        x = self.layer1(x)<br>        x = self.layer2(x)<br>        x = self.layer3(x)<br>        x = self.layer4(x)<br><br>        x = self.avgpool(x)<br>        x = x.view(x.size(<span class="hljs-number">0</span>), -<span class="hljs-number">1</span>)<br>        x = self.fc(x)<br><br>        <span class="hljs-keyword">return</span> x<br><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">se_resnet_18</span>(<span class="hljs-params">pretrained=<span class="hljs-literal">False</span>, **kwargs</span>):<br>    <span class="hljs-string">&quot;&quot;&quot;Constructs a ResNet-18 model.</span><br><span class="hljs-string">    Args:</span><br><span class="hljs-string">        pretrained (bool): If True, returns a model pre-trained on ImageNet</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    model = SENet(BasicBlock, [<span class="hljs-number">2</span>, <span class="hljs-number">2</span>, <span class="hljs-number">2</span>, <span class="hljs-number">2</span>], **kwargs)<br>    <span class="hljs-keyword">return</span> model<br><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">se_resnet_34</span>(<span class="hljs-params">pretrained=<span class="hljs-literal">False</span>, **kwargs</span>):<br>    <span class="hljs-string">&quot;&quot;&quot;Constructs a ResNet-34 model.</span><br><span class="hljs-string">    Args:</span><br><span class="hljs-string">        pretrained (bool): If True, returns a model pre-trained on ImageNet</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    model = SENet(BasicBlock, [<span class="hljs-number">3</span>, <span class="hljs-number">4</span>, <span class="hljs-number">6</span>, <span class="hljs-number">3</span>], **kwargs)<br>    <span class="hljs-keyword">return</span> model<br><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">se_resnet_50</span>(<span class="hljs-params">pretrained=<span class="hljs-literal">False</span>, **kwargs</span>):<br>    <span class="hljs-string">&quot;&quot;&quot;Constructs a ResNet-50 model.</span><br><span class="hljs-string">    Args:</span><br><span class="hljs-string">        pretrained (bool): If True, returns a model pre-trained on ImageNet</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    model = SENet(Bottleneck, [<span class="hljs-number">3</span>, <span class="hljs-number">4</span>, <span class="hljs-number">6</span>, <span class="hljs-number">3</span>], **kwargs)<br>    <span class="hljs-keyword">return</span> model<br><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">se_resnet_101</span>(<span class="hljs-params">pretrained=<span class="hljs-literal">False</span>, **kwargs</span>):<br>    <span class="hljs-string">&quot;&quot;&quot;Constructs a ResNet-101 model.</span><br><span class="hljs-string">    Args:</span><br><span class="hljs-string">        pretrained (bool): If True, returns a model pre-trained on ImageNet</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    model = SENet(Bottleneck, [<span class="hljs-number">3</span>, <span class="hljs-number">4</span>, <span class="hljs-number">23</span>, <span class="hljs-number">3</span>], **kwargs)<br>    <span class="hljs-keyword">return</span> model<br><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">se_resnet_152</span>(<span class="hljs-params">pretrained=<span class="hljs-literal">False</span>, **kwargs</span>):<br>    <span class="hljs-string">&quot;&quot;&quot;Constructs a ResNet-152 model.</span><br><span class="hljs-string">    Args:</span><br><span class="hljs-string">        pretrained (bool): If True, returns a model pre-trained on ImageNet</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    model = SENet(Bottleneck, [<span class="hljs-number">3</span>, <span class="hljs-number">8</span>, <span class="hljs-number">36</span>, <span class="hljs-number">3</span>], **kwargs)<br>    <span class="hljs-keyword">return</span> model<br></code></pre></td></tr></table></figure><h2 id="语义分割中的自注意力机制和低秩重重建">语义分割中的自注意力机制和低秩重重建</h2><h3 id="语义分割">语义分割</h3><p><img src="/2020/08/10/SENet-%E8%AF%AD%E4%B9%89%E5%88%86%E5%89%B2%E7%9B%B8%E5%85%B3%E7%9F%A5%E8%AF%86%E5%AD%A6%E4%B9%A0/1.png"></p><ul><li>原始的网络主要进行图像分类，通过卷积层+全连接层得到最后的一个分类结果</li><li>当网络的最后几层，依旧采用卷积层，再通过上采样输出一个nxn的结果输出<ul><li>全卷积网络，无论卷积核多大，总是收到感受野大小的限制</li><li>而进行语义分割，需要<strong>更大的感受野范围</strong></li></ul></li><li><img src="/2020/08/10/SENet-%E8%AF%AD%E4%B9%89%E5%88%86%E5%89%B2%E7%9B%B8%E5%85%B3%E7%9F%A5%E8%AF%86%E5%AD%A6%E4%B9%A0/2.png"></li></ul><h3 id="nonlocal-networks">Nonlocal Networks</h3><ul><li>对于卷积神经网络的感受野，其大小就是卷积核的大小，只考虑局部区域，因此是local的，而non-local指的就是感受野可以很大，而不是一个局部领域（全连接层就是non-local的）</li><li>预测一个物体的信息，需要尽可能多的采集整个图像中各个位置的信息，考虑当前像素点和其他像素点的关联信息<ul><li>即利用两个点的相似性对每个位置的特征做加权</li><li><span class="math inline">\(y_i = \frac1{C(x)} \sum_{ \forallj}f(x_i,x_j)g(x_j)\)</span></li><li><span class="math inline">\(f(x_,x_j) = e^{\theta(x_i)^T\phi(x_j)}\)</span> 表示 <span class="math inline">\(x_i\)</span> 和<span class="math inline">\(x_j\)</span>的相关度计算，C(x)表示一个归一化操作，<span class="math inline">\(g(x_j)\)</span> 表示参考像素的变换<ul><li>实现原理如图：</li><li><img src="/2020/08/10/SENet-%E8%AF%AD%E4%B9%89%E5%88%86%E5%89%B2%E7%9B%B8%E5%85%B3%E7%9F%A5%E8%AF%86%E5%AD%A6%E4%B9%A0/2.jpg"></li><li>其相似度的计算有多种方法，不过差异不大，选了一个好操作的</li><li>其中Embedding的实现方式，以图像为例，在文章中都采用1*1的卷积 ，即<span class="math inline">\(\theta\)</span> 和 <span class="math inline">\(\phi\)</span> 都是1x1卷积操作。</li></ul></li><li><span class="math inline">\(z_i = W_zy_i + x_i\)</span><ul><li>构成一个残差模型</li><li>这样也成了一个block组件，可以直接插入到神经网络中</li><li>实验也证明了这些结构其存在的必要性和有效性</li></ul></li><li>与全连接层的关联<ul><li>当两个点之间不再根据位置信息计算相似性，而是直接运算</li><li><span class="math inline">\(g(x_j) = x_j\)</span></li><li>归一化系数为1</li><li>那么就成了全连接层，可以将全连接层理解为non-local的一个特例</li></ul></li></ul></li><li>其具体实现如下：</li><li><img src="/2020/08/10/SENet-%E8%AF%AD%E4%B9%89%E5%88%86%E5%89%B2%E7%9B%B8%E5%85%B3%E7%9F%A5%E8%AF%86%E5%AD%A6%E4%B9%A0/6.png"><ul><li>不过当输入featuremap的尺寸很大时，其non-local的计算量会很庞大，因此只在比较深的网络层（高阶语义层）上使用</li></ul></li></ul><h2 id="图像语义分割前沿进展">图像语义分割前沿进展</h2><h3 id="res2net">Res2Net</h3><ul><li>为了更好的利用多尺度信息，在一个ResNetblock中，再次进行多尺度信息的分割，从而充分利用尺度信息</li><li><img src="/2020/08/10/SENet-%E8%AF%AD%E4%B9%89%E5%88%86%E5%89%B2%E7%9B%B8%E5%85%B3%E7%9F%A5%E8%AF%86%E5%AD%A6%E4%B9%A0/7.png"></li></ul><h3 id="strip-pooling">Strip Pooling</h3><ul><li>带状池化<ul><li>传统的标准pooling多是方形，而实际场景中会有一些物体是长条形，因此希望尽可能捕获一个long-range的特征</li><li>把标准的spatialpooling的kernel的宽或高置为1，然后每次取所有水平元素或垂直元素相加求平均</li></ul></li><li>SP模块<ul><li><img src="/2020/08/10/SENet-%E8%AF%AD%E4%B9%89%E5%88%86%E5%89%B2%E7%9B%B8%E5%85%B3%E7%9F%A5%E8%AF%86%E5%AD%A6%E4%B9%A0/8.png"><ul><li>对于一个输入 x（HxW）， 用两个pathway 分别处理水平和垂直的strippooling，然后再expand到输入的原尺寸 （HxW）</li><li>然后将两个pathway的结果相加进行融合，再用1x1卷积进行降维，最后使用sigmoid激活</li><li>不过感觉上面的处理部分像是计算得到了一个权重矩阵，得到了每个像素位置的权重分布情况，这样理解起来，有点像SENet的注意力机制。。</li></ul></li><li>同时其中任意两个像素点之间的信息也可以通过这种类似桥接的方式得到连接，从而获得更多的全局信息</li></ul></li></ul>]]></content>
    
    
    <categories>
      
      <category>study</category>
      
    </categories>
    
    
    <tags>
      
      <tag>深度学习</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>代码学习和论文阅读</title>
    <link href="/2020/08/06/%E4%BB%A3%E7%A0%81%E5%AD%A6%E4%B9%A0%E5%92%8C%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/"/>
    <url>/2020/08/06/%E4%BB%A3%E7%A0%81%E5%AD%A6%E4%B9%A0%E5%92%8C%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/</url>
    
    <content type="html"><![CDATA[<p>学习理解MobileNetV1、MobileNetV2的代码；阅读《HybridSN: Exploring3-D–2-DCNN Feature Hierarchy for Hyperspectral ImageClassification》，并学习其代码实现，理解3D卷积和2D卷积； 阅读《Beyond aGaussian Denoiser: Residual Learning of Deep CNN for ImageDenoising》、CVPR2018的论文《Squeeze-and-ExcitationNetworks》以及CVPR2019的论文《Deep Supervised Cross-modalRetrieval》</p><span id="more"></span><h2 id="mobilenetv1代码实现">MobileNetV1代码实现</h2><p>关于MobileNetV1的相关学习和理解，都整理在上一篇文章里面，其核心内容就是将传统卷积拆分为Depthwise+Pointwise两部分，从而减少了参数量，并保持了网络性能。<img src="/2020/08/06/%E4%BB%A3%E7%A0%81%E5%AD%A6%E4%B9%A0%E5%92%8C%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/14.jpg"></p><ul><li>假如当前输入为19x19x3</li><li>标准卷积：3x3x3x4（stride = 2, padding =1），那么得到的输出为10x10x4</li><li>深度可分离卷积：<ul><li>深度卷积：3x3x1x3（3个卷积核对应着输入的三个channel），得到10x10x3的中间输出</li><li>点卷积：1x1x3x4，得到最终输出10x10x4</li></ul></li><li>一个标准的卷积层以<span class="math inline">\(D_F * D_F *M\)</span>大小的feature map F作为输入，然后输出一个<span class="math inline">\(D_G * D_G * N\)</span>的feature G<ul><li>卷积核K的参数量为<ul><li><span class="math inline">\(D_K * D_K * M * N\)</span></li></ul></li><li>标准卷积的计算量为<ul><li><span class="math inline">\(D_K * D_K * M * N * D_F *D_F\)</span></li></ul></li><li>深度可分离卷积的计算量为<ul><li><span class="math inline">\(D_K * D_K * M * D_F * D_F + M * N * D_F* D_F\)</span></li></ul></li></ul></li><li>MobileNet使用了大量的3 ×3的深度可分解卷积核，极大地减少了计算量（1/8到1/9之间），但准确率下降的很小</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">import</span> torch.nn <span class="hljs-keyword">as</span> nn<br><span class="hljs-keyword">import</span> torch.nn.functional <span class="hljs-keyword">as</span> F<br><span class="hljs-keyword">import</span> torchvision<br><span class="hljs-keyword">import</span> torchvision.transforms <span class="hljs-keyword">as</span> transforms<br><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">import</span> torch.optim <span class="hljs-keyword">as</span> optim<br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">Block</span>(nn.Module):<br>    <span class="hljs-string">&#x27;&#x27;&#x27;Depthwise conv + Pointwise conv&#x27;&#x27;&#x27;</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, in_planes, out_planes, stride=<span class="hljs-number">1</span></span>):<br>        <span class="hljs-built_in">super</span>(Block, self).__init__()<br>        <span class="hljs-comment"># Depthwise 卷积，3*3 的卷积核，分为 in_planes，即各层单独进行卷积</span><br>        self.conv1 = nn.Conv2d(in_planes, in_planes, kernel_size=<span class="hljs-number">3</span>, stride=stride, padding=<span class="hljs-number">1</span>, groups=in_planes, bias=<span class="hljs-literal">False</span>)<br>        self.bn1 = nn.BatchNorm2d(in_planes)<br>        <span class="hljs-comment"># Pointwise 卷积，1*1 的卷积核</span><br>        self.conv2 = nn.Conv2d(in_planes, out_planes, kernel_size=<span class="hljs-number">1</span>, stride=<span class="hljs-number">1</span>, padding=<span class="hljs-number">0</span>, bias=<span class="hljs-literal">False</span>)<br>        self.bn2 = nn.BatchNorm2d(out_planes)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x</span>):<br>        out = F.relu(self.bn1(self.conv1(x)))<br>        out = F.relu(self.bn2(self.conv2(out)))<br>        <span class="hljs-keyword">return</span> out<br><br><span class="hljs-comment"># 使用GPU训练，可以在菜单 &quot;代码执行工具&quot; -&gt; &quot;更改运行时类型&quot; 里进行设置</span><br>device = torch.device(<span class="hljs-string">&quot;cuda:0&quot;</span> <span class="hljs-keyword">if</span> torch.cuda.is_available() <span class="hljs-keyword">else</span> <span class="hljs-string">&quot;cpu&quot;</span>)<br><br><span class="hljs-comment"># 创建DataLoader</span><br>transform_train = transforms.Compose([<br>    transforms.RandomCrop(<span class="hljs-number">32</span>, padding=<span class="hljs-number">4</span>),<br>    transforms.RandomHorizontalFlip(),<br>    transforms.ToTensor(),<br>    transforms.Normalize((<span class="hljs-number">0.4914</span>, <span class="hljs-number">0.4822</span>, <span class="hljs-number">0.4465</span>), (<span class="hljs-number">0.2023</span>, <span class="hljs-number">0.1994</span>, <span class="hljs-number">0.2010</span>))])<br><br>transform_test = transforms.Compose([<br>    transforms.ToTensor(),<br>    transforms.Normalize((<span class="hljs-number">0.4914</span>, <span class="hljs-number">0.4822</span>, <span class="hljs-number">0.4465</span>), (<span class="hljs-number">0.2023</span>, <span class="hljs-number">0.1994</span>, <span class="hljs-number">0.2010</span>))])<br><br>trainset = torchvision.datasets.CIFAR10(root=<span class="hljs-string">&#x27;./data&#x27;</span>, train=<span class="hljs-literal">True</span>,  download=<span class="hljs-literal">True</span>, transform=transform_train)<br>testset  = torchvision.datasets.CIFAR10(root=<span class="hljs-string">&#x27;./data&#x27;</span>, train=<span class="hljs-literal">False</span>, download=<span class="hljs-literal">True</span>, transform=transform_test)<br><br>trainloader = torch.utils.data.DataLoader(trainset, batch_size=<span class="hljs-number">128</span>, shuffle=<span class="hljs-literal">True</span>, num_workers=<span class="hljs-number">2</span>)<br>testloader = torch.utils.data.DataLoader(testset, batch_size=<span class="hljs-number">128</span>, shuffle=<span class="hljs-literal">False</span>, num_workers=<span class="hljs-number">2</span>)<br><br><br><span class="hljs-comment"># 创建 MobileNetV1 网络</span><br><span class="hljs-comment"># 32×32×3 ==&gt;</span><br><br><span class="hljs-comment"># 32×32×32 ==&gt; 32×32×64 ==&gt; 16×16×128 ==&gt; 16×16×128 ==&gt;</span><br><br><span class="hljs-comment"># 8×8×256 ==&gt; 8×8×256 ==&gt; 4×4×512 ==&gt; 4×4×512 ==&gt;</span><br><br><span class="hljs-comment"># 2×2×1024 ==&gt; 2×2×1024</span><br><br><span class="hljs-comment"># 接下来为均值 pooling ==&gt; 1×1×1024</span><br><br><span class="hljs-comment"># 最后全连接到 10个输出节点</span><br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">MobileNetV1</span>(nn.Module):<br>    <span class="hljs-comment"># (128,2) means conv planes=128, stride=2</span><br>    cfg = [(<span class="hljs-number">64</span>,<span class="hljs-number">1</span>), (<span class="hljs-number">128</span>,<span class="hljs-number">2</span>), (<span class="hljs-number">128</span>,<span class="hljs-number">1</span>), (<span class="hljs-number">256</span>,<span class="hljs-number">2</span>), (<span class="hljs-number">256</span>,<span class="hljs-number">1</span>), (<span class="hljs-number">512</span>,<span class="hljs-number">2</span>), (<span class="hljs-number">512</span>,<span class="hljs-number">1</span>), <br>           (<span class="hljs-number">1024</span>,<span class="hljs-number">2</span>), (<span class="hljs-number">1024</span>,<span class="hljs-number">1</span>)]<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, num_classes=<span class="hljs-number">10</span></span>):<br>        <span class="hljs-built_in">super</span>(MobileNetV1, self).__init__()<br>        self.conv1 = nn.Conv2d(<span class="hljs-number">3</span>, <span class="hljs-number">32</span>, kernel_size=<span class="hljs-number">3</span>, stride=<span class="hljs-number">1</span>, padding=<span class="hljs-number">1</span>, bias=<span class="hljs-literal">False</span>)<br>        self.bn1 = nn.BatchNorm2d(<span class="hljs-number">32</span>)<br>        self.layers = self._make_layers(in_planes=<span class="hljs-number">32</span>)<br>        self.linear = nn.Linear(<span class="hljs-number">1024</span>, num_classes)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">_make_layers</span>(<span class="hljs-params">self, in_planes</span>):<br>        layers = []<br>        <span class="hljs-keyword">for</span> x <span class="hljs-keyword">in</span> self.cfg:<br>            out_planes = x[<span class="hljs-number">0</span>]   <span class="hljs-comment"># out_planes</span><br>            stride = x[<span class="hljs-number">1</span>]       <span class="hljs-comment"># padding操作</span><br>            layers.append(Block(in_planes, out_planes, stride))<br>            in_planes = out_planes<br>        <span class="hljs-keyword">return</span> nn.Sequential(*layers)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x</span>):<br>        out = F.relu(self.bn1(self.conv1(x)))<br>        out = self.layers(out)<br>        out = F.avg_pool2d(out, <span class="hljs-number">2</span>)<br>        out = out.view(out.size(<span class="hljs-number">0</span>), -<span class="hljs-number">1</span>)<br>        out = self.linear(out)<br>        <span class="hljs-keyword">return</span> out<br><br><span class="hljs-comment"># 网络放到GPU上</span><br>net = MobileNetV1().to(device)<br>criterion = nn.CrossEntropyLoss()<br>optimizer = optim.Adam(net.parameters(), lr=<span class="hljs-number">0.001</span>)<br><br><span class="hljs-comment"># 模型训练</span><br><span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">10</span>):  <span class="hljs-comment"># 重复多轮训练</span><br>    <span class="hljs-keyword">for</span> i, (inputs, labels) <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(trainloader):<br>        inputs = inputs.to(device)<br>        labels = labels.to(device)<br>        <span class="hljs-comment"># 优化器梯度归零</span><br>        optimizer.zero_grad()<br>        <span class="hljs-comment"># 正向传播 +　反向传播 + 优化 </span><br>        outputs = net(inputs)<br>        loss = criterion(outputs, labels)<br>        loss.backward()<br>        optimizer.step()<br>        <span class="hljs-comment"># 输出统计信息</span><br>        <span class="hljs-keyword">if</span> i % <span class="hljs-number">100</span> == <span class="hljs-number">0</span>:   <br>            <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;Epoch: %d Minibatch: %5d loss: %.3f&#x27;</span> %(epoch + <span class="hljs-number">1</span>, i + <span class="hljs-number">1</span>, loss.item()))<br><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;Finished Training&#x27;</span>)<br><br><span class="hljs-comment"># 模型测试</span><br>correct = <span class="hljs-number">0</span><br>total = <span class="hljs-number">0</span><br><br><span class="hljs-keyword">for</span> data <span class="hljs-keyword">in</span> testloader:<br>    images, labels = data<br>    images, labels = images.to(device), labels.to(device)<br>    outputs = net(images)<br>    _, predicted = torch.<span class="hljs-built_in">max</span>(outputs.data, <span class="hljs-number">1</span>)<br>    total += labels.size(<span class="hljs-number">0</span>)<br>    correct += (predicted == labels).<span class="hljs-built_in">sum</span>().item()<br><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;Accuracy of the network on the 10000 test images: %.2f %%&#x27;</span> % (<br>    <span class="hljs-number">100</span> * correct / total))<br></code></pre></td></tr></table></figure><h2 id="mobilenetv2学习与代码实现">MobileNetV2学习与代码实现</h2><p>其主要改动包括两个地方：</p><h3 id="inverted-residual-block"><strong>Inverted residualblock</strong></h3><ul><li>在ResNet中的bottleneck中，会先对输入进行一个1x1卷积操作，来进行降维，从而减少参数量，然后在进行3x3卷积等操作，所以bottleneck是两边宽中间窄</li><li><img src="/2020/08/06/%E4%BB%A3%E7%A0%81%E5%AD%A6%E4%B9%A0%E5%92%8C%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/1.png"></li><li>而在MobileNet中，通过卷积核分解，可以有效地减少参数量，因此可以在残差网络中对输入进行一个升维操作，即先用1x1卷积提升通道数，然后用Depthwise3x3的卷积，再使用1x1的卷积降维。这样得到的block结构便成了中间宽，两边窄。（其目的就是提升通道数，获得更多特征嘛）</li><li><img src="/2020/08/06/%E4%BB%A3%E7%A0%81%E5%AD%A6%E4%B9%A0%E5%92%8C%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/2.png"></li></ul><h3 id="linear-bottleneck"><strong>Linear Bottleneck</strong></h3><ul><li>ReLU6 ：普通的ReLU但是限制最大输出值为 6</li><li>为了在移动端设备 float16/int8的低精度的时候，也能有很好的数值分辨率。</li><li>作者认为激活函数在高维空间能够有效的增加非线性，而在低维空间时则会破坏特征，不如线性的效果好</li><li>由于在block块输出的时候使用了1x1卷积进行了降维操作，因此按照上面的理论，降维之后就不宜再使用 ReLU6 了</li><li><img src="/2020/08/06/%E4%BB%A3%E7%A0%81%E5%AD%A6%E4%B9%A0%E5%92%8C%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/3.png"></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 因为一些代码和v1的代码是相似或重复的，所以也就不在重复了，这里就放一些网络构建的主要代码</span><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">Block</span>(nn.Module):<br>    <span class="hljs-string">&#x27;&#x27;&#x27;expand + depthwise + pointwise&#x27;&#x27;&#x27;</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, in_planes, out_planes, expansion, stride</span>):<br>        <span class="hljs-built_in">super</span>(Block, self).__init__()<br>        self.stride = stride<br>        <span class="hljs-comment"># 通过 expansion 增大 feature map 的数量</span><br>        planes = expansion * in_planes<br>        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=<span class="hljs-number">1</span>, stride=<span class="hljs-number">1</span>, padding=<span class="hljs-number">0</span>, bias=<span class="hljs-literal">False</span>)<br>        self.bn1 = nn.BatchNorm2d(planes)<br>        <span class="hljs-comment"># depthwise</span><br>        self.conv2 = nn.Conv2d(planes, planes, kernel_size=<span class="hljs-number">3</span>, stride=stride, padding=<span class="hljs-number">1</span>, groups=planes, bias=<span class="hljs-literal">False</span>)<br>        self.bn2 = nn.BatchNorm2d(planes)<br>        <span class="hljs-comment"># pointwise</span><br>        self.conv3 = nn.Conv2d(planes, out_planes, kernel_size=<span class="hljs-number">1</span>, stride=<span class="hljs-number">1</span>, padding=<span class="hljs-number">0</span>, bias=<span class="hljs-literal">False</span>)<br>        self.bn3 = nn.BatchNorm2d(out_planes)<br><br>        <span class="hljs-comment"># 步长为 1 时，如果 in 和 out 的 feature map 通道不同，用一个卷积改变通道数</span><br>        <span class="hljs-keyword">if</span> stride == <span class="hljs-number">1</span> <span class="hljs-keyword">and</span> in_planes != out_planes:<br>            self.shortcut = nn.Sequential(<br>                nn.Conv2d(in_planes, out_planes, kernel_size=<span class="hljs-number">1</span>, stride=<span class="hljs-number">1</span>, padding=<span class="hljs-number">0</span>, bias=<span class="hljs-literal">False</span>),<br>                nn.BatchNorm2d(out_planes))<br>        <span class="hljs-comment"># 步长为 1 时，如果 in 和 out 的 feature map 通道相同，直接返回输入</span><br>        <span class="hljs-keyword">if</span> stride == <span class="hljs-number">1</span> <span class="hljs-keyword">and</span> in_planes == out_planes:<br>            self.shortcut = nn.Sequential()<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x</span>):<br>        out = F.relu(self.bn1(self.conv1(x)))<br>        out = F.relu(self.bn2(self.conv2(out)))<br>        out = self.bn3(self.conv3(out))<br>        <span class="hljs-comment"># 步长为1，加 shortcut 操作</span><br>        <span class="hljs-keyword">if</span> self.stride == <span class="hljs-number">1</span>:<br>            <span class="hljs-keyword">return</span> out + self.shortcut(x)<br>        <span class="hljs-comment"># 步长为2，直接输出</span><br>        <span class="hljs-keyword">else</span>:<br>            <span class="hljs-keyword">return</span> out<br>  <br>  <br><span class="hljs-keyword">class</span> <span class="hljs-title class_">MobileNetV2</span>(nn.Module):<br>    <span class="hljs-comment"># (expansion, out_planes, num_blocks, stride)</span><br>    cfg = [(<span class="hljs-number">1</span>,  <span class="hljs-number">16</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>), (<span class="hljs-number">6</span>,  <span class="hljs-number">24</span>, <span class="hljs-number">2</span>, <span class="hljs-number">1</span>), (<span class="hljs-number">6</span>,  <span class="hljs-number">32</span>, <span class="hljs-number">3</span>, <span class="hljs-number">2</span>),<br>           (<span class="hljs-number">6</span>,  <span class="hljs-number">64</span>, <span class="hljs-number">4</span>, <span class="hljs-number">2</span>), (<span class="hljs-number">6</span>,  <span class="hljs-number">96</span>, <span class="hljs-number">3</span>, <span class="hljs-number">1</span>), (<span class="hljs-number">6</span>, <span class="hljs-number">160</span>, <span class="hljs-number">3</span>, <span class="hljs-number">2</span>),<br>           (<span class="hljs-number">6</span>, <span class="hljs-number">320</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>)]<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, num_classes=<span class="hljs-number">10</span></span>):<br>        <span class="hljs-built_in">super</span>(MobileNetV2, self).__init__()<br>        self.conv1 = nn.Conv2d(<span class="hljs-number">3</span>, <span class="hljs-number">32</span>, kernel_size=<span class="hljs-number">3</span>, stride=<span class="hljs-number">1</span>, padding=<span class="hljs-number">1</span>, bias=<span class="hljs-literal">False</span>)<br>        self.bn1 = nn.BatchNorm2d(<span class="hljs-number">32</span>)<br>        self.layers = self._make_layers(in_planes=<span class="hljs-number">32</span>)<br>        self.conv2 = nn.Conv2d(<span class="hljs-number">320</span>, <span class="hljs-number">1280</span>, kernel_size=<span class="hljs-number">1</span>, stride=<span class="hljs-number">1</span>, padding=<span class="hljs-number">0</span>, bias=<span class="hljs-literal">False</span>)<br>        self.bn2 = nn.BatchNorm2d(<span class="hljs-number">1280</span>)<br>        self.linear = nn.Linear(<span class="hljs-number">1280</span>, num_classes)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">_make_layers</span>(<span class="hljs-params">self, in_planes</span>):<br>        layers = []<br>        <span class="hljs-keyword">for</span> expansion, out_planes, num_blocks, stride <span class="hljs-keyword">in</span> self.cfg:<br>            strides = [stride] + [<span class="hljs-number">1</span>]*(num_blocks-<span class="hljs-number">1</span>)<br>            <span class="hljs-keyword">for</span> stride <span class="hljs-keyword">in</span> strides:<br>                layers.append(Block(in_planes, out_planes, expansion, stride))<br>                in_planes = out_planes<br>        <span class="hljs-keyword">return</span> nn.Sequential(*layers)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x</span>):<br>        out = F.relu(self.bn1(self.conv1(x)))<br>        out = self.layers(out)<br>        out = F.relu(self.bn2(self.conv2(out)))<br>        out = F.avg_pool2d(out, <span class="hljs-number">4</span>)<br>        out = out.view(out.size(<span class="hljs-number">0</span>), -<span class="hljs-number">1</span>)<br>        out = self.linear(out)<br>        <span class="hljs-keyword">return</span> out<br>  <br></code></pre></td></tr></table></figure><h2 id="hybridsn-exploring-3d-2d-cnn-feature-hierarchy-for-hyperspectral-image-classification">HybridSN:Exploring 3D-2D CNN Feature Hierarchy for Hyperspectral ImageClassification</h2><h3 id="基础知识">基础知识</h3><ul><li>高光谱图像分类：<ul><li>手工设计的特征提取技术</li><li>基于学习的特征提取技术</li></ul></li><li>不过仅使用2D-CNN或3D-CNN分别存在一些缺点，阻止了这些方法在高光谱图像上获得更好的准确性<ul><li>2D-CNN:缺少通道关系信息，不能从光谱维度中提取出良好的区分特征图</li><li>3D-CNN: 模型非常复杂，对于在许多光谱带上具有相似纹理的类，似乎单独表现较差</li></ul></li><li>因此作者提出使用混合CNN模型 来进行高光谱图像分类</li><li>2D-CNN<ul><li>输入数据与2D卷积核进行卷积，通过计算输入数据与卷积核之间的点积之和来得到特征图</li><li>卷积核跨越输入数据以覆盖整个空间维度</li><li>同时通过激活函数传递，以在模型中引入非线性</li><li><span class="math inline">\(v_{i,j}^{x,y} = \phi(b_{i,j} +\sum_{\tau = 1}^{d_{l - 1} }\sum_{\rho = -\gamma}^{\gamma}\sum_{\sigma =-\delta}^{\delta} w_{i,j,\tau}^{\sigma,\rho}v_{i-1,\tau}^{x+\sigma,y+\rho} )\)</span></li><li>方程表示第i层的第j个特征图中空间位置（x，y）的激活值 ，表示为 <span class="math inline">\(v_{i,j}^{x,y}\)</span> ，其中 <span class="math inline">\(\phi\)</span> 为激活函数， <span class="math inline">\(b_{i,j}\)</span>为偏差参数，<span class="math inline">\(d_{l-1}\)</span>为l-1层的特征图数目和第i层第j个特征图的卷积核<span class="math inline">\(w_{i,j}\)</span> 的深度，<span class="math inline">\(2\gamma+1\)</span> 是卷积核的宽度， <span class="math inline">\(2\delta+1\)</span> 是卷积核的高度， <span class="math inline">\(w_{i,j}\)</span>是第i层第j个特征图的权重参数值</li></ul></li><li>3D-CNN<ul><li>通过对三维卷积核与三维数据进行卷积来实现的</li><li>HSI数据模型中，利用三维卷积核在输入层的多个连续频带上生成卷积层的特征图</li><li><span class="math inline">\(v_{i,j}^{x,y,z} = \phi(b_{i,j} +\sum_{\tau = 1}^{d_{l - 1} }\sum_{\lambda = -\eta}^{\eta}\sum_{\rho =-\gamma}^{\gamma}\sum_{\sigma = -\delta}^{\delta}w_{i,j,\tau}^{\sigma,\rho,\lambda}v_{i-1,\tau}^{x+\sigma,y+\rho,z+\lambda} )\)</span></li><li>这个方程的得来就是在二维卷积基础上得到，其中 <span class="math inline">\(2\eta+1\)</span> 是 沿光谱维度的卷积核深度</li></ul></li><li>在HSI分类问题中，会需要网络捕获与空间信息一起在多个频带中编码的频谱信息</li><li>虽然使用3D-CNN卷积核可以同时从HSI数据中提取光谱和空间特征，但这会增加计算复杂度。</li><li>为此作者提出了一种混合特征学习框架，称为HybridSN<ul><li>它由三个3D卷积（方程2），一个2D卷积（方程1）和三个完全连接的层组成</li></ul></li></ul><h3 id="网络结构">网络结构</h3><p><img src="/2020/08/06/%E4%BB%A3%E7%A0%81%E5%AD%A6%E4%B9%A0%E5%92%8C%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/9.jpg"></p><ul><li><p>多光谱图像输入：</p><ul><li><span class="math inline">\(I \in R^{M * N * D}\)</span><ul><li>其中 <span class="math inline">\(I\)</span>为原始输入，M、N、D分别为宽度、高度和谱带数/通道数</li></ul></li><li>因此I中包含的每一个像素点包括D个光谱测量值，并形成一个热点标签向量<span class="math inline">\(Y \in (y_1, y_2,...,y_c)\)</span><ul><li>C是土地覆盖类别</li></ul></li><li>高光谱像素表现出混合的土地覆盖类别，不过也将较高的<strong>类别内变异性</strong>和<strong>类别间相似性</strong>同时引进引入了输入<span class="math inline">\(I\)</span> 中</li><li>为了消除频谱冗余，首先将传统的主成分分析（PCA）沿频谱带应用于原始HSI数据<span class="math inline">\(I\)</span><ul><li>PCA减少了从D到B的光谱带数量，同时保持了相同的空间尺寸 ，保留了空间信息</li></ul></li><li><span class="math inline">\(X \in R^{M * N * B}\)</span> 表示PCA简化的数据立方体</li></ul></li><li><p>3D neighboring patches</p></li><li><p>为了利用图像分类技术，将HSI数据立方体划分为重叠的三维小块，其中的真值标签由重叠像素的标签决定</p></li><li><p>即从 X 中获取neighboring patche <span class="math inline">\(P\in  R^{S*S*B}\)</span> ， 以空间位置 <span class="math inline">\((\alpha , \beta)\)</span> 为中心</p></li><li><p>因此从 X 中，一共可以生成 <span class="math inline">\((M-S+1) *(N-S+1)\)</span> 个neighboring patches</p></li><li><p>也就是说，一个neighboring patche $P_{(, )} $ 其覆盖了 <span class="math inline">\(\alpha -(S-1)/2\)</span> 到 <span class="math inline">\(\alpha +(S-1)/2\)</span> 的宽度； <span class="math inline">\(\beta -(S-1)/2\)</span> 到 <span class="math inline">\(\beta +(S-1)/2\)</span> 的高度</p></li><li><p><img src="/2020/08/06/%E4%BB%A3%E7%A0%81%E5%AD%A6%E4%B9%A0%E5%92%8C%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/10.png"></p></li><li><p>混合特征学习框架，称为HybridSN：由三个3D卷积（方程2），一个2D卷积（方程1）和三个完全连接的层组成</p></li><li><p>在数据进入全连接层之前，还会进行一个flatten操作，即把多维的输入一维化（常用在从卷积层到全连接层的过渡 ）</p></li></ul><h3 id="代码实现">代码实现</h3><p>因为代码比较多，所以我都放在我的Github上了，<a href="https://github.com/hyzs1220/Classic-network-model/blob/master/HybridSN.ipynb">链接</a></p><p>所以这里只放一下实现的网络模块部分代码</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br></pre></td><td class="code"><pre><code class="hljs python">class_num = <span class="hljs-number">16</span><br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">HybridSN</span>(nn.Module):<br><br>  <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self</span>):<br>    <span class="hljs-built_in">super</span>(HybridSN, self).__init__()<br><span class="hljs-comment"># 3个3D卷积</span><br>    <span class="hljs-comment"># conv1：（1, 30, 25, 25）， 8个 7x3x3 的卷积核 ==&gt; （8, 24, 23, 23）</span><br>    self.conv1_3d = nn.Sequential(<br>        nn.Conv3d(<span class="hljs-number">1</span>,<span class="hljs-number">8</span>,(<span class="hljs-number">7</span>,<span class="hljs-number">3</span>,<span class="hljs-number">3</span>)),<br>        nn.BatchNorm3d(<span class="hljs-number">8</span>),<br>        nn.ReLU()<br>    )<br><span class="hljs-comment"># conv2：（8, 24, 23, 23）， 16个 5x3x3 的卷积核 ==&gt;（16, 20, 21, 21）</span><br>    self.conv2_3d = nn.Sequential(<br>        nn.Conv3d(<span class="hljs-number">8</span>,<span class="hljs-number">16</span>,(<span class="hljs-number">5</span>,<span class="hljs-number">3</span>,<span class="hljs-number">3</span>)),<br>        nn.BatchNorm3d(<span class="hljs-number">16</span>),<br>        nn.ReLU()<br>    )<br><span class="hljs-comment"># conv3：（16, 20, 21, 21），32个 3x3x3 的卷积核 ==&gt;（32, 18, 19, 19）</span><br>    self.conv3_3d = nn.Sequential(<br>        nn.Conv3d(<span class="hljs-number">16</span>,<span class="hljs-number">32</span>,(<span class="hljs-number">3</span>,<span class="hljs-number">3</span>,<span class="hljs-number">3</span>)),<br>        nn.BatchNorm3d(<span class="hljs-number">32</span>),<br>        nn.ReLU()<br>    )<br><span class="hljs-comment"># 二维卷积：（576, 19, 19） 64个 3x3 的卷积核，得到 （64, 17, 17）</span><br>    self.conv4_2d = nn.Sequential(<br>        nn.Conv2d(<span class="hljs-number">576</span>,<span class="hljs-number">64</span>,(<span class="hljs-number">3</span>,<span class="hljs-number">3</span>)),<br>        nn.BatchNorm2d(<span class="hljs-number">64</span>),<br>        nn.ReLU()<br>    )<br><span class="hljs-comment"># 接下来依次为256，128节点的全连接层，都使用比例为0.1的 Dropout</span><br>    self.fn1 = nn.Linear(<span class="hljs-number">18496</span>,<span class="hljs-number">256</span>)<br>    self.fn2 = nn.Linear(<span class="hljs-number">256</span>,<span class="hljs-number">128</span>)<br><br>    self.fn_out = nn.Linear(<span class="hljs-number">128</span>,class_num)<br><br>    self.drop = nn.Dropout(p = <span class="hljs-number">0.1</span>)<br>    <span class="hljs-comment"># emm我在这里使用了softmax之后，网络在训练过程中loss就不再下降了，不知道具体是为啥，很奇怪，，</span><br>    <span class="hljs-comment"># self.soft = nn.Softmax(dim = 1)</span><br><br>  <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x</span>):<br>    out = self.conv1_3d(x)<br>    out = self.conv2_3d(out)<br>    out = self.conv3_3d(out)<br><span class="hljs-comment"># 进行二维卷积，因此把前面的 32*18 reshape 一下，得到 （576, 19, 19）</span><br>    b,x,y,m,n = out.size()<br>    out = out.view(b,x*y,m,n)<br><br>    out = self.conv4_2d(out)<br><span class="hljs-comment"># 接下来是一个 flatten 操作，变为 18496 维的向量</span><br>    <span class="hljs-comment"># 进行重组，以b行，d列的形式存放（d自动计算）</span><br>    out = out.reshape(b,-<span class="hljs-number">1</span>)<br><br>    out = self.fn1(out)<br>    out = self.drop(out)<br>    out = self.fn2(out)<br>    out = self.drop(out)<br><br>    out = self.fn_out(out)<br><br>    <span class="hljs-comment"># out = self.soft(out)</span><br><br>    <span class="hljs-keyword">return</span> out<br><br><span class="hljs-comment"># 随机输入，测试网络结构是否通</span><br>x = torch.randn(<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">30</span>, <span class="hljs-number">25</span>, <span class="hljs-number">25</span>)<br>net = HybridSN()<br>y = net(x)<br><span class="hljs-built_in">print</span>(y.shape)<br><span class="hljs-built_in">print</span>(y)<br></code></pre></td></tr></table></figure><h3 id="改进">改进</h3><ul><li>原始网络的准确率能达到93%左右</li><li>引入批归一化处理（Batch Normalization）<ul><li>网络会很快的收敛，并且准确率也有明显的提升</li><li><img src="/2020/08/06/%E4%BB%A3%E7%A0%81%E5%AD%A6%E4%B9%A0%E5%92%8C%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/11.png"></li><li>准确率能提高到98%左右</li></ul></li><li>数据量过少<ul><li>在该网络训练中，由于训练数据较少，所以并不太可能会出现过拟合现象，所以这里尝试把dropout设置为0.1，测试一下网络的性能和准确率</li><li>网络的准确率略有提升</li><li><img src="/2020/08/06/%E4%BB%A3%E7%A0%81%E5%AD%A6%E4%B9%A0%E5%92%8C%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/12.png"></li></ul></li></ul><h2 id="beyond-a-gaussian-denoiser-residual-learning-of-deep-cnn-for-image-denoising">Beyonda Gaussian Denoiser: Residual Learning of Deep CNN for ImageDenoising</h2><ul><li>作者提出了一种feed-forward denoising convolutional neuralnetworks--前馈去噪卷积神经网络，称之为DnCNN<ul><li>其将深层的网络结构、学习算法和正则化方法使用到图像去噪的过程中</li><li>使用了残差学习和批量归一化，对于整个网络的训练过程和去噪性能提升显著</li><li>与传统的去噪网络相比，DnCNNs可以处理具有未知噪声水平的高斯去噪（即进行盲高斯去噪）</li></ul></li><li>其主要工作:<ul><li>提出了一种高斯去噪的端到端可训练的深层次的CNN。与现有的基于深度神经网络的直接估计原始图像的方法不同，网络采用残差学习策略从噪声观察中去除原始图像</li><li>残差学习和批量归一化可以极大地有利于CNN学习，不仅可以加速训练，还可以提高去噪性能</li><li>DnCNN可以轻松扩展，以处理一般的图像去噪任务：高斯盲去噪，SISR 和JPEG图像去块</li></ul></li><li>不过网络的输出并不是直接得到干净的原始图像x，而是残差图像v，也就是噪声观察y和原始干净图像x之间的差异，即通过隐藏层中的操作隐含去除了原始图像。</li></ul><h3 id="图像噪声">图像噪声</h3><ul><li>图像去噪的目的在一个遵循图像退化的模型 y = x +v中，让网络能够从噪声观察 y 中恢复原始数据 x<ul><li>其中x就是原始不包含噪声的图像</li><li>通过人为的制造或者随机获得噪声信息v<ul><li>设置不同的噪声水平，得到的噪声图像也会不同</li><li><img src="/2020/08/06/%E4%BB%A3%E7%A0%81%E5%AD%A6%E4%B9%A0%E5%92%8C%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/4.png"></li></ul></li><li>将两者进行叠加，得到噪声图像y</li></ul></li></ul><h3 id="贝叶斯观点">贝叶斯观点</h3><ul><li>when the likelihood is known, the image prior modeling will play acentral role in image denoising</li><li>所以在实验之前能够预先得到一个噪声水平，那么训练得到对应噪声水平的网络，这对图像去噪是十分重要的</li><li>不过这样的先验模型也存在一定的缺点<ul><li>•在测试阶段通常涉及复杂的优化问题，使去噪过程时非常耗时</li><li>•模型通常是非凸的并且涉及几个手动选择的参数</li><li>•针对特定水平的噪音训练特定的模型，因此在盲图像去噪上会受到限制</li></ul></li></ul><h3 id="残差学习">残差学习</h3><ul><li>CNN的残差学习最初被提出用于解决性能退化问题，即训练精度随着网络深度的增加而降低</li><li>通过假设残差映射 f(x) - x 比原始映射 f(x)更容易学习，残差网络便很好的学习到了我们需要的残差映射，从而训练深层次的CNN并且提高用于图像分类和对象检测的准确性</li><li>这一部分也是在上一篇有记录，这里就不在记录了</li></ul><h3 id="批量归一化">批量归一化</h3><ul><li><p>小批量随机梯度下降（SGD）已被广泛用于训练CNN模型</p></li><li><p>不过其内部协变量移位大大降低了其训练效率，即训练期间内部非线性输入的分布的改变</p></li><li><p>批量归一化：通过在每层中的非线性之前引入归一化步骤和缩放和移位步骤来减轻内部协变量偏移。并且仅新引入了两个参数，同时可以使用反向传播更新它们</p></li><li><p><img src="/2020/08/06/%E4%BB%A3%E7%A0%81%E5%AD%A6%E4%B9%A0%E5%92%8C%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/5.png"></p></li><li><p>均值为0，方差为1的标准正态分布、sigmoid函数及其导数分布</p><ul><li>假设某个隐层神经元原先的激活输入x取值符合标准正态分布，那么也就是说95%的概率x其值落在了[-2,2]的范围内</li><li>对应在sigmoid函数里面，sigmoid（x）取值大概在[0.1,0.9]之间，那么其导数取值就会在【0.05，0.25】之间吧</li><li>虽然只是一个大概，不过基本可以知道，如果我们每一个隐层的输入，都趋近于正态分布，那么便能够避免梯度消失问题</li></ul></li><li><p>“对于每个隐层神经元，把逐渐向非线性函数映射后向取值区间极限饱和区靠拢的输入分布强制拉回到均值为0方差为1的比较标准的正态分布，使得非线性变换函数的输入值落入对输入比较敏感的区域，以此避免梯度消失问题。”</p></li></ul><h3 id="网络模型">网络模型</h3><p>整体上采用了VGG网络结构，并加以修改</p><ul><li>将卷积滤波器的大小设置为3x3，并去除了所有的池化层</li><li>对于DnCNN网络，网络层数是d的时候，网络的感受野就是（2d+1) *(2d+1)，即DnCNN的感受野与网络深度d相关。所以作者参考主流的几个去噪算法，根据2d+1=effectivepatch size，反向推出DnCNN一个合适的网络深度</li><li>“It is interesting to verify whether DnCNN with the receptive fieldsize similar to EPLL can compete against the leading denoisingmethods.”</li><li>因此作者采用了和EPLL网络一样的感受野大小（35x35），因此网络深度也设置为17<ul><li>对于其他一般图像去噪任务，我们采用更大的感受野并将深度设置为20</li></ul></li><li><img src="/2020/08/06/%E4%BB%A3%E7%A0%81%E5%AD%A6%E4%B9%A0%E5%92%8C%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/7.png"></li><li>网络层结构：<ul><li>Conv+ReLU(第一层):使用64个大小为3x3xc（图像channel）的滤波器被用于生成64个特征图</li><li>Conv+BN+ReLU:对应于层2-(D-1)，使用64个大小3x3x64的滤波器，并且将批量归一化加在卷积和ReLU之间</li><li>对应于最后一层，c个大小为3x3x64的滤波器被用于重建输出</li></ul></li><li>整合残差学习和批量归一化进行图像去噪 ：<ul><li>主要观点：<strong>当原始映射更像识别映射时，那么使用残差映射将更容易优化</strong></li><li>DnCNN网络可用于训练原始映射F(y)以预测x或残差映射R(y)以预测v。不过F(y)比R(y)更接近于识别映射，所以残差学习公式更适合于图像去噪</li><li><img src="/2020/08/06/%E4%BB%A3%E7%A0%81%E5%AD%A6%E4%B9%A0%E5%92%8C%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/8.png"></li></ul></li><li>对于高斯去噪，残差学习和批量归一化很可能相互关联：<ul><li>残差学习从批量归一化中受益。（例如减轻内部协变量偏移问题）</li><li>批量归一化有益于残差学习。在没有残差学习的情况下，批量归一化甚至对网络收敛（蓝线）具有一定的不利影响；同时在网络隐层中去除了原始图像，保留下来高斯噪声信息，所以与图像内容的相关性较小，也会有利于缓和内部协变量偏移问题</li></ul></li><li>拓展到一般图像去噪：<ul><li>现有的判别式高斯去噪方法在处理未知噪声水平的图像时：首先估计噪声水平，然后使用训练有相应噪声水平的模型</li><li>DnCNN：在训练阶段，我们使用来自各种噪声水平的噪声图像来训练单个DnCNN模型（对噪声图像进行去噪而不估计其噪声水平）。最终依然可以得到十分优异的去噪结果</li></ul></li></ul>]]></content>
    
    
    <categories>
      
      <category>study</category>
      
    </categories>
    
    
    <tags>
      
      <tag>深度学习</tag>
      
      <tag>python</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Google:Inception&amp;MobileNets</title>
    <link href="/2020/07/26/Google-Inception-MobileNets/"/>
    <url>/2020/07/26/Google-Inception-MobileNets/</url>
    
    <content type="html"><![CDATA[<p>ResNet网络学习；整理Google的Inception V1到V4模块，参考文章：<a href="https://my.oschina.net/u/876354/blog/1637819">大话CNN经典模型</a>；阅读《MobileNets:Efficient Convolutional Neural Networks for Mobile VisionApplications》的总结</p><span id="more"></span><h2 id="resnet">ResNet</h2><p>之前接触过ResNet网络，不过老师作业中给了另一个学习视频，所以这里再进行一下学习整理</p><h3 id="残差">残差</h3><ul><li>在传统的卷积神经网络中，CNN其实就是一个函数拟合的过程，对于输入x，通过CNN这样一个复合函数，来得到预测输出<span class="math inline">\(\hat y\)</span></li><li>不过随着网络深度的增加，会出现网络退化，导致学习能力下降，难以得到预期的函数效果H(x)</li><li>不过发现，如果让网络学习达到F(x) = H(x) -x的函数效果（Residual，残差）则会容易许多</li><li>而最终我们预期得到的H(x)通过H(x) = F(x) + x得到</li><li><img src="/2020/07/26/Google-Inception-MobileNets/10.png"></li><li>其中右侧x直接连接到下一层输入，称之为恒等映射（短接）</li></ul><h3 id="网络结构">网络结构</h3><ul><li><img src="/2020/07/26/Google-Inception-MobileNets/11.png"></li><li>常见的ResNet一般由5个stage组成，每个stage又包含若干个block，每个block中包含多个卷积层</li><li>通过这样的结构划分，可以增加代码的可扩展性</li><li>在最后一层，设置了一个Global Average pool<ul><li>用来替代全连接层</li><li>参数更少，避免出现过拟合</li></ul></li><li>在网络的50层结构以上的时候会出现Bottle Neck（瓶颈）<ul><li><img src="/2020/07/26/Google-Inception-MobileNets/12.png"></li><li>在入口处通过1x1卷积，来进行降维操作。出口处通过1x1卷积，再恢复原有维度数</li><li>通过这样的形式来减少参数量和计算量</li></ul></li><li>在跟着视频学习的时候，也跟着老师一起手敲了一遍ResNet的模型搭建代码，感觉还是收获很多的，之前一直是看一些现成的网络模型代码，感觉能看懂就不再进一步钻研了，而当自己去亲手敲代码的时候才明白自己的不足之处——所谓“纸上得来终觉浅，绝知此事要躬行”吧！！</li></ul><h2 id="inception">Inception</h2><h3 id="原始的inception结构">原始的Inception结构</h3><p><img src="/2020/07/26/Google-Inception-MobileNets/1.png"></p><p>这是Google提出的原始的Inception模块，输入通过不同的卷积和池化操作，然后在将每部分的结果进行堆叠，得到输出。具体实例如下：</p><p><img src="/2020/07/26/Google-Inception-MobileNets/2.jpg"></p><ul><li>其中不同的卷积核操作之后，得到的featuremap的尺寸大小是不一样的，然后对其进行padding操作恢复原来尺寸大小，再将多个卷积核的操作结构进行堆叠即可</li><li>同时在每一个卷积层之后都要进行ReLU操作，增加非线性的拟合能力</li><li>通过这种操作，有两方面好处<ul><li>增加了网络的宽度</li><li>增加了网络对尺度的适应性</li></ul></li><li>不过也存在着问题，即在多个Inception模块后，输出的featuremap会十分厚，导致计算量增大。</li></ul><h3 id="inception-v1">Inception V1</h3><p>为解决上述问题，通过在3x3前、5x5前以及maxpooling后分别加上了1x1的卷积核，来达到<strong>降维</strong>的目的。</p><p><img src="/2020/07/26/Google-Inception-MobileNets/3.png"></p><p><img src="/2020/07/26/Google-Inception-MobileNets/4.jpg"></p><blockquote><p>比如，上一层的输出为100x100x128，经过具有256个通道的5x5卷积层之后(stride=1，pad=2)，输出数据为100x100x256，其中，卷积层的参数为128x5x5x256=819200。而假如上一层输出先经过具有32个通道的1x1卷积层，再经过具有256个输出的5x5卷积层，那么输出数据仍为为100x100x256，但卷积参数量已经减少为128x1x1x32+ 32x5x5x256= 204800，大约减少了4倍。</p></blockquote><h3 id="inception-v2">Inception V2</h3><ul><li><p>卷积分解</p><ul><li><p>感受野的越大，那么在卷积的过程中，便能够同时捕获更多的图像信息，不过相应的参数量也会较大（5x5的卷积核的参数为25个）。因此Google提出了一种卷积分解的方法。</p></li><li><p><img src="/2020/07/26/Google-Inception-MobileNets/5.png"></p><ul><li>5x5的图像块，通过5x5的卷积核可以直接得到最终对应的featuremap。参数个数为25</li><li>5x5的图像块，先通过3x3的卷积核得到3x3的featuremap，然后再进行一次卷积得到最终的feature map。参数个数为9+9=18</li><li>同时大量实验表明，这种替换方案并不会造成表达缺失</li></ul></li><li><p><img src="/2020/07/26/Google-Inception-MobileNets/6.png"></p><ul><li>同时，任意nxn的卷积都可以通过1xn卷积后接nx1卷积来替代</li><li><blockquote><p>在中度大小的特征图（featuremap）上使用效果才会更好（特征图大小建议在12到20之间）</p></blockquote></li></ul></li><li><p>因此原有的一个1x1→5x5卷积核</p><ul><li>可以先分解成1x1→3x3→3x3</li><li>然后进一步分解得到1x1→1x3→3x1→1x3→3x1</li></ul></li></ul></li><li><p>降低feature map大小</p><ul><li>先卷积，再池化<ul><li>这样操作，虽然保留了特征信息，但在本层的计算量并没有减少</li></ul></li><li>先池化，再卷积<ul><li>先进行池化操作，可能会导致部分特征消失</li></ul></li><li>为了同时保持特征表示且降低计算量，使用以下结构（使用两个并行化的模块来降低计算量（卷积、池化并行执行，再进行合并））<ul><li><img src="/2020/07/26/Google-Inception-MobileNets/7.png"></li></ul></li></ul></li></ul><h3 id="inception-v3">Inception V3</h3><p>对卷积核进行分解，增加网络深度，同时每增加一层都要进行ReLU操作，增加网络的非线性（增加非线性激活函数使网络产生更多独立特征，表征能力更强，训练更快）。</p><h3 id="inception-v4">Inception V4</h3><ul><li>将Inception模块与ResNet的残差相结合。</li><li>利用残差结构来进一步改进Inception V3</li><li>原始残差结构<ul><li><img src="/2020/07/26/Google-Inception-MobileNets/8.png"></li></ul></li><li>与Inception相结合<ul><li><img src="/2020/07/26/Google-Inception-MobileNets/9.png"></li></ul></li></ul><h2 id="mobilenets">MobileNets</h2><p>学习阅读了《MobileNets: Efficient Convolutional Neural Networks forMobile Vision Applications》，然后整理一下学习的主要内容。</p><h3 id="简述">简述</h3><p>文章中介绍的MobileNets基于流线型架构，使用深度可分离的卷积（depthwiseseparable convolutions）来构建轻量级深度神经网络。</p><p>文章介绍了一种网络结构和两种超参数，从而构建一个小规模、低延迟的模型，从而应用于各种特定的场景中。</p><h3 id="mobilenets框架">MobileNets框架</h3><h4 id="深度可分离卷积depthwise-separable-conv">深度可分离卷积（DepthwiseSeparable Conv）</h4><p>在MobileNet中，将传统的标准卷积进行分解，得到一个深度卷积（depthwiseconvolution）和一个1x1的点卷积（pointwiseconvolution）。其实现原理可以理解为是一种矩阵的因式分解。<strong>在进行深度卷积的时候，每个卷积核只在一个对应的channel上进行卷积操作，接着在点卷积的过程中，将卷积操作后的channel进行组合操作。</strong></p><p><img src="/2020/07/26/Google-Inception-MobileNets/13.png"></p><p>这是论文中的图例，我在网上找到了更好理解的图示</p><p><img src="/2020/07/26/Google-Inception-MobileNets/14.jpg"></p><ul><li>假如当前输入为19x19x3</li><li>标准卷积：3x3x3x4（stride = 2, padding =1），那么得到的输出为10x10x4</li><li>深度可分离卷积：<ul><li>深度卷积：3x3x1x3（3个卷积核对应着输入的三个channel），得到10x10x3的中间输出</li><li>点卷积：1x1x3x4，得到最终输出10x10x4</li></ul></li><li>一个标准的卷积层以 <span class="math inline">\(D_F* D_F * M\)</span>大小的feature map F作为输入，然后输出一个 <span class="math inline">\(D_G * D_G * N\)</span> 的feature G<ul><li>卷积核K的参数量为<ul><li><span class="math inline">\(D_K * D_K * M * N\)</span></li></ul></li><li>标准卷积的计算量为<ul><li><span class="math inline">\(D_K * D_K * M * N * D_F *D_F\)</span></li></ul></li><li>深度可分离卷积的计算量为<ul><li><span class="math inline">\(D_K * D_K * M * D_F * D_F + M * N * D_F* D_F\)</span></li></ul></li></ul></li><li>MobileNet使用了大量的3 ×3的深度可分解卷积核，极大地减少了计算量（1/8到1/9之间），但准确率下降的很小</li></ul><h4 id="网络结构和训练">网络结构和训练</h4><p><img src="/2020/07/26/Google-Inception-MobileNets/15.png"></p><p>MobileNet第一层采用标准卷积，其它层均采用文章中提出的深度可分解卷积。每一层后面跟着一个batchnorm和ReLU非线性激活函数，除了最后一层全连接层。在最后的全连接层之后直接输入到softmax层进行分类。网络中的下采样操作是采用带stride的卷积实现的。</p><p><img src="/2020/07/26/Google-Inception-MobileNets/16.png"></p><p>上图对比了标准卷积与深度可分解卷积的结构。</p><p>在MobileNet中95％的计算时间用于有75％的参数的1×1点卷积。</p><h4 id="宽度因子width-multiplier-thinner-models">宽度因子（WidthMultiplier: Thinner Models）</h4><ul><li>宽度乘数α的作用是将每一层的网络宽度变瘦</li><li><span class="math inline">\(\alpha\)</span>是一个属于(0,1]之间的数，附加于网络的通道数</li><li>对于一个给定的层和一个宽度乘数 <span class="math inline">\(\alpha\)</span> ，输入通道M变成 <span class="math inline">\(\alpha\)</span> M，输出通道N变成 <span class="math inline">\(\alpha\)</span> N</li><li><span class="math inline">\(\alpha\)</span>常用的配置为1,0.75,0.5,0.25；当 <span class="math inline">\(\alpha\)</span> 等于1时就是标准的MobileNet</li><li>通过参数 <span class="math inline">\(\alpha\)</span>可以非常有效的将计算量和参数量减少到原来的 <span class="math inline">\(\alpha^2\)</span> 倍。计算量为<ul><li><span class="math inline">\(D_K * D_K * \alpha M * D_F * D_F +\alpha M * \alpha N * D_F * D_F\)</span></li></ul></li></ul><h4 id="分辨率因子resolution-multiplier-reduced-representation">分辨率因子（ResolutionMultiplier: Reduced Representation）</h4><ul><li>分辨率乘数用来改变输入数据层的分辨率</li><li>将其应用于输入图像，然后通过相同的乘数来减少每一层的内部表示</li><li><span class="math inline">\(\rho \in (0,1]\)</span>通常是隐式设置的，因此网络的输入分辨率为224、192、160或128</li><li><span class="math inline">\(\rho\)</span> = 1是基准MobileNet，而<span class="math inline">\(\rho\)</span>&lt;1是简化的计算MobileNets</li><li>可以有效的将计算量和参数量减少到原来的 <span class="math inline">\(\rho^2\)</span> 倍。与宽度因子结合，其计算量为<ul><li><span class="math inline">\(D_K * D_K * \alpha M * \rho D_F * \rhoD_F + \alpha M * \alpha N * \rho D_F * \rho D_F\)</span></li></ul></li></ul><p>令 <span class="math inline">\(\alpha\)</span> 和 <span class="math inline">\(\rho\)</span>都小于1，可以构建更少参数的mobilenet。下面是一个具体参数设置下，网络计算量和参数数目的变化情况。</p><p><img src="/2020/07/26/Google-Inception-MobileNets/17.png"></p><h3 id="实验">实验</h3><p>使用相同的MobileNet的架构，在使用可分离卷积的情况下，其精度值略有下降（下降了1%），但其所hi用的参数量仅为1/7。</p><p><img src="/2020/07/26/Google-Inception-MobileNets/18.png"></p><h4 id="model-choices">Model Choices</h4><p>在面对“是更浅的网络更好，还是更瘦的网络更好呢？”这样的问题的时候，作者设计了参数和计算量相近的两个网络进行了比较，其结论是相对而言。</p><p><img src="/2020/07/26/Google-Inception-MobileNets/19.png"></p><h4 id="alpha和rho的定量影响">alpha和rho的定量影响</h4><p><img src="/2020/07/26/Google-Inception-MobileNets/20.png"></p><p>可以看到，超参数 <span class="math inline">\(\alpha\)</span>减小的时候，模型准确率随着模型的变瘦而下降。</p><p><img src="/2020/07/26/Google-Inception-MobileNets/21.png"></p><p>超参数 <span class="math inline">\(\rho\)</span>减小的时候，模型准确率随着模型的分辨率下降而下降。</p><p>能够看出来，两个超参数的引入，都会导致模型准确率的下降，降低MobileNet的性能表现，不过更重要的是在计算量和准确率之间、模型大小和准确率之间的一个取舍和权衡。</p>]]></content>
    
    
    <categories>
      
      <category>study</category>
      
    </categories>
    
    
    <tags>
      
      <tag>深度学习</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>深度学习笔记-4</title>
    <link href="/2020/02/29/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-4/"/>
    <url>/2020/02/29/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-4/</url>
    
    <content type="html"><![CDATA[<p>生成式对抗网络&amp;实践</p><span id="more"></span><h2 id="生成式对抗网络">生成式对抗网络</h2><h3 id="gan">GAN</h3><p>Generative Adversarial Nets, 生成式对抗网络</p><ul><li>生成模型<ul><li><figure><img src="/2020/02/29/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-4/1582617712264.png" alt="1582617712264"><figcaption aria-hidden="true">1582617712264</figcaption></figure></li><li>生成式对抗网络（GAN）的目的是训练这样一个生成模型，生成我们想要的数据</li></ul></li><li>GAN框架<ul><li><figure><img src="/2020/02/29/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-4/1582632077385.png" alt="1582632077385"><figcaption aria-hidden="true">1582632077385</figcaption></figure></li><li>判别器(Discriminator)：区分真实(real)样本和虚假(fake)样本。对于真实样本，尽可能给出高的评分1；对于虚假数据，尽可能给出低个评分0<ul><li><figure><img src="/2020/02/29/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-4/1582632804680.png" alt="1582632804680"><figcaption aria-hidden="true">1582632804680</figcaption></figure></li></ul></li><li>生成器(Generator)：<strong>欺骗判别器</strong>。生成虚假数据，使得判别器D能够尽可能给出高的评分1<ul><li><figure><img src="/2020/02/29/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-4/1582632769055.png" alt="1582632769055"><figcaption aria-hidden="true">1582632769055</figcaption></figure></li></ul></li><li>随机噪声z：从一个先验分布（人为定义，一般是均匀分布或者正态分布）中随机采样的向量（输入的向量维度越高，其生成图像的种类越多）</li><li>真实样本x：从数据库中采样的样本</li><li>合成样本G(z)：生成模型G输出的样本</li><li>目标函数：<ul><li><figure><img src="/2020/02/29/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-4/1582730872597.png" alt="1582730872597"><figcaption aria-hidden="true">1582730872597</figcaption></figure></li><li><figure><img src="/2020/02/29/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-4/1582632289057.png" alt="1582632289057"><figcaption aria-hidden="true">1582632289057</figcaption></figure></li><li>让真实样本的输出值尽可能大，同时让生成样本的输出值尽可能小</li><li>所以判别器D最大化的目标函数就是对于真实样本尽可能输出1，对于生成器的生成样本输出0</li><li>生成器G最小化目标函数就是让生成样本能够欺骗判别器，让其尽量输出1</li></ul></li></ul></li><li>训练算法<ul><li>随机初始化生成器和判别器</li><li>交替训练判别器D 和生成器G，直到收敛<ul><li>固定生成器G，训练判别器D区分真实图像与合成图像</li><li><figure><img src="/2020/02/29/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-4/1582730150790.png" alt="1582730150790"><figcaption aria-hidden="true">1582730150790</figcaption></figure><ul><li>一个样本x来自真实分布 <span class="math inline">\(P_{data}\)</span>和生成分布 <span class="math inline">\(P_g\)</span> 概率的相对比例</li><li>如果来自生成分布的概率为0（<span class="math inline">\(P_g =0\)</span>），那么就给出概率1，即确定该样本是真的</li><li>如果来自真实分布的概率为0（<span class="math inline">\(P_{data} =0\)</span>），那么就给出概率0，即确定该样本是假的</li><li>因为最优判别器的输出属于[0,1] ，所以判别器的输出用sigmoid激活</li></ul></li><li><figure><img src="/2020/02/29/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-4/1582633035617.png" alt="1582633035617"><figcaption aria-hidden="true">1582633035617</figcaption></figure></li><li>固定判别器D，训练生成器G欺骗判别器D<ul><li><figure><img src="/2020/02/29/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-4/1582730817910.png" alt="1582730817910"><figcaption aria-hidden="true">1582730817910</figcaption></figure></li></ul></li><li><figure><img src="/2020/02/29/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-4/1582633084889.png" alt="1582633084889"><figcaption aria-hidden="true">1582633084889</figcaption></figure></li></ul></li></ul></li><li>KL散度：一种衡量两个概率分布的匹配程度的指标<ul><li>当 <span class="math inline">\(P_1 = P_2\)</span>时，KL散度为零</li><li><figure><img src="/2020/02/29/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-4/1582723689966.png" alt="1582723689966"><figcaption aria-hidden="true">1582723689966</figcaption></figure></li><li>具有非负性，但存在不对称性（在优化的时候，会因为不对称性优化出不同的结果）</li><li><strong>极大似然估计 等价于最小化生成数据分布和真实分布的KL散度</strong></li></ul></li><li>JS散度<ul><li><figure><img src="/2020/02/29/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-4/1582723814551.png" alt="1582723814551"><figcaption aria-hidden="true">1582723814551</figcaption></figure></li><li>具有非负性，以及对称性</li></ul></li><li><figure><img src="/2020/02/29/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-4/1582728272282.png" alt="1582728272282"><figcaption aria-hidden="true">1582728272282</figcaption></figure><ul><li>但存在问题：生成数据和真实数据分布的表达形式我们不知道，无法计算散度，也就没有目标函数，无法优化生成器</li><li>GAN：生成式对抗网络通过对抗训练，间接计算出散度(JS)，使得模型可以优化</li><li><strong>最大化判别器损失，等价于计算合成数据分布和真实数据分布的JS散度</strong></li><li><strong>最小化生成器损失，等价于最小化JS散度（也就是优化生成模型）</strong></li></ul></li></ul><h3 id="cgan">cGAN</h3><p>Conditional GAN, 条件生成式对抗网络</p><ul><li>网络结构<ul><li><figure><img src="/2020/02/29/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-4/1582732893033.png" alt="1582732893033"><figcaption aria-hidden="true">1582732893033</figcaption></figure></li><li>为了能够满足条件生成，所以需要添加一个class标签</li><li>对于生成器而言，没啥变化，只是增加了一个label输入</li><li>对于判别器而言，输入为图片以及对应标签；判别器不仅判断图片是否为真，同时也要判断时候和标签匹配<ul><li>真实图片+正确label==》1</li><li>真实图片+错误label==》0</li><li>合成图片+任意label==》0</li></ul></li></ul></li></ul><h3 id="dcgan">DCGAN</h3><p>Deep Convlutional GAN, 深度卷积生成式对抗网络</p><ul><li>原始GAN，使用全连接网络作为判别器和生成器<ul><li>不利于建模图像信息</li><li>参数量大，需要大量的计算资源，难以优化</li></ul></li><li>DCGAN，使用卷积神经网络作为判别器和生成器</li><li>通过大量的<strong>工程实践</strong>，经验性地提出一系列的网络结构和优化策略，来有效的建模图像数据</li><li>判别器<ul><li>通过Pooling下采样，Pooling是不可学习的，这可能造成GAN训练困难</li><li>使用滑动卷积（步长大于1），让其可以学习自己的下采样策略</li><li><figure><img src="/2020/02/29/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-4/1582789203990.png" alt="1582789203990"><figcaption aria-hidden="true">1582789203990</figcaption></figure></li></ul></li><li>生成器：滑动反卷积<ul><li>通过插值法上采样，插值方法是固定的，不可学习的，这可能给训练造成困难</li><li>使用滑动反卷积（进行扩展），让其可以学习自己的上采样策略</li><li><figure><img src="/2020/02/29/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-4/1582789168180.png" alt="1582789168180"><figcaption aria-hidden="true">1582789168180</figcaption></figure></li></ul></li></ul><h3 id="wganwgan-gp">WGAN/WGAN-GP</h3><p>Wasserstein GAN with Weight Clipping/ Gradient Penalty</p><ul><li><p>原始GAN存在的问题</p><ul><li>训练困难：生成器无法生成想要的数据</li><li>模式崩塌：生成器无法学习到完整的数据分布</li></ul></li><li><p>JS散度</p><ul><li>已知GAN网络的目标函数<ul><li><figure><img src="/2020/02/29/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-4/1582730817910.png" alt="1582730817910"><figcaption aria-hidden="true">1582730817910</figcaption></figure></li></ul></li><li>所以可以看到GAN网络的训练效果是和JS散度相关联的</li><li><figure><img src="/2020/02/29/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-4/1582872903044.png" alt="1582872903044"><figcaption aria-hidden="true">1582872903044</figcaption></figure><ul><li>分析任意一个点x对JS散度的贡献：<ul><li>当 <span class="math inline">\(P_1(x) = 0\)</span> and <span class="math inline">\(P_2(x)=0\)</span>（即该数据既没有在真实数据中出现，也没有在生成数据中出现）<ul><li><span class="math inline">\(JS(P_1||P_2)=0\)</span>，对计算JS散度无贡献</li></ul></li><li>当 <span class="math inline">\(P_1(x) \not= 0\)</span> and <span class="math inline">\(P_2(x)=0\)</span> 或 <span class="math inline">\(P_1(x) = 0\)</span> and <span class="math inline">\(P_2(x) \not=0\)</span><ul><li>贡献等与常数 <span class="math inline">\(log2\)</span>，但梯度等于零</li></ul></li><li>当 <span class="math inline">\(P_1(x) \not= 0\)</span> and <span class="math inline">\(P_2(x)\not=0\)</span><ul><li>对计算JS散度有贡献且不为常熟，因此梯度不为零</li></ul></li></ul></li></ul></li><li>但 <span class="math inline">\(P_{data}(x)\)</span> 与 <span class="math inline">\(P_g(x)\)</span>发生不重叠（或重叠部分可忽略）的可能性非常大，即<span class="math inline">\(P_1(x) \not= 0\)</span> and <span class="math inline">\(P_2(x)\not=0\)</span>这种情况发生概率很小</li><li>GAN：真实数据分布 <span class="math inline">\(P_{data}\)</span>和生成数据分布 <span class="math inline">\(P_G\)</span>是高维空间中的维度流形，它们重叠的区域可以忽略不记（<strong>能够提供梯度信息的数据</strong>可以忽略不记）===》所以无论它们相距多少，其JS散度都是常数（仅当完全重合时，JS散度为零），导致生成器的梯度（近似）为零，造成梯度消失===》GAN优化困难</li></ul></li><li><p>Wasserstein距离</p><ul><li><figure><img src="/2020/02/29/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-4/1582874596249.png" alt="1582874596249"><figcaption aria-hidden="true">1582874596249</figcaption></figure></li><li><p><span class="math inline">\(W(P_1,P_2) = inf_{γ\sim\prod(P_1,P_2)E_{(x,y)\simγ}[||x-y||]}\)</span></p><ul><li><p><span class="math inline">\(\prod(P_1,P_2)\)</span>：P1和P2组合起来的<strong>所有可能的联合分布的集合</strong></p></li><li><p><span class="math inline">\(|| x - y ||\)</span>：样本x和y的距离</p></li><li><p>inf：所有可能的下界</p></li><li><p>假设对于P1和P2上的点x，y有一个联合分布γ；在这个联合分布上取一对点(x,y)，计算他们的距离<span class="math inline">\(||x-y||\)</span>，然后通过很多的点，计算出它的期望；然后我们再遍历所有可能的联合分布，得到一个最小的距离，就是我们定义的W距离。</p><ul><li><p>e.g</p></li><li><p>假设有两个离散的分布，每个分布只包含两个点（A、B和C、D），每个点的出现概率相等，横竖距离均为1</p></li><li><figure><img src="/2020/02/29/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-4/1582881754386.png" alt="1582881754386"><figcaption aria-hidden="true">1582881754386</figcaption></figure></li><li><p>通过这个分布，可以计算出距离 <span class="math inline">\(||x-y||\)</span></p><ul><li><table><thead><tr class="header"><th>距离</th><th>C</th><th>D</th></tr></thead><tbody><tr class="odd"><td>A</td><td>1</td><td><span class="math inline">\(\sqrt{2}\)</span></td></tr><tr class="even"><td>B</td><td><span class="math inline">\(\sqrt{2}\)</span></td><td>1</td></tr></tbody></table></li></ul></li><li><p>下面讨论所有可能出现的联合分布形式，下面仅举例其中可能的两种</p><ul><li><figure><img src="/2020/02/29/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-4/1582882144751.png" alt="1582882144751"><figcaption aria-hidden="true">1582882144751</figcaption></figure></li><li>在联合分布1下，可以计算出期望 <span class="math inline">\(2\sqrt2\)</span></li><li>同理，在联合分布2下，计算得出期望为2（可以得到是所有可能的联合分布中最小的期望值）</li></ul></li><li><p>所以得到W距离为2</p></li></ul></li><li><p>可以理解为：将分布P1移动到分布P2，已知彼此距离 <span class="math inline">\(||x-y||\)</span>，选择一个合适的移动方案γ（联合分布），让走的路径最小，就是W距离。</p><ul><li>在最优的路径规划γ下，把土堆从P1移动到P2所需的路径（最短距离消耗）</li></ul></li><li><p>但inf（穷举所有可能，取下界）项无法直接求解</p></li></ul></li><li><figure><img src="/2020/02/29/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-4/1582883657973.png" alt="1582883657973"><figcaption aria-hidden="true">1582883657973</figcaption></figure><ul><li>为了解决其中李普希思系数存在的限制，使用了权重截断的方法</li><li>权重截断：在每次优化判别器D之后，把D的权重的绝对值限制在某个范围</li></ul></li><li><figure><img src="/2020/02/29/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-4/1582883701722.png" alt="1582883701722"><figcaption aria-hidden="true">1582883701722</figcaption></figure></li></ul></li><li><p>WGAN-GP</p><ul><li>WGAN存在的问题<ul><li>权重二值化：WGAN几乎所有参数都是正负0.01，浪费神经网络的拟合能力（权重截断）</li><li>梯度衰减、爆炸（权重截断的截断阈值）<ul><li><figure><img src="/2020/02/29/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-4/1582888577293.png" alt="1582888577293"><figcaption aria-hidden="true">1582888577293</figcaption></figure></li></ul></li></ul></li><li>WGAN-GP：把Lipschitz限制作为一个正则项加到Wasserstein损失上，在优化GAN损失的同时，尽可能满足Lipschitz限制<ul><li><figure><img src="/2020/02/29/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-4/1582888937806.png" alt="1582888937806"><figcaption aria-hidden="true">1582888937806</figcaption></figure></li></ul></li></ul></li></ul><h3 id="progressive-gan">Progressive GAN</h3><figure><img src="/2020/02/29/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-4/1582890752823.png" alt="1582890752823"><figcaption aria-hidden="true">1582890752823</figcaption></figure><h3 id="sn-gan">SN-GAN</h3><p>谱归一化层实现利普希茨连续</p><ul><li>WGAN：权重截断--&gt; 权重二值化、梯度消失/爆炸</li><li>WGAN-GP：惩罚项--&gt; 收敛速度仍然慢于DCGAN</li><li>SN-GAN：每层权重除以该层矩阵谱范数即可满足利普希茨连续</li></ul><h3 id="self-attention-gan">Self-Attention GAN</h3><p>建模长距离依赖关系</p><ul><li>原始GAN存在的问题<ul><li>局部效果很逼真，全局效果不逼真</li></ul></li><li>引入自注意力机制<ul><li>建立一个长距离依赖关系</li><li><figure><img src="/2020/02/29/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-4/1582891406092.png" alt="1582891406092"><figcaption aria-hidden="true">1582891406092</figcaption></figure></li><li>让最终的输出每一部分不仅仅是和对应的感受野相关，也和整体全局相关联</li></ul></li></ul><h3 id="biggan">BigGAN</h3><ul><li>增大Batch Size</li><li>噪声不单单加在输入层，还加在中间层</li><li>截断技巧<ul><li>在一个分布中采样噪声，如果噪声超过阈值，则丢弃再次采样</li><li>阈值越大，图片多样性越好，但图片质量会略微下降</li><li>阈值越小，图片多样性越差，但图像质量更好</li></ul></li></ul><h3 id="实战项目-基础">实战项目-基础</h3><ul><li>图像翻译<ul><li>学习图像到图像的映射<ul><li>黑白图像--&gt;彩色图像：学习彩色图像的条件分布</li></ul></li></ul></li><li>U-NET<ul><li>神经网络中，浅层卷积核提取Low-Level特征，深层卷积核提取High-Level特征</li><li>High-Level特征：分类、检测需要对图像深层理解等任务</li><li>Low-Level特征：保留更多图像细节</li><li>图像翻译任务：需要Low-Level和High-Level 特征</li><li><figure><img src="/2020/02/29/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-4/1582897625212.png" alt="1582897625212"><figcaption aria-hidden="true">1582897625212</figcaption></figure></li><li>ResGenerator<ul><li>在残差网络中，通过残差的形式，不断向后传递</li><li>同时还解决了退化的问题</li><li><figure><img src="/2020/02/29/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-4/1582898041996.png" alt="1582898041996"><figcaption aria-hidden="true">1582898041996</figcaption></figure></li></ul></li></ul></li></ul><h3 id="pixel2pixel">Pixel2Pixel</h3><p>用cGAN实现图像翻译</p><ul><li>已知我们有一一对应的数据集（例如：轮廓&lt;--&gt;建筑）</li><li>使用cGAN<ul><li>首先输入轮廓和随机噪声，通过生成器生成建筑图像，再将输入和建筑图像输入到判别其中</li><li><figure><img src="/2020/02/29/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-4/1582962063550.png" alt="1582962063550"><figcaption aria-hidden="true">1582962063550</figcaption></figure></li><li>判别器进行判断，输出判别值</li><li><figure><img src="/2020/02/29/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-4/1582962144960.png" alt="1582962144960"><figcaption aria-hidden="true">1582962144960</figcaption></figure></li><li><figure><img src="/2020/02/29/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-4/1582962180368.png" alt="1582962180368"><figcaption aria-hidden="true">1582962180368</figcaption></figure></li></ul></li></ul><h3 id="深度学习三要素">深度学习三要素</h3><figure><img src="/2020/02/29/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-4/1582962229023.png" alt="1582962229023"><figcaption aria-hidden="true">1582962229023</figcaption></figure><h3 id="cyclegan">CycleGAN</h3><p>无须paired数据，学习图像翻译</p><ul><li>有两组数组，但彼此之间并不是一一对应</li><li>因为没有很好的全局损失信息，可能会出现生成的结果不能保持对应的位置信息等</li><li>eg<ul><li>CycleGAN中，首先将斑马生成马，然后再将马重新返回生成斑马，然后比对原始斑马和生成的斑马的损失，即可以保存一定的位置等信息</li><li><figure><img src="/2020/02/29/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-4/1582962960739.png" alt="1582962960739"><figcaption aria-hidden="true">1582962960739</figcaption></figure></li></ul></li><li>循环一致性损失<ul><li>·对于输入图像：将生成图像重建回输入图像，应当让原始输入图像和重建的输入图像尽可能相似</li><li><figure><img src="/2020/02/29/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-4/1582963179840.png" alt="1582963179840"><figcaption aria-hidden="true">1582963179840</figcaption></figure></li><li>同理，有</li><li><figure><img src="/2020/02/29/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-4/1582963316159.png" alt="1582963316159"><figcaption aria-hidden="true">1582963316159</figcaption></figure></li><li><figure><img src="/2020/02/29/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-4/1582963520300.png" alt="1582963520300"><figcaption aria-hidden="true">1582963520300</figcaption></figure></li></ul></li></ul><h3 id="stargan">StarGAN</h3><p>一个GAN，多种数据，任意变换</p><ul><li>与GAN相比，其输入多了一个Target domain，输出多了一个Domainclassification，用来分类表情的种类</li><li>于此同时还增加了一个循环一致性损失<ul><li>对于生成器生成的Fakeimage，和期望表情的标签一起返回给生成器，让其尽可能地再次还原出原始的输入图像</li></ul></li><li><figure><img src="/2020/02/29/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-4/1582964132072.png" alt="1582964132072"><figcaption aria-hidden="true">1582964132072</figcaption></figure></li></ul>]]></content>
    
    
    <categories>
      
      <category>study</category>
      
    </categories>
    
    
    <tags>
      
      <tag>深度学习</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>深度学习笔记-3</title>
    <link href="/2020/02/22/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-3/"/>
    <url>/2020/02/22/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-3/</url>
    
    <content type="html"><![CDATA[<p>循环神经网络&amp;基于深度学习的目标检测方法</p><span id="more"></span><h2 id="循环神经网络">循环神经网络</h2><h3 id="绪论">绪论</h3><ul><li>全连接网络-&gt;卷积神经网络<ul><li>全连接网络处理图像问题时，参数太多，容易出现过拟合现象。</li><li>卷积神经网络使用“局部关联，参数共享”的方法</li></ul></li><li>卷积、特征图feature map、padding、深度channel以及池化概念</li><li>eg<ul><li>输入为7x7x3的图像，则卷积核的大小应为nxnx3（此处为3x3x3），即一个卷积核里面有三个矩阵，分别与输入的三个channel做运算，每一个矩阵运算得到一个结果，然后将这些结果求加和，再加上偏置项，得到featuremap中的值。然后根据步长继续进行计算，最终得到完整的featuremap（3x3）</li><li>一个卷积核对应一个featuremap，如果有m个卷积核，那么输出就会有m个feature map（3x3xm）</li><li><figure><img src="/2020/02/22/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-3/1582008762089.png" alt="1582008762089"><figcaption aria-hidden="true">1582008762089</figcaption></figure></li></ul></li><li>卷积神经网络-&gt;循环神经网络<ul><li>传统神经网络、卷积神经网络，输入和输出之间是相互独立的</li><li>RNN可以更好的处理具有时序关系的任务</li><li>RNN通过其循环结构引入“记忆”的概念</li></ul></li></ul><h3 id="基本组成结构">基本组成结构</h3><ul><li>基本结构<ul><li>传统结构对于一些问题，只能处理当前输入，不能够结合之前的输入信息进行处理，所以神经网络需要“记忆”</li><li><figure><img src="/2020/02/22/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-3/1582027665644.png" alt="1582027665644"><figcaption aria-hidden="true">1582027665644</figcaption></figure></li><li>两种输入：正常输入，记忆单元的输入；</li><li>两种输出：正常输出，输出到记忆单元中；</li><li><figure><img src="/2020/02/22/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-3/1582028374107.png" alt="1582028374107"><figcaption aria-hidden="true">1582028374107</figcaption></figure><ul><li>U:从输入到隐藏状态的参数；W:从前一隐藏状态到下一个隐藏状态的参数；V:从隐藏状态到输出的参数；</li><li><span class="math inline">\(x_t\)</span> 是时间t处的输入</li><li><span class="math inline">\(h_t\)</span>是时间t处的记忆，<span class="math inline">\(h_t =f(h_{t-1},x)\)</span>，f可以是双曲正切(tanh)等</li><li><span class="math inline">\(h_t = tanh(Wh_{t-1}+Ux)\)</span></li><li><span class="math inline">\(y_t\)</span>是时间t时刻的输出，<span class="math inline">\(y_t = softmax(Vh_t)\)</span></li><li>函数f被不断利用；模型所需要学习的参数是固定的；这样的话就可以避免因为输入长度的不同而训练不同的网络。</li></ul></li></ul></li><li>深度RNN<ul><li><figure><img src="/2020/02/22/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-3/1582029267085.png" alt="1582029267085"><figcaption aria-hidden="true">1582029267085</figcaption></figure></li></ul></li><li>双向RNN<ul><li><figure><img src="/2020/02/22/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-3/1582029415008.png" alt="1582029415008"><figcaption aria-hidden="true">1582029415008</figcaption></figure></li></ul></li><li>BPTT算法<ul><li>BP算法<ul><li><figure><img src="/2020/02/22/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-3/1582029689391.png" alt="1582029689391"><figcaption aria-hidden="true">1582029689391</figcaption></figure></li><li><figure><img src="/2020/02/22/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-3/1582029751764.png" alt="1582029751764"><figcaption aria-hidden="true">1582029751764</figcaption></figure></li><li>使用sigmoid函数时，可能在链式求导中的某一项为零，则整个求导为零，出现梯度消失</li></ul></li><li>RNN的基本公式<ul><li><span class="math inline">\(h_t = tanh(Wh_{t-1}+Ux)\)</span></li><li><span class="math inline">\(y_t = softmax(Vh_t)\)</span></li></ul></li><li><figure><img src="/2020/02/22/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-3/1582030126686.png" alt="1582030126686"><figcaption aria-hidden="true">1582030126686</figcaption></figure></li><li><figure><img src="/2020/02/22/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-3/1582030169096.png" alt="1582030169096"><figcaption aria-hidden="true">1582030169096</figcaption></figure><ul><li>所以可以在此基础上继续展开，最终得到以下公式</li><li><figure><img src="/2020/02/22/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-3/1582030321778.png" alt="1582030321778"><figcaption aria-hidden="true">1582030321778</figcaption></figure></li></ul></li></ul></li></ul><h3 id="循环神经网络的变种">循环神经网络的变种</h3><ul><li>传统RNN的问题<ul><li>当循环神经网络在时间维度上非常深的时候，会导致梯度消失或者梯度爆炸的问题</li><li><span class="math inline">\(h_t = tanh(Wh_{t-1}+Ux)\)</span>，对其求偏导 <span class="math inline">\(= tanh&#39; * W\)</span></li><li><figure><img src="/2020/02/22/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-3/1582030823126.png" alt="1582030823126"><figcaption aria-hidden="true">1582030823126</figcaption></figure></li><li>梯度爆炸的改进<ul><li>权重衰减</li><li>梯度阶段：检查误差梯度的值是否超过阈值，吐过超过了那么就截断梯度，并将梯度设置为阈值</li></ul></li><li>梯度消失导致的问题：长时依赖问题<ul><li>随着时间间隔的不断增大，RNN会丧失学习到链接如此远的信息的能力</li><li>改进模型：LSTM，GRU</li></ul></li></ul></li><li>LSTM（long short-term memory长短期记忆模型）<ul><li>LSTM拥有三个门（遗忘门，输入门，输出门）</li><li><figure><img src="/2020/02/22/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-3/1582035415196.png" alt="1582035415196"><figcaption aria-hidden="true">1582035415196</figcaption></figure><ul><li>遗忘门：<span class="math inline">\(f_t = σ(W_f[h_{t-1},x_t] +b_f)\)</span> σ为sigmoid函数</li><li>sigmoid层输出0-1之间的值，描述每个部分有多少量可以通过。0表示“不允许”，1表示“允许量通过”</li></ul></li><li><figure><img src="/2020/02/22/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-3/1582035839037.png" alt="1582035839037"><figcaption aria-hidden="true">1582035839037</figcaption></figure><ul><li>输入门：<span class="math inline">\(i_t = σ(W_i[h_{t-1},x_t] +b_i)\)</span> ， <span class="math inline">\(\overline C _i =tanh(W_C[h_{t-1},x_t] + b_C)\)</span></li><li>首先经过Sigmoid层决定什么信息需要更新，然后通过tanh层输出备选的需要更新的内容，然后加入新的状态中；0代表“不更新”，1 就指“完全更新”</li></ul></li><li><figure><img src="/2020/02/22/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-3/1582036819558.png" alt="1582036819558"><figcaption aria-hidden="true">1582036819558</figcaption></figure><ul><li>得到新的<span class="math inline">\(C_t\)</span> ：<span class="math inline">\(C_t = f_t * C_{t-1} + i_t * \overlineC_t\)</span></li><li><span class="math inline">\(f_t * C_{t-1}\)</span>忘掉不需要的记忆信息；<span class="math inline">\(i_t * \overlineC_t\)</span> 加入需要更新的出入信息</li></ul></li><li><figure><img src="/2020/02/22/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-3/1582037147080.png" alt="1582037147080"><figcaption aria-hidden="true">1582037147080</figcaption></figure><ul><li>输出门：<span class="math inline">\(o_t = σ(W_o[h_{t-1},x_t] +b_o)\)</span> ， <span class="math inline">\(h_t = o_t *tanh(C_t)\)</span></li><li>首先，通过sigmoid来确定细胞状态的哪个部分将输出出去。然后，将细胞状态通过tanh进行处理并将它和sigmoid门的输出相乘，最终仅仅会输出我们确定输出的那部分;0 代表“不输出”，1就指“完全输出"</li></ul></li><li>RNN的“记忆”在每个时间点都会被新的输入覆盖；但LSTM中“记忆”是与新的输入相加（各自乘上一定的比例），一种线性操作</li><li>LSTM：如果前边的输入对 <span class="math inline">\(c_t\)</span>产生了影响，那这个影响会一直存在，除非遗忘门的权重为0</li></ul></li><li>LSTM变形<ul><li><figure><img src="/2020/02/22/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-3/1582038107952.png" alt="1582038107952"><figcaption aria-hidden="true">1582038107952</figcaption></figure></li></ul></li><li>GRU（Gated Recurrent Unit 门控循环单元）<ul><li><figure><img src="/2020/02/22/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-3/1582039332224.png" alt="1582039332224"><figcaption aria-hidden="true">1582039332224</figcaption></figure></li><li>GRU只有两个门，分别为重置门和更新门<ul><li>重置门：控制忽略前一时刻的状态信息的程度，重置门越小说明忽略的越多</li><li>更新门：控制前一时刻的状态信息被带入到当前状态中的程度，更新门值越大表示前一时刻的状态信息带入越多</li></ul></li></ul></li></ul><h3 id="拓展">拓展</h3><ul><li>基于attention的RNN<ul><li>受到人类注意力机制的启发，根据需求将注意力集中到图像的特定部分。</li></ul></li><li><img src="/2020/02/22/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-3/1582040392538.png"><ul><li>首先输入图像通过CNN，得到特征图（14x14x512）；</li><li>然后根据当前记忆学习到一个attention的权重矩阵a1(14x14)，并且权重矩阵在每个channel上是共享的</li></ul></li><li><figure><img src="/2020/02/22/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-3/1582040532172.png" alt="1582040532172"><figcaption aria-hidden="true">1582040532172</figcaption></figure><ul><li>然后将权重矩阵a1与每一个featuremap做运算，得到一个向量z1(1x512)</li><li>给出一个学习信号y1</li></ul></li><li><figure><img src="/2020/02/22/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-3/1582040770081.png" alt="1582040770081"><figcaption aria-hidden="true">1582040770081</figcaption></figure><ul><li>经过h1可以得到新的权重矩阵a2和输出d1，通过d1确定输出文字</li><li>然后权重矩阵a2与feature map运算，得到向量z2</li><li>通过这种形式一直运算下去，可以得到描述这幅图片的一句话</li></ul></li></ul><h2 id="基于深度学习的目标检测方法">基于深度学习的目标检测方法</h2><ul><li>面临的挑战<ul><li>太多尺度/位置的样本需要被测试</li><li>目标可能存在的情况太多</li></ul></li><li>评价准则<ul><li>准确率（Precision）：正确的框选物体个数a/所有的框选物体个数n</li><li>召回率（Recall）：正确的框选物体个数a/图像样本中应被框选的物体个数m（固定）</li><li>交并比（IOU,Intersection-over-union）：正确的标准框和算法给出的框，两者的交集/两者的并集</li><li>平均准确率（AP,Average Precision）<ul><li><figure><img src="/2020/02/22/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-3/1582102501162.png" alt="1582102501162"><figcaption aria-hidden="true">1582102501162</figcaption></figure></li><li>当算法框选的物体n越多，那么正确的框选物体个数a也会增多，使得召回率a/m变高。但这些框选的物体n中正确的框选物体a所占的比例可能会下降a/n，所以准确率下降。围成的面积就是平均准确率</li></ul></li></ul></li><li>目标候选生成 Selective Search（SS）<ul><li>类似一种层次聚类算法</li><li>首先在图像中选择一些种子点，然后这些种子点向周围聚合扩大，直到把整张图片覆盖</li><li>在种子点扩大的过程中，算法会检测当前所包含的范围中的物体性的强弱；如果物体性较强，则产生一个候选框</li></ul></li><li>难样本挖掘<ul><li>在训练过程中，给予难以分辨的负样本更大的权重</li></ul></li><li>非极大值抑制<ul><li>在同一个目标，可能算法给出了多个框。这时候，把与指定框overlap大于一定阈值（通常为0.5）的其他框移除</li></ul></li><li>两阶段目标检测方法<ul><li>输入图片，得到目标候选</li><li>对候选框进行分类操作</li></ul></li></ul><h3 id="两阶段目标检测算法">两阶段目标检测算法</h3><h3 id="r-cnn">R-CNN</h3><ul><li>用Selective Search方法产生目标候选<ul><li>并对目标候选进行规整（Warped），使得所有目标候选变成统一尺寸</li></ul></li><li>利用CNN提取特征<ul><li>首先使用ImageNet等数据集，训练一个用于图片分类的CNN模型（有1000个分类输出）</li><li>移除最后一个FC层（ImageNet分类层），替换为一个小的FC层（关注的21个类别，即21个分类输出的全连接层）并进行Fine-Tune（微调）</li><li>最后将规整好的目标候选输入到CNN网络中，提取CNN特征<ul><li>pool5属于比较底层的特征，fc6和fc7属于高层特征，所以这里使用的是fc7层的特征</li></ul></li></ul></li><li>利用SVM进行分类<ul><li>为什么还要单独训练SVM？（CNN已经可以完成分类任务）<ul><li>CNN需要大量的训练样本（对样本质量的要求松）</li><li>SVM用少量的样本就可以训练（对样本质量的要求严）</li></ul></li></ul></li><li>存在不足<ul><li>每个目标候选都进行特征提取，花费大量时间、空间且多有重叠部分</li><li>训练过程分步完成不是end-to-end（端到端：在一个神经网络里面整体完成）</li><li>Warp操作会使图片失真</li></ul></li></ul><h3 id="spp-net">SPP-NET</h3><p>Spatial PyramidPolling（空间金字塔Pooling），解决了每个目标候选都要提取特征的不足</p><ul><li>在R-CNN中，一张输入图片中的每一个目标候选都要进行Warp操作，然后输入到CNN中进行特征提取</li><li>而在SPP-NET中，进行了改进操作<ul><li>首先将整张图片输入到CNN网络中，然后得到整体的特征图（在全连接层之前的操作，如下图）</li><li><figure><img src="/2020/02/22/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-3/1582192293675.png" alt="1582192293675"><figcaption aria-hidden="true">1582192293675</figcaption></figure></li><li>在特征图的层次上，根据目标候选（SS）的空间位置，将对应位置的region进行SSPPolling（多级pooling操作），最终得到一个定长的数组，输入到全连接层中（如下图）</li><li><figure><img src="/2020/02/22/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-3/1582192012486.png" alt="1582192012486"><figcaption aria-hidden="true">1582192012486</figcaption></figure></li><li>这样做就有效的解决了“每一个目标候选都要进行Warp操作”的麻烦</li></ul></li></ul><h3 id="fast-r-cnn">Fast R-CNN</h3><figure><img src="/2020/02/22/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-3/1582192982313.png" alt="1582192982313"><figcaption aria-hidden="true">1582192982313</figcaption></figure><ul><li>将整张图片作为CNN的输入，得到特征图</li><li>在特征图的层次上，根据目标候选（SS）的空间位置，将对应位置的region进行RolPooling（一个简单SPPPooling，只具有一个空间层级），得到一个定长的特征向量，输入到全连接层中</li><li>最后用softmax进行分类以及一个全连接层进行边界框回归</li><li>但依旧是使用SS得到目标候选，在一定程度上限制了速度</li></ul><h3 id="faster-r-cnn">Faster R-CNN</h3><figure><img src="/2020/02/22/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-3/1582193523490.png" alt="1582193523490"><figcaption aria-hidden="true">1582193523490</figcaption></figure><ul><li>基本和FastR-CNN一样，不过在确定目标候选的方法上，其不再使用SS，而是在最后一个卷积层后面插入一个RegionProposalNetwork（RPN）；RPN可以直接产生区域候选，不需要额外算法产生区域候选</li><li><figure><img src="/2020/02/22/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-3/1582201158081.png" alt="1582201158081"><figcaption aria-hidden="true">1582201158081</figcaption></figure></li><li>Region Proposal Network（RPN）<ul><li><figure><img src="/2020/02/22/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-3/1582198850735.png" alt="1582198850735"><figcaption aria-hidden="true">1582198850735</figcaption></figure></li><li>设置了一个滑动窗，在滑动窗口中有k个锚点框（anchorboxes），这些锚点框在滑动窗口中的位置保持不变</li><li>在滑动窗口的滑动过程中，锚点框会在特征图的对应位置进行运算，得到一个256位的向量，把这个向量输入到回归器和分类器中进行学习，得到输出</li><li>回归器为每个锚点框计算偏移（回归边界框）</li><li>分类器评测每个锚点框（回归后的）是物体的可能性</li></ul></li><li>共享特征<ul><li><figure><img src="/2020/02/22/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-3/1582199838040.png" alt="1582199838040"><figcaption aria-hidden="true">1582199838040</figcaption></figure></li><li>首先训练得到一个RPN网络（使用一个已有网络，如AlexNet）</li><li>使用这个RPN网络得到一些目标候选框数据</li><li>然后使用这些目标候选框数据和RPN网络训练得到Fast R-CNN网络</li><li>固定该FastR-CNN网络的卷积层参数，然后使用该网络重新训练得到一个新的RPN</li><li>使用新的RPN得到新的目标候选框数据</li><li>使用新的RPN和新的目标候选框数据再次训练FastR-CNN网络，其中卷积层参数依然固定</li><li>最后将Fast R-CNN网络和RPN合并即可</li></ul></li><li>存在不足：不能实现实时检测（检测一张图片约耗时0.2秒）</li></ul><h3 id="fpn">FPN</h3><figure><img src="/2020/02/22/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-3/1582201318737.png" alt="1582201318737"><figcaption aria-hidden="true">1582201318737</figcaption></figure><p>将高层的低分辨率、高语义信息的特征 和浅层的高分辨率、低语义信息的特征 自然地结合</p><h3 id="rfcn">RFCN</h3><ul><li>出发点：图片分类的平移不变性与物体检测之间的平移变换性之间的矛盾</li><li><figure><img src="/2020/02/22/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-3/1582202475881.png" alt="1582202475881"><figcaption aria-hidden="true">1582202475881</figcaption></figure><ul><li>首先也是通过一个卷积网络得到feature map</li><li>然后再加入一个卷积网络输出 <span class="math inline">\(k^2(C+1)\)</span> 个feature map<ul><li><span class="math inline">\(k^2\)</span> 代表的就是位置</li></ul></li><li>如上图，例如在最浅色的长方体，在进行pooling操作的时候只保留最右下角的块（位置敏感）；即不同的长方体块，在进行pooling操作的时候保留不同的位置块进行输出（学习其位置特征）</li><li>然后将这些pooling选择出来的块组合起来</li></ul></li></ul><h3 id="单阶段目标检测算法">单阶段目标检测算法</h3><h3 id="yolo">YOLO</h3><figure><img src="/2020/02/22/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-3/1582203943497.png" alt="1582203943497"><figcaption aria-hidden="true">1582203943497</figcaption></figure><ul><li>把输入图像划分为S*S个网格（S=7）。位于目标中心的网格负责检测该目标</li><li>每个网格会预测目标的可能的B个边界框和这个边界框是物体的概率（Objectness）；具体的，每个边界框会预测出5个值：x,y,w,h和置信度Pr*IOU（交并比）</li><li>每个网格预测C个条件概率Pr（所有的目标种类个数）===》于是得到一个7x7xC的立方体</li><li>在测试阶段，预测每个检测框的分数</li><li><figure><img src="/2020/02/22/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-3/1582204639029.png" alt="1582204639029"><figcaption aria-hidden="true">1582204639029</figcaption></figure></li><li>损失函数<ul><li><figure><img src="/2020/02/22/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-3/1582204821788.png" alt="1582204821788"><figcaption aria-hidden="true">1582204821788</figcaption></figure></li></ul></li><li>存在不足：小物体、重叠物体无法检测</li></ul><h3 id="ssd">SSD</h3><p>Single Shot MultiBox Detector（单阶段多框的检测器）</p><figure><img src="/2020/02/22/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-3/1582207522957.png" alt="1582207522957"><figcaption aria-hidden="true">1582207522957</figcaption></figure><ul><li>和Faster R-CNN中的RPN中的锚点框十分类似<ul><li>在3x3的卷积核中设置了默认框，将默认框进行回归和分类，输出默认框的类别和回归后的尺寸</li><li>对于每个尺寸为m xn，通道数为p的特征层，SSD使用3x3xp的卷积核去产生对应于多个defaultboxes的物体概率分布（分类）和相对偏移（回归）</li><li><figure><img src="/2020/02/22/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-3/1582207917178.png" alt="1582207917178"><figcaption aria-hidden="true">1582207917178</figcaption></figure></li><li>对不同尺度featuremap中目标进行预测，用到多尺度特征的融合（和FPN相类似），提高检测精度</li><li>运用底层特征可以检测出小物体</li><li>同时提高速度和精度</li></ul></li></ul><h3 id="retinanet">RetinaNet</h3><ul><li><p>单阶段方法效果差</p><ul><li>模型聚焦于大量的背景样本中（两阶段方法中RPN预筛选了大量背景样本）</li><li>直接对大量样本进行分类</li><li>解决思路：降低背景样本在训练中的权重</li></ul></li><li><p>提出了一种Focal Loss</p></li><li><figure><img src="/2020/02/22/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-3/1582210512032.png" alt="1582210512032"><figcaption aria-hidden="true">1582210512032</figcaption></figure></li><li><p>蓝色线代表交叉熵</p><ul><li>可以看出，当背景框的预测接近0.8的时候（预测较为准确），但其损失loss仍为0.1、0.2的水品。这就会导致大量的背景样本仍可能会聚集产生比较大的损失，导致网络聚焦在背景样本上，而不是聚焦在想要预测物品上</li></ul></li><li><p>紫色线代表Focal Loss</p><ul><li>可以看出，在0.8左右的loss已经很小，所以不会让网络聚焦在这里，从而提高准确性</li></ul></li></ul><h3 id="视频中目标检测">视频中目标检测</h3><ul><li>DFF（Deep Feature Flow）<ul><li>利用当前关键帧进行特征提取，然后计算下一帧的光流信息，将关键帧的特征信息和光流信息输入到新的任务中，得到下一帧的预测信息（利用光流信息指导特征传递）</li><li><figure><img src="/2020/02/22/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-3/1582209322033.png" alt="1582209322033"><figcaption aria-hidden="true">1582209322033</figcaption></figure></li></ul></li></ul>]]></content>
    
    
    <categories>
      
      <category>study</category>
      
    </categories>
    
    
    <tags>
      
      <tag>深度学习</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>深度学习笔记-2</title>
    <link href="/2020/02/22/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-2/"/>
    <url>/2020/02/22/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-2/</url>
    
    <content type="html"><![CDATA[<p>数学基础&amp;卷积神经网络</p><span id="more"></span><h2 id="数学基础">数学基础</h2><p>主要知识：线性代数，概率论，最优化</p><ul><li>矩阵线性变换<ul><li>Ax = λx</li><li>x为A的特征向量，λ为特征值</li><li>即特征向量对矩阵A的作用只有尺度变化而没有方向变化</li><li>从而实现神经网络每一层的放大/缩小以及旋转操作</li></ul></li><li>秩<ul><li>行列之间的相关性</li><li><strong>数据分布模式越容易捕获</strong>，即需要的基越少，秩就越小</li><li><strong>数据的冗余度越大</strong>（重复信息），需要的基就越少，秩越小</li><li>数据降维<ul><li>较大奇异值包含了矩阵的主要信息</li><li>只保留前r个较大奇异值及其对应的特征向量，可实现数据降维</li><li><img src="/2020/02/22/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-2/1.jpg"></li><li>保留决定数据分布的最主要的模式（丢弃的可能是噪声或其他不关键信息）</li><li>推荐系统，图像去噪</li></ul></li></ul></li><li>机器学习三要素<ul><li>模型（问题建模，确定假设空间）<ul><li>确定概率形式（函数形式）</li><li>能够从概率形式推导出对应的函数形式（高斯分布-&gt;线性回归）</li></ul></li><li>策略（确定目标函数）<ul><li>极大似然估计（经验风险最小化）</li><li>策略设计：训练误差-&gt;泛化误差</li><li><img src="/2020/02/22/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-2/2.jpg"></li><li>策略设计：无免费午餐定理<ul><li>当考虑在所以问题的平均性能的时候，任意两个模型都是相同的</li></ul></li><li>策略设计：奥卡姆剃刀原理<ul><li>如无必要，勿增实体；即简单有效原理</li><li>如果多种模型能够同等程度地符合一个问题的观测结果，应该选择其中使用假设最小的——最简单的模型</li><li>欠拟合<ul><li>训练集的一般性质尚未被学习好（训练误差大）</li><li>提过模型复杂度</li></ul></li><li>过拟合<ul><li>学习器把训练集特点当作样本的一般特点（训练误差小，测试误差大）</li><li>降低模型复杂度</li><li>数据增广（数据集越大，越不容易过拟合）</li></ul></li></ul></li></ul></li><li>算法（求解模型参数）</li></ul></li><li>频率学派 vs 贝叶斯学派<ul><li>频率学派<ul><li>关注可独立重复的随机试验中某个事件发生的频率</li><li>事件发生频率的极限值-&gt;发生的概率会收敛到真实的概率</li><li>模型参数是唯一的，需要从有限的观测数据中估计参数值</li><li>统计机器学习</li></ul></li><li>贝叶斯学派<ul><li>关注随机事件的“可信程度”（无法重复）</li><li>可能性 =假设+数据；数据的作用是对初识假设作出修正，使观测者对概率的主观认识（先验）更接近客观实际（观测）</li><li>模型参数本身是随机变量，需要估计参数的整个概率分布</li><li>概率图模型</li></ul></li><li><img src="/2020/02/22/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-2/3.jpg"></li></ul></li></ul><h2 id="卷积神经网络">卷积神经网络</h2><h3 id="绪论">绪论</h3><ul><li>常用步骤<ul><li>搭建神经网络结构</li><li>找到合适的损失函数（交叉熵损失、均方误差）</li><li>找到一个合适的优化函数，更新参数（反向传播BP、随机梯度下降）</li></ul></li><li>传统神经网络 vs 卷积神经网络<ul><li>全连接网络处理图像的问题<ul><li>参数太多：权重矩阵的参数太多-&gt;过拟合</li></ul></li><li>卷积神经网络的解决方式<ul><li>局部关联，参数共享</li><li>使用卷积核，每一个节点对应的<strong>参数个数就是卷积核大小（加上偏置项）</strong></li></ul></li></ul></li></ul><h3 id="基本组成结构">基本组成结构</h3><ul><li><p>卷积 Convolutional Layer</p><ul><li><img src="/2020/02/22/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-2/4.jpg"></li><li>感受野：input的大小</li><li>特征图 feature map ：对应的输出</li><li>步长：每次滑动的距离</li><li>计算方式：求input和kernel的内积（对应位置相乘，然后求和），得到特征图中对应位置的值</li><li><strong>深度 channel</strong>：比如rgb三层深度；<ul><li>与之对应的，每个卷积核中也会有三个矩阵，分别对应处理三层rgb的输入</li><li>然后会得到三个内积值，然后加和，再加上偏置项，得到特征图中对应位置的图</li><li>每一个卷积和对应一个特征图，即每一次的卷积操作，<strong>输出的深度等于卷积操作中的卷积核个数</strong></li></ul></li><li>padding<ul><li>对输入的四周都进行拓宽</li></ul></li><li><strong>输出特征图的大小</strong><ul><li><span class="math inline">\((N - F)/stride +1\)</span></li><li>输入大小N减去感受野大小F，除以步长，加一</li><li>有padding时，<span class="math inline">\((N + padding *2 -F)/stride+1\)</span></li></ul></li></ul></li><li><p>池化 Pooling Layer</p><ul><li>将一个feature map进行缩放操作</li><li>保留了主要特征的同时减少参数与计算量，防止过拟合，提高模型繁华能力</li><li>处在卷积层与卷积层之间，全连接层与全连接层之间</li><li>Max pooling（最大值池化）/Average pooling（平均池化）</li><li>在池化过程中，也是有对应的filter和stride（进行池化操作，非卷积操作）</li></ul></li><li><p>全连接</p><ul><li>两层之间所有神经元都有权重连接（参数量最大）</li><li>通常在卷积神经网络尾部</li></ul></li></ul><h3 id="卷积神经网络典型结构">卷积神经网络典型结构</h3><h4 id="alexnet">AlexNet</h4><ul><li><img src="/2020/02/22/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-2/5.jpg"><ul><li>成功原因<ul><li>大数据训练 ImageNet</li><li>非线性激活函数 ReLU</li><li>防止过拟合 Dropout，Data augmentation（数据增强）</li><li>其他 双GPU实现（网络被分为上下两部分）</li></ul></li><li>激活函数<ul><li><img src="/2020/02/22/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-2/6.jpg"><ul><li>ReLU函数解决了梯度消失问题（导数恒为一）；计算速度快；收敛速度远快于sigmoid</li></ul></li><li>DropOut（随机失活）<ul><li>训练时随机关闭部分神经元，测试时整合所有神经元（防止过拟合）</li></ul></li><li>Data augmentation（数据增强）<ul><li>平移、翻转、对称</li><li>改变rgb通道强度</li></ul></li></ul></li></ul></li></ul><h4 id="zfnet">ZFNet</h4><p><img src="/2020/02/22/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-2/7.jpg"></p><h4 id="googlenet">GoogleNet</h4><ul><li>增加了两个辅助分类器</li><li>参数量大概是Alexnet的1/2</li><li>没有额外的全连接层（FC层）</li><li>inception模块<ul><li>初衷：多卷积核增加特征多样性</li><li>不同的卷积核操作之后，会有不同的channel和featuremap大小，然后进行padding操作，再将多个卷积核的操作结构进行串联即可</li><li><img src="/2020/02/22/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-2/8.jpg"></li><li>但会存在问题，就是随着网络的进行，channel会越来越大，导致计算越来越复杂</li><li>解决思路：插入1*1卷积核进行降维</li><li><img src="/2020/02/22/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-2/9.jpg"></li><li><img src="/2020/02/22/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-2/10.jpg"></li></ul></li></ul><h4 id="resnet残差学习网络">ResNet（残差学习网络）</h4><p><img src="/2020/02/22/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-2/11.jpg"></p><ul><li>这种残差形式的网络，输入x输出F(x)+x，这样就避免了梯度消失的问题；同时网络自身也可以进行学习，例如某一部分的输出F(x)为零，即该部分网络无用，所以网络自身可以根据任务需求，自动学习到一个适当的网络层数。</li></ul><h3 id="代码实践-tensorflow-cnn">代码实践 Tensorflow-CNN</h3><p>因为之前学习过这个视频内容，所以这一部分代码自己也动手实践过，写在了自己之前的blog上。链接地址：https://hyzs1220.github.io/2019/11/17/CNN-%E7%AE%80%E5%8D%95%E5%9B%BE%E7%89%87%E5%88%86%E7%B1%BB/</p>]]></content>
    
    
    <categories>
      
      <category>study</category>
      
    </categories>
    
    
    <tags>
      
      <tag>深度学习</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>深度学习笔记-1</title>
    <link href="/2020/02/22/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-1/"/>
    <url>/2020/02/22/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-1/</url>
    
    <content type="html"><![CDATA[<p>绪论&amp;卷积神经网络基础</p><span id="more"></span><h2 id="绪论">绪论</h2><ul><li><p>什么是人工智能使一部机器像人一样进行感知、认知、决策、执行的人工程序或系统。</p></li><li><p>图灵测试假设在一个封闭空间中，完成一个任务，外部人员能否判断出完成这个任务的是机器还是人。（验证码系统）</p></li><li><p>逻辑演绎 VS 归纳总结</p><ul><li>逻辑演绎<ul><li>知识表达与推理</li><li>符号主义：使用符号、规则和逻辑来表征知识和进行逻辑推理（自上而下）</li><li>定理证明机、专家系统</li></ul></li><li>归纳总结<ul><li>模型、策略、算法</li><li>贝叶斯：对事件发生的可能性进行概率推理（自上而下、自下而上）</li><li>联结主义：模拟脑结构，使用概率矩阵来识别和归纳模式（自下而上）</li><li>朴素贝叶斯隐马尔科夫、神经网络</li></ul></li></ul></li><li><p>知识工程/专家系统 vs 机械学习</p><ul><li>知识工程<ul><li>根据专家定义的知识和经验，进行推理和判断，从而模拟人类专家的决策过程来解决问题。</li><li>基于<strong>手工设计</strong>规则建立专家系统</li><li>结果容易解释</li><li>系统构建费时费力</li><li>依赖专家主观经验，难以保证一致性和准确性</li></ul></li><li>机器学习<ul><li>建立决策树，对输入的测试内容进行标注，让其自动学习；当输入一个当内容，自动进行分类</li><li>基于<strong>数据自动学习</strong></li><li>减少人工复杂工作，但结果可能不易解释</li><li>提高信息处理的效率，且准确率较高</li><li>来源于真实数据，减少人工规则主观性，可信度高</li></ul></li></ul></li><li><p>模型分类</p><ul><li>数据标记<ul><li>监督学习模型：输入样本具有标记，从数据中学习标记分界面，适用于预测数据标记<img src="/2020/02/22/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-1/1.jpg"></li><li>无监督学习模型：输入样本没有标记，从数据中学习模式，适用于描述数据<img src="/2020/02/22/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-1/2.jpg"></li><li>半监督学习：部分数据标记已知</li><li>强化学习：数据标记未知，但知道与输出目标相关的反馈</li></ul></li><li>数据分布<ul><li>参数模型：对数据分布进行假设，待求解的数据模式/映射可以用一组<strong>有限且固定数目的参数</strong>进行刻画</li><li>非参数模型：不对数据分布进行假设，数据的所有统计特征都来源于数据本身</li></ul></li><li>建模对象<ul><li>生成模型：对输入和输出的联合分布P(X,Y)建模；从数据中先学习到 X 和 Y的联合分布，然后通过贝叶斯公式求的 P(Y|X)</li><li>判别模型：对已知输入X条件下输出Y的条件分布 P(Y|X) 建模；直接学习P(Y|X)，输入 X ，直接预测 Y</li></ul></li></ul></li><li><p>深度学习的“不能”</p><ul><li>算法不稳定，容易被攻击；eg，一张图像加入噪声或像素点更改就会产生完全不同的输出结果</li><li>模型复杂度高，难以纠错和调试</li><li>模型层级复合程度高，参数不透明</li><li>端到端训练方式对数据依赖性强，模型增量性差；当样本数据量小的时候，深度学习无法体现强大拟合能力；深度学习可以进行语义标注和关系检测，但无法进一步完成图像描述（可能需要一个新的神经网络）</li><li>专注直观感知类问题，对开放性推理问题无能为力</li><li>人类知识无法有效引入进行监督，机器偏见难以避免不</li></ul></li></ul><h2 id="神经网络基础">神经网络基础</h2><h3 id="浅层神经网络">浅层神经网络</h3><ul><li>神经元<ul><li>多输入单输出</li><li>空间整合和时间整合</li><li><strong>兴奋性输入和抑制性输入</strong></li><li><strong>阈值特性</strong></li></ul></li><li><h2 id="m-p神经元">M-P神经元</h2></li></ul><p><img src="/2020/02/22/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-1/3.jpg"></p><ul><li>多输入信号进行累加x</li><li>权值w正负模拟兴奋/抑制，大小模拟强度</li><li>输入和超过阈值，神经元被激活</li><li>激活函数f<ul><li>输入超过阈值，神经元被激活，但不一定传递</li><li>没有激活函数相当于矩阵相乘；每一层相当于一个矩阵，矩阵相乘仍为矩阵，即多层和一层一样</li><li></li></ul></li></ul><p><img src="/2020/02/22/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-1/4.jpg"></p><ul><li>单层感知器<ul><li>通过非线性激活函数，可以实现简单的逻辑非、与、或操作（线性问题），但却不可以解决异或问题（非线性问题）</li><li>但可以通过多层感知器实现异或问题（数字电路），第一层的感知器可以理解为对一个非线性问题进行空间变化，将其转变为一个线性分类问题</li></ul></li><li>万有逼近定理<ul><li>如果一个隐层包含足够多的神经元，三层前反馈神经网络（输入-隐层-输出）能以任意精度逼近任意预定的<strong>连续函数</strong></li><li>双隐层感知器（输入-隐层1-隐层2-输出）逼近<strong>非连续函数</strong></li><li>神经网络每一层的作用</li><li>每一层公式 <span class="math inline">\(x^{l+1} = f(Wx^l +b)\)</span><ul><li>完成输入-&gt;输出空间变换<ul><li>升维/降维（W*x）</li><li>放大/缩小（W*x）</li><li>旋转（W*x）</li><li>平移（+b）</li><li>弯曲 a()</li></ul></li><li>利用矩阵的线性变换和激活函数的非线性变换，<strong>将原始输入空间投影到线性可分的空间</strong>去分类/回归</li><li>数据训练就是为了让神经网络选择这样一种线性和非线性变换</li><li>增加节点：增加维度，即增加线性转换能力</li><li>增加层数：增加激活函数的次数，即增加非线性转换次数</li></ul></li></ul></li><li><h2 id="误差反向传播"><strong>误差反向传播</strong></h2></li></ul><p><img src="/2020/02/22/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-1/5.jpg"></p><ul><li>梯度和梯度下降<ul><li>参数沿负梯度方向更新可以使函数值下降，但可能会陷入局部极值点，无法找到全局的极值点（与初始点位置有关）</li></ul></li><li></li></ul><p><img src="/2020/02/22/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-1/6.jpg"></p><ul><li><h2 id="梯度消失">梯度消失</h2></li></ul><p><img src="/2020/02/22/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-1/7.jpg"></p><ul><li><strong>当激活函数（sigmoid）落在饱和区的时候，其导数会很小，从而导致整个链式求导的反向传播的值很小，接近于零</strong>；从而在反向传播过程中，最后几层网络的训练效果很好，但前面的网络层因为梯度消失原因得不到很好的训练，从而在有新的训练输入后，经过前面的网络层，使得数据混乱，导致最后几层的训练失去效果</li><li>增加深度会造成梯度消失，误差无法传播</li><li>多层网络容易陷入局部极值，难以训练</li><li>逐层预训练<ul><li>首先训练一个三层网络，然后在该网络上逐层往上加，达到逐层预训练的目的</li><li>但因为是前面的网络层，所以没有很好的监督信息，解决方法：受限玻尔兹曼机和自编码器</li><li><h2 id="自编码器">自编码器</h2></li></ul></li></ul><p><img src="/2020/02/22/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-1/8.jpg"></p><pre><code class="hljs">- 对每一个隐层，使用自编码器进行训练，然后再把所有隐层结合起来，然后微调</code></pre><ul><li><h2 id="受限玻尔兹曼机rbm">受限玻尔兹曼机（RBM）</h2></li></ul><p><img src="/2020/02/22/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-1/9.jpg"></p>]]></content>
    
    
    <categories>
      
      <category>study</category>
      
    </categories>
    
    
    <tags>
      
      <tag>深度学习</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Hadoop+HBase安装配置</title>
    <link href="/2019/12/02/Hadoop-HBase%E5%AE%89%E8%A3%85%E9%85%8D%E7%BD%AE/"/>
    <url>/2019/12/02/Hadoop-HBase%E5%AE%89%E8%A3%85%E9%85%8D%E7%BD%AE/</url>
    
    <content type="html"><![CDATA[<p>最近大数据学习，做实验接触到了hadoop的使用，不过在这个过程中，回想起来真的是一部辛酸史，，我都不知道为了实验踩了多少坑，这里把自己的配置过程和一些该注意的地方整理总结一下，也方便以后看看~</p><span id="more"></span><h2 id="实验环境">实验环境</h2><ul><li>操作系统：CentOS7（Master * 1，Slave * 2）</li><li>Hadoop版本：2.7.7</li><li>JDK版本：1.8.0_211</li><li>HBase版本：2.0.0-alpha4</li></ul><blockquote><p>emm在整个实验中最好都要使用root用户，不然过程中还是会遇到一些麻烦的地方，，，（辛酸史😭）</p></blockquote><h2 id="设置静态ip及相关网络设置">设置静态ip及相关网络设置</h2><h3 id="设置静态ip">设置静态ip</h3><p>首先通过ifconfig和netstat-rn命令查看当前网络状态（其中eno16777736为本机的网卡名字，可以查看到当前的网络IP地址inet、子网掩码netmask和网关gateway）</p><p><img src="/2019/12/02/Hadoop-HBase%E5%AE%89%E8%A3%85%E9%85%8D%E7%BD%AE/1.jpg"></p><p>修改网络IP地址配置文件</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">vi /etc/sysconfig/network-scripts/ifcfg-eno16777736<br></code></pre></td></tr></table></figure><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><code class="hljs text">HWADDR=00:0C:29:B9:DC:E8<br>TYPE=Ethernet<br>BOOTPROTO=static# 使用静态IP地址，默认为dhcp<br>IPADDR=192.168.72.132# 设置的静态IP地址<br>NETMASK=255.255.255.0# 子网掩码<br>GATEWAY=192.168.72.2# 网关地址<br>DNS1=8.8.8.8# DNS服务器<br>DNS2=114.114.114.114<br>DEFROUTE=yes<br>PEERDNS=yes<br>PEERROUTES=yes<br>IPV4_FAILURE_FATAL=no<br>IPV6INIT=yes<br>IPV6_AUTOCONF=yes<br>IPV6_DEFROUTE=yes<br>IPV6_PEERDNS=yes<br>IPV6_PEERROUTES=yes<br>IPV6_FAILURE_FATAL=no<br>NAME=eno16777736<br>UUID=b66961b7-567e-408b-9d83-5b2046f8980b<br>ONBOOT=no# 是否开机启用<br></code></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs shell">service network restart# 重启网络服务<br>systemctl status firewalld.service# 查看防火墙状态，绿色字样标注的“active（running”，说明防火墙是开启状态；disavtive（dead）的字样，说明防火墙已经关闭<br>systemctl stop firewalld.service# 关闭运行的防火墙<br>systemctl disable firewalld.service # 禁止防火墙开机启动<br></code></pre></td></tr></table></figure><p>对实验使用的三台虚拟机分别进行上述操作，设置为静态ip。</p><h3 id="配置hosts文件">配置hosts文件</h3><p>在hosts文件中，为每台机器设置别名（三台机器都需要）。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">vi /etc/hosts<br></code></pre></td></tr></table></figure><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs text">127.0.0.1   localhost localhost.localdomain localhost4 localhost4.localdomain4<br>::1         localhost localhost.localdomain localhost6 localhost6.localdomain6<br><br>192.168.72.132 master<br>192.168.72.130 slave1<br>192.168.72.131 slave2<br></code></pre></td></tr></table></figure><h3 id="配置ssh免密登录">配置SSH免密登录</h3><p>三台机器都需要，成功后，可以通过sshroot@slave1命令直接登陆另一台机器。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs shell">ssh-keygen# 一路回车即可<br>ssh-copy-id -i .ssh/id_rsa.pub root@master# 设置本机的免密登录<br>ssh-copy-id -i .ssh/id_rsa.pub root@slave1<br>ssh-copy-id -i .ssh/id_rsa.pub root@slave2<br></code></pre></td></tr></table></figure><h2 id="jdk安装">JDK安装</h2><p>在官网下载相应的JDK版本（注意要下载64位，hadoop-3.x版本要使用jdk1.8，相应的版本对应情况可以在网上直接查到），在虚拟机中解压即可（三台机器都需要配置）。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs shell">yum list  installed  | grep java        # 查看系统内自带的Java-jdk程序<br>yum  remove  -y  程序名称# 将自带的java卸载掉<br><br>tar -zxvf jdk-8u221-linux-x64.tar.gz<br>mv ./jdk1.8.0_221 /opt/jdk/jdk1.8.0_231<br>rm -rf jdk-8u221-linux-x64.tar.gz<br></code></pre></td></tr></table></figure><p>添加环境变量</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">vi ~/.bashrc# 我用的时候，/etc/profile有时候会不起作用，所以为了省事都直接写到.bashrc中了<br></code></pre></td></tr></table></figure><p>添加以下内容</p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs text">export JAVA_HOME=/opt/jdk/jdk1.8.0_231<br>export CLASSPATH=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar<br>export PATH=$PATH:$JAVA_HOME/bin<br></code></pre></td></tr></table></figure><p>即使生效</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs shell">source ~/.bashrc<br><br>java -version# 查看是否配置成功<br></code></pre></td></tr></table></figure><h2 id="haoop安装">Haoop安装</h2><h3 id="环境配置">环境配置</h3><p>下载相应的hadoop版本，解压到/opt/目录</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs shell">cd /opt<br>sudo tar -zxvf hadoop-2.7.7.tar.gz<br>sudo mv /opt/hadoop-2.7.7 /opt/hadoop<br></code></pre></td></tr></table></figure><p>添加环境变量（在~/.bashrc中，如上）</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs shell">export HADOOP_HOME=/opt/hadoop<br>export PATH=$PATH:$HADOOP_HOME/sbin:$HADOOP_HOME/bin<br></code></pre></td></tr></table></figure><p>然后source ~/.bashrc 命令生效</p><p>修改 hadoop-env.sh 和yarn-env.sh文件（相关配置文件都在$HADOOP_HOME/etc/hadoop/目录下）</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs shell">cd /opt/hadoop/etc/hadoop/<br>vi hadoop-env.sh<br>vi yarn-env.sh<br></code></pre></td></tr></table></figure><p>添加JAVA_HOME</p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs text">export JAVA_HOME=/opt/jdk/jdk1.8.0_221<br></code></pre></td></tr></table></figure><h3 id="文件配置">文件配置</h3><p>修改同在该目录下的slave文件，添加两个slave结点</p><p><img src="/2019/12/02/Hadoop-HBase%E5%AE%89%E8%A3%85%E9%85%8D%E7%BD%AE/2.jpg"></p><p>修改core-site.xml文件</p><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs html"><span class="hljs-tag">&lt;<span class="hljs-name">configuration</span>&gt;</span><br>    <span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>fs.defaultFS<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>hdfs://Master:9000<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span><br>    <span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span><br>    <span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>hadoop.tmp.dir<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>/home/hadoop/tmp<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span><br>    <span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span><br><span class="hljs-tag">&lt;/<span class="hljs-name">configuration</span>&gt;</span><br></code></pre></td></tr></table></figure><p>修改hdfs-site.xml 文件，因为副本数量不能超过结点数，设置副本为2，DataNode的心跳间隔为2S，接口地址为http://master:50070</p><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs html"><span class="hljs-tag">&lt;<span class="hljs-name">configuration</span>&gt;</span><br>    <span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>dfs.replication<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>2<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span><br>    <span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span><br>    <span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>dfs.http.address<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>Master:50070<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span><br>    <span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span><br>    <span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>dfs.heartbeat.interval<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>2<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span><br><span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span><br><span class="hljs-tag">&lt;/<span class="hljs-name">configuration</span>&gt;</span><br></code></pre></td></tr></table></figure><p>修改mapred-site.xml文件</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">cp mapred-site.xml.template mapred-site.xml<br></code></pre></td></tr></table></figure><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs html"><span class="hljs-tag">&lt;<span class="hljs-name">configuration</span>&gt;</span><br><span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span><br><span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>mapreduce.framework.name<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span><br><span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>yarn<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span><br><span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span><br><span class="hljs-tag">&lt;/<span class="hljs-name">configuration</span>&gt;</span><br></code></pre></td></tr></table></figure><p>修改yarn-site.xml文件</p><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs html"><span class="hljs-tag">&lt;<span class="hljs-name">configuration</span>&gt;</span><br>    <span class="hljs-comment">&lt;!-- Site specific YARN configuration properties --&gt;</span><br>    <span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>yarn.nodemanager.aux-services<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>mapreduce_shuffle<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span><br>    <span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span><br><span class="hljs-tag">&lt;/<span class="hljs-name">configuration</span>&gt;</span><br></code></pre></td></tr></table></figure><p>将配置完成的hadoop发送给slave节点，并给slave节点配置HADOOP_HOME环境变量（如上）</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs shell">scp -r /opt/hadoop root@slave1:/opt/<br>scp -r /opt/hadoop root@slave2:/opt/<br></code></pre></td></tr></table></figure><h3 id="启动使用">启动使用</h3><p>在namenode（master）上格式化文件系统</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs shell">hdfs namenode -format<br>start-all.sh# 若格式化正常，没有error报错，便可启动hadoop系统<br>stop-all.sh# 关闭Hadoop<br></code></pre></td></tr></table></figure><p><img src="/2019/12/02/Hadoop-HBase%E5%AE%89%E8%A3%85%E9%85%8D%E7%BD%AE/3.jpg"></p><p>ps：在这一步的时候，如果要反复输入密码，则表示ssh免密登录配置有问题（包括本机的ssh免密登陆）</p><p>正常启动之后，可以查看webUI</p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs text">http://master:50070<br>http://master:8088<br></code></pre></td></tr></table></figure><p>也可以通过jps命令查看各个节点的相关进程</p><p><img src="/2019/12/02/Hadoop-HBase%E5%AE%89%E8%A3%85%E9%85%8D%E7%BD%AE/4.jpg"></p><blockquote><p>NameNode：HDFS的命名空间，里面存储着整个HDFS的所有文件的元数据信息，这些信息都会加载到内存中，元数据信息分为两部分，第一部分是文件系统树及整棵树内所有的文件和目录，第二部分是每个文件的各个组成块所在的数据节点信息。</p><p>DataNode：文件系统的工作节点，它会根据需要存储并检索数据块，受NameNode调度，并且定期向NameNode发送该DataNode上存储的块的列表信息。</p><p>SecondaryNode：辅助NameNode，它是用来定期合并NameNode产生的编辑日志（edits.log）和命名空间镜像文件(fsImage)，以防止edits.log过大。</p><p>ResourceManager：负责集群中所有资源的统一管理和分配，它接收来自各个节点（NodeManager）的资源汇报信息，并把这些信息按照一定的策略分配给各个应用程序。</p><p>NodeManager：每个节点上的代理，它管理hadoop集群中单个计算节点，包括与ResourceManger保持通信，监督container的生命周期管理，监控每个Container的资源使用（内存、CPU等）情况，追踪节点健康状况，管理日志和不同应用程序用到的附属服务。</p></blockquote><p>如果出现start-all.sh之后，在slave上没有出现datanode的话，可能是因为多次hdfsnamenode -format对文件系统格式化，在每次格式化的时候，都会生成新的clusterID ,而datanode还是保持原来的clusterID，所以无法正常启动datanode。</p><p>解决方法：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">cat /home/hadoop/tmp/dfs/data/current/VERSION# 可以看到namenode的clusterID<br></code></pre></td></tr></table></figure><p>然后用这个clusterID，替换掉所有slave结点上的该clusterID，然后重新启动即可。</p><p>遇到各种问题，学会多查看log日志文件，真的能学到很多~</p><h3 id="java-api接口">java api接口</h3><p>网上有很多api接口使用的教程，我就不再写了，这一部分我在试验过程中没有遇到太大的问题。这里稍微说一点就是使用hadoop提供的javaapi接口的时候，需要导入的jar包如下（即common下3个，hdfs下3个和conmmon/lib下所有）:</p><p><img src="/2019/12/02/Hadoop-HBase%E5%AE%89%E8%A3%85%E9%85%8D%E7%BD%AE/5.jpg"></p><h2 id="hbase安装">HBase安装</h2><h3 id="环境配置-1">环境配置</h3><p>下载相应的hbase版本，因为仅是实验使用，所以没有在单独下载zookeeper，使用的hbase中自带的zookeeper，解压到/opt/目录</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs shell">sudo tar -zxvf hbase-2.0.0-alpha4-bin.tar.gz<br>sudo mv /opt/hbase-2.0.0-alpha4 /opt/hbase<br></code></pre></td></tr></table></figure><p>添加环境变量（在~/.bashrc中，如上）</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs shell">export HBASE_HOME=/opt/hbase<br>export PATH=$HBASE_HOME/bin:$PATH<br></code></pre></td></tr></table></figure><p>然后source ~/.bashrc 命令生效</p><h3 id="文件配置-1">文件配置</h3><p>修改hbase-env.sh文件（仅修改下面两项即可）</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">vi /opt/hbase/conf/hbase-env.sh<br></code></pre></td></tr></table></figure><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs text">export JAVA_HOME=/opt/jdk/jdk1.8.0_221# jdk安装路径<br>export HBASE_MANAGES_ZK=true# 使用hbase自带的zookeeper,如果是独立的zookeeper的话，这里为false<br></code></pre></td></tr></table></figure><p>修改hbase-site.xml文件</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">vi /opt/hbase/conf/hbase-site.xml<br></code></pre></td></tr></table></figure><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><code class="hljs html"><span class="hljs-tag">&lt;<span class="hljs-name">configuration</span>&gt;</span><br>    <span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>hbase.master<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>master:60000<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span><br>    <span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span><br>    <span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>hbase.rootdir<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>hdfs://master:9000/hbase<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span><br>    <span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span><br>    <span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>hbase.zookeeper.quorum<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>master,slave1,slave2<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span><br>    <span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span><br>    <span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>hbase.cluster.distributed<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>true<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span><br>    <span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span><br>    <span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>hbase.zookeeper.property.clientPort<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>2181<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span><br>    <span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span><br>    <span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>hbase.zookeeper.property.dataDir<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>/home/hadoop/zookeeper<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span><br>    <span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span><br><span class="hljs-tag">&lt;/<span class="hljs-name">configuration</span>&gt;</span><br></code></pre></td></tr></table></figure><p>修改regionservers文件</p><p><img src="/2019/12/02/Hadoop-HBase%E5%AE%89%E8%A3%85%E9%85%8D%E7%BD%AE/6.jpg"></p><p>在完成上面的所有修改之后，将hbase文件传输到各个slave节点上。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs shell">scp -r /opt/hbase root@slave1:/opt/<br>scp -r /opt/hbase root@slave2:/opt/<br></code></pre></td></tr></table></figure><p>同步服务器时间</p><p>在使用hbase的时候，一定要保证各台机器的时间是同步的，不然hbase会出现很多错误。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs shell">yum -y install ntp ntpdate# 安装ntpdate工具<br>timedatectl# 查看系统时间<br>ntpdate cn.pool.ntp.org# 与网络时间同步，同步后可以在查看确认一下<br></code></pre></td></tr></table></figure><p><img src="/2019/12/02/Hadoop-HBase%E5%AE%89%E8%A3%85%E9%85%8D%E7%BD%AE/10.jpg"></p><h3 id="启动使用-1">启动使用</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">start-hbase.sh# 启动habse（需要先启动hadoop）<br></code></pre></td></tr></table></figure><p><img src="/2019/12/02/Hadoop-HBase%E5%AE%89%E8%A3%85%E9%85%8D%E7%BD%AE/7.jpg"></p><p>正常启动之后，可以查看webUI</p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs text">http://master:16010<br></code></pre></td></tr></table></figure><p>ps：我当时做到这一步的时候，遇到了一个问题，是webUI可以正常访问，但是网页上RegionSever下面是空的，没有正常显示slave结点，日志报错部分如下：</p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs text">Call to localhost/127.0.0.1:16000 failed on connection exception<br></code></pre></td></tr></table></figure><p>我的问题是因为三台机器的主机名始终为localhost，虽然已经修改host文件，但是并没有生效，在修改完主机名之后，问题消失。</p><p>也可以通过jps命令查看各个节点的相关进程</p><p><img src="/2019/12/02/Hadoop-HBase%E5%AE%89%E8%A3%85%E9%85%8D%E7%BD%AE/8.jpg"></p><h3 id="java-api接口-1">java api接口</h3><p>使用hadoop提供的javaapi接口的时候，需要导入/opt/hbase/lib路径下的所有jar包。</p><h3 id="hbase使用">hbase使用</h3><p>除了可以使用java api接口外，也可以使用hbase提供的shell命令</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">hbase shell# 进入hbase shell<br></code></pre></td></tr></table></figure><p><img src="/2019/12/02/Hadoop-HBase%E5%AE%89%E8%A3%85%E9%85%8D%E7%BD%AE/9.jpg"></p><h2 id="总结">总结</h2><p>虽然在实验过程中，遇到了很多麻烦，但现在回想起来很多都是因为不理解导致的，如果能够明白每一项配置的作用和参数含义的话，其实整个实验都会顺利很多，但也正是在这个过程中不断学习进步的。</p><p>继续加油啊~</p>]]></content>
    
    
    <categories>
      
      <category>study</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Hadoop&amp;HBase</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>区域填充</title>
    <link href="/2019/11/18/%E5%8C%BA%E5%9F%9F%E5%A1%AB%E5%85%85/"/>
    <url>/2019/11/18/%E5%8C%BA%E5%9F%9F%E5%A1%AB%E5%85%85/</url>
    
    <content type="html"><![CDATA[<p>最近在做一个图形学的实验，关于区域填充（逐点法填充、有序边表填充、图案填充以及种子填充），还有补充的多边形三角剖分。在做这几个实验的时候，学习到了很多知识，也掉了很多头发，在这里集中整理一下，也方便以后回顾学习。</p><span id="more"></span><h2 id="橡皮筋效果">橡皮筋效果</h2><p>本次实验的环境是使用vs提供的MFC框架，在鼠标交互输入中使用橡皮筋算法来实现多边形的输入。首先定义相应变量来保存输入的数据。</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-type">int</span> m_pNumbers;                               <span class="hljs-comment">//输入点的个数</span><br>CPoint m_pAccord[N], m_mousePoint;<span class="hljs-comment">//保存输入的顶点信息和当前的鼠标所处位置</span><br></code></pre></td></tr></table></figure><ul><li>每次进行鼠标点击的时候，便调用OnLButtonDown(UINT nFlags, CPointpoint)函数，若输入的顶点个数不超过规定最大值，便保存当前输入点的信息。</li><li>在输入第一个点之后，移动鼠标，便会有一条线从上一个输入点A连接到鼠标当前位置，由OnMouseMove(UINTnFlags, CPointpoint)函数实现该功能。每次鼠标移动时t1时刻，再一次绘制上一个点A到t1时刻鼠标位置的线段，将该直线隐藏。然后更新m_mousePoint的值，然后绘制当前t2时刻，上一个输入点A到当前t2时刻鼠标位置的线段。</li><li>在用户输入完成之后，双击完成输入。 <strong>在这次也是发现了MFC的一个隐藏的知识点，就是在用户双击的时候，MFC会先触发单击事件函数，然后调用双击事件函数，</strong>所以这样我们就不用在双击时间函数中去保存双击的输入点信息，只需要将最后的输入点和起点相连即可。</li><li>以上操作便完成了橡皮筋效果。在完成多边形的绘制之后，便根据用户的选择来进行相应操作。</li></ul><p>代码实现如下~</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-function"><span class="hljs-type">void</span> <span class="hljs-title">CcgPDHFillPolyView::OnMouseMove</span><span class="hljs-params">(UINT nFlags, CPoint point)</span></span><br><span class="hljs-function"></span>&#123;<br><span class="hljs-comment">// <span class="hljs-doctag">TODO:</span> 在此添加消息处理程序代码和/或调用默认值</span><br><span class="hljs-keyword">if</span> (m_pNumbers) &#123;<br><br>m_pDC-&gt;<span class="hljs-built_in">SetROP2</span>(<span class="hljs-number">2</span>);<br>m_pDC-&gt;<span class="hljs-built_in">MoveTo</span>(m_pAccord[m_pNumbers - <span class="hljs-number">1</span>]);<br>m_pDC-&gt;<span class="hljs-built_in">LineTo</span>(m_mousePoint);<br><br>m_mousePoint = point;<br>m_pDC-&gt;<span class="hljs-built_in">MoveTo</span>(m_pAccord[m_pNumbers - <span class="hljs-number">1</span>]);<br>m_pDC-&gt;<span class="hljs-built_in">LineTo</span>(m_mousePoint);<br>&#125;<br><br>CView::<span class="hljs-built_in">OnMouseMove</span>(nFlags, point);<br>&#125;<br><br><br><span class="hljs-function"><span class="hljs-type">void</span> <span class="hljs-title">CcgPDHFillPolyView::OnLButtonDblClk</span><span class="hljs-params">(UINT nFlags, CPoint point)</span></span><br><span class="hljs-function"></span>&#123;<br><span class="hljs-comment">// <span class="hljs-doctag">TODO:</span> 在此添加消息处理程序代码和/或调用默认值</span><br>m_pDC-&gt;<span class="hljs-built_in">MoveTo</span>(m_pAccord[m_pNumbers - <span class="hljs-number">1</span>]);<br>m_pDC-&gt;<span class="hljs-built_in">LineTo</span>(m_pAccord[<span class="hljs-number">0</span>]);<br><br>m_pAccord[m_pNumbers] = m_pAccord[<span class="hljs-number">0</span>];<br>m_pNumbers++;<br><br>CcgPDHFillPolyDoc *pDoc = <span class="hljs-built_in">GetDocument</span>();<br><span class="hljs-keyword">if</span>( pDoc-&gt;m_drawMode == <span class="hljs-number">0</span> || pDoc-&gt;m_drawMode == <span class="hljs-number">1</span>)<span class="hljs-comment">//有序边表法、图案填充算法</span><br><span class="hljs-built_in">Fillpolygon</span>(m_pNumbers, m_pAccord, m_pDC);<br><span class="hljs-keyword">else</span> <span class="hljs-keyword">if</span>(pDoc-&gt;m_drawMode == <span class="hljs-number">2</span>)&#123;<br><span class="hljs-built_in">PointFillpoly</span>(m_pNumbers, m_pAccord, m_pDC);<span class="hljs-comment">// 逐点法</span><br>&#125;<br><span class="hljs-keyword">else</span> <span class="hljs-keyword">if</span> (pDoc-&gt;m_drawMode == <span class="hljs-number">3</span>) &#123;<br>seedFlag = <span class="hljs-literal">true</span>;<br>&#125;<br><span class="hljs-keyword">else</span> &#123;<br><br><span class="hljs-built_in">Triangulation</span>(m_pAccord, m_pNumbers<span class="hljs-number">-1</span>, m_pNumbers - <span class="hljs-number">1</span>);<span class="hljs-comment">//多边形顶点集合，当前多边形顶点数，多边形顶点数</span><br>&#125;<br><br>m_pNumbers = <span class="hljs-number">0</span>;<br><br>CView::<span class="hljs-built_in">OnLButtonDblClk</span>(nFlags, point);<br>&#125;<br><br><br><span class="hljs-function"><span class="hljs-type">void</span> <span class="hljs-title">CcgPDHFillPolyView::OnLButtonDown</span><span class="hljs-params">(UINT nFlags, CPoint point)</span></span><br><span class="hljs-function"></span>&#123;<br><span class="hljs-comment">// <span class="hljs-doctag">TODO:</span> 在此添加消息处理程序代码和/或调用默认值</span><br><span class="hljs-keyword">if</span> (seedFlag) &#123;<br>seedPoint = point;<br><span class="hljs-built_in">ScanLineFill4</span>(seedPoint.x, seedPoint.y, <span class="hljs-number">0</span>, m_pDC);<br>seedFlag = <span class="hljs-literal">false</span>;<br><br>&#125;<br><span class="hljs-keyword">else</span> <span class="hljs-keyword">if</span> (m_pNumbers &lt; N) &#123;<br>m_pAccord[m_pNumbers] = point;<br>m_pNumbers++;<br><br>m_mousePoint = point;<br>&#125;<br><br>CView::<span class="hljs-built_in">OnLButtonDown</span>(nFlags, point);<br>&#125;<br></code></pre></td></tr></table></figure><h2 id="有序边表法填充">有序边表法填充</h2><p>首先给出有序边表法中定义使用的变量。</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-type">int</span> m_Begin, m_End, m_edgeNumbers, m_Scan;       <span class="hljs-comment">//求交边集指针，有效边数，当前扫描位置</span><br><span class="hljs-type">float</span> m_yMax[N], m_yMin[N], m_Xa[N], m_Dx[N];<span class="hljs-comment">//每条边y最大值，y最小值，每条边与扫描线x的交点，每条边斜率倒数</span><br></code></pre></td></tr></table></figure><p>基本思想：用水平扫描线从上到下（或从下到上）扫描由多条首尾相连的线段构成的多边形，每根扫描线与多边形的某些边产生一系列交点。将这些交点按照x坐标排序，将排序后的点两两成对，作为线段的两个端点，以所填的颜色画水平直线。</p><figure><img src="/2019/11/18/%E5%8C%BA%E5%9F%9F%E5%A1%AB%E5%85%85/12.jpg" alt="img"><figcaption aria-hidden="true">img</figcaption></figure><ul><li>在实现中首先需要对多边形的每一条边进行处理操作，因为是采用水平线扫描填充，所以对于水平线不做处理。对其余边根据端点的y值大小进行排序，保存yMax，yMin，Xa(相对应的x坐标)以及Dx(斜率的倒数)。</li><li>在进行扫描的过程中，很重要的一部分操作便是<strong>求交边集指针的移动</strong>。初始状态位0，在扫描线开始向下移动后，调用Include()函数，检查是否有边进入扫描线交集(即判断所有y最大值大于扫描线当前y值的边线)，此时将m_End++，即尾指针向后移动。在Include()函数中，也会调整起始点位置，将Dx调整为位移量。</li><li>之后调用UpdateXvalue()函数，判断是否有边退出求交边集。<ul><li>如果没有边退出，则移动x，并根据x值大小进行排序。</li><li>有边退出，更新数组，删除该边，m_Begin++，即头指针向后移动。</li></ul></li><li>调用pFillScan(pDC)函数，进行填充。</li><li>m_Scan--，回到第二步进行循环，直到m_Begin==m_End。</li></ul><p>实现代码如下~</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-function"><span class="hljs-type">void</span> <span class="hljs-title">CcgPDHFillPolyView::Fillpolygon</span><span class="hljs-params">(<span class="hljs-type">int</span> pNumbers, CPoint *points, CDC *pDC)</span></span><br><span class="hljs-function"></span>&#123;<br><br>m_edgeNumbers = <span class="hljs-number">0</span>;<br><span class="hljs-built_in">pLoadPolygon</span>(pNumbers, points);   <span class="hljs-comment">// Polygon Loading, calculates every edge&#x27;s m_yMax[],m_yMin[],m_Xa[],m_Dx[]</span><br><br><span class="hljs-comment">//求交边集范围，因为数组已经根据y值大小进行边的排序，所以end向后移动即代表有边进入，start向后移动，代表有边退出</span><br>m_Begin = m_End = <span class="hljs-number">0</span>;          <br>m_Scan = (<span class="hljs-type">int</span>)m_yMax[<span class="hljs-number">0</span>];          <span class="hljs-comment">//从顶向下扫描</span><br><span class="hljs-built_in">Include</span>();                        <span class="hljs-comment">//检查是否有边进入扫描线</span><br><span class="hljs-built_in">UpdateXvalue</span>();                   <span class="hljs-comment">//检查是否有边退出扫描线</span><br><span class="hljs-keyword">while</span> (m_Begin != m_End) &#123;<br><span class="hljs-built_in">pFillScan</span>(pDC);<br>m_Scan--;<br><span class="hljs-built_in">Include</span>();<br><span class="hljs-built_in">UpdateXvalue</span>();<br>&#125;<br><br>&#125;<br><br><span class="hljs-function"><span class="hljs-type">void</span> <span class="hljs-title">CcgPDHFillPolyView::pLoadPolygon</span><span class="hljs-params">(<span class="hljs-type">int</span> pNumbers, CPoint *points)</span></span><br><span class="hljs-function"></span>&#123;<br><span class="hljs-type">float</span> x1,y1,x2,y2;<br><br>x1 = points[<span class="hljs-number">0</span>].x;    y1 = points[<span class="hljs-number">0</span>].y+<span class="hljs-number">0.5</span>;<br><span class="hljs-keyword">for</span> (<span class="hljs-type">int</span> i = <span class="hljs-number">1</span>; i &lt; pNumbers; i++) &#123;<br>x2 = points[i].x;    y2 = points[i].y+<span class="hljs-number">0.5</span>;<br><span class="hljs-keyword">if</span> (<span class="hljs-built_in">abs</span>(<span class="hljs-built_in">int</span>(y2 - y1)) &gt;= <span class="hljs-number">5</span>)<span class="hljs-comment">//水平线不做处理</span><br>&#123;<br><span class="hljs-built_in">pInsertLine</span>(x1, y1, x2, y2);<br>x1 = x2;      y1 = y2;<br>&#125;<br><span class="hljs-keyword">else</span><br>x2 = x1;<br>&#125;<br>&#125;<br><br><span class="hljs-function"><span class="hljs-type">void</span> <span class="hljs-title">CcgPDHFillPolyView::pInsertLine</span><span class="hljs-params">(<span class="hljs-type">float</span> x1, <span class="hljs-type">float</span> y1, <span class="hljs-type">float</span> x2, <span class="hljs-type">float</span> y2)</span></span><br><span class="hljs-function"></span>&#123;<br><span class="hljs-type">int</span> i;<br><span class="hljs-type">float</span> Ymax,Ymin;<br><br>Ymax = (y2 &gt; y1) ? y2 : y1;<br>Ymin = (y2 &lt; y1) ? y2 : y1;<br>i = m_edgeNumbers;<br><span class="hljs-comment">//根据y值的大小，进行排序插入，大的在前面</span><br><span class="hljs-keyword">while</span> ( i!=<span class="hljs-number">0</span> &amp;&amp; m_yMax[i<span class="hljs-number">-1</span>]&lt;Ymax ) &#123;<br>m_yMax[i] = m_yMax[i<span class="hljs-number">-1</span>];<br>m_yMin[i] = m_yMin[i<span class="hljs-number">-1</span>];<br>m_Xa[i] = m_Xa[i<span class="hljs-number">-1</span>];<br>m_Dx[i] = m_Dx[i<span class="hljs-number">-1</span>];<br>i--;<br>&#125;<br>m_yMax[i] = Ymax;<br>m_yMin[i] = Ymin;<br><span class="hljs-keyword">if</span> (y2 &gt; y1) m_Xa[i] = x2;<span class="hljs-comment">//根据y大小确定Xa的值，y大的会先于扫描线相交</span><br><span class="hljs-keyword">else</span>         m_Xa[i] = x1;<br><br>m_Dx[i] = (x2-x1)/(y2-y1);<span class="hljs-comment">//斜率的倒数</span><br>m_edgeNumbers++;<br>&#125;<br><br><span class="hljs-function"><span class="hljs-type">void</span> <span class="hljs-title">CcgPDHFillPolyView::Include</span><span class="hljs-params">()</span></span><br><span class="hljs-function"></span>&#123;<br><span class="hljs-comment">//end向后移动，找出所有边最高点y值大于当前扫描线的边，看是否有新的边进入交集</span><br><span class="hljs-keyword">while</span> ( m_End&lt;m_edgeNumbers &amp;&amp; m_yMax[m_End]&gt;=m_Scan ) &#123;<br><span class="hljs-comment">//有边进入，调整起始点位置，然后将Dx调整为位移量</span><br>m_Xa[m_End] = m_Xa[m_End] + (<span class="hljs-number">-0.5</span>)*m_Dx[m_End];<br>m_Dx[m_End] = -m_Dx[m_End];<br>m_End++;<br>&#125;<br>&#125;<br><br><span class="hljs-function"><span class="hljs-type">void</span> <span class="hljs-title">CcgPDHFillPolyView::UpdateXvalue</span><span class="hljs-params">()</span></span><br><span class="hljs-function"></span>&#123;<br><span class="hljs-type">int</span> i,start = m_Begin;<br><br><span class="hljs-keyword">for</span> (i = start; i &lt; m_End; i++) &#123;<br><span class="hljs-keyword">if</span> ( m_Scan &gt; m_yMin[i]) &#123;<br><span class="hljs-comment">//当前边没有退出，则移动x，然后在进行排序</span><br>m_Xa[i] += m_Dx[i];<br><span class="hljs-built_in">pXsort</span>(m_Begin, i);<br>&#125; <span class="hljs-keyword">else</span> &#123;<br><span class="hljs-comment">//有边退出，更新数组，然后begin++</span><br><span class="hljs-keyword">for</span> (<span class="hljs-type">int</span> j = i; j &gt; m_Begin; j--) &#123;<br>m_yMin[j] = m_yMin[j<span class="hljs-number">-1</span>];<br>m_Xa[j] = m_Xa[j<span class="hljs-number">-1</span>];<br>m_Dx[j] = m_Dx[j<span class="hljs-number">-1</span>];<br>&#125;<br>m_Begin++;<br>&#125;<br>&#125;<br><br>&#125;<br><br><span class="hljs-function"><span class="hljs-type">void</span> <span class="hljs-title">CcgPDHFillPolyView::pXsort</span><span class="hljs-params">(<span class="hljs-type">int</span> Begin, <span class="hljs-type">int</span> i)</span></span><br><span class="hljs-function"></span>&#123;<br><span class="hljs-type">float</span> temp;<br><br><span class="hljs-keyword">while</span> (i &gt; Begin &amp;&amp; m_Xa[i] &lt; m_Xa[i<span class="hljs-number">-1</span>]) &#123;<br>temp = m_Xa[i];   m_Xa[i]   = m_Xa[i<span class="hljs-number">-1</span>];   m_Xa[i<span class="hljs-number">-1</span>] = temp;<br>temp = m_Dx[i];   m_Dx[i]   = m_Dx[i<span class="hljs-number">-1</span>];   m_Dx[i<span class="hljs-number">-1</span>] = temp;<br>temp = m_yMin[i]; m_yMin[i] = m_yMin[i<span class="hljs-number">-1</span>]; m_yMin[i<span class="hljs-number">-1</span>] = temp;<br>i--;<br>&#125;<br>&#125;<br><br><span class="hljs-function"><span class="hljs-type">void</span> <span class="hljs-title">CcgPDHFillPolyView::pFillScan</span><span class="hljs-params">(CDC* pDC)</span></span><br><span class="hljs-function"></span>&#123;<br><span class="hljs-type">int</span> x,y;<br>CcgPDHFillPolyDoc* pDoc = <span class="hljs-built_in">GetDocument</span>();<br><br>pDC-&gt;<span class="hljs-built_in">SetROP2</span>(<span class="hljs-number">10</span>);<br><span class="hljs-keyword">for</span> (<span class="hljs-type">int</span> i = m_Begin; i &lt; m_End; i += <span class="hljs-number">2</span>) &#123;<span class="hljs-comment">//成对取交点</span><br><br><span class="hljs-keyword">if</span> (pDoc-&gt;m_drawMode == <span class="hljs-number">0</span>) &#123;<br>pDC-&gt;<span class="hljs-built_in">MoveTo</span>(m_Xa[i],   m_Scan);<br>pDC-&gt;<span class="hljs-built_in">LineTo</span>(m_Xa[i+<span class="hljs-number">1</span>], m_Scan);<br><br>&#125; <span class="hljs-keyword">else</span> &#123;<span class="hljs-comment">//图案填充</span><br>  y = m_Scan;<br>  <span class="hljs-keyword">for</span> (<span class="hljs-type">int</span> x = m_Xa[i]; x &lt; m_Xa[i+<span class="hljs-number">1</span>]; x++) <br><span class="hljs-keyword">if</span> (m_patternData[y%<span class="hljs-number">12</span>][x%<span class="hljs-number">12</span>])<br>pDC-&gt;<span class="hljs-built_in">SetPixel</span>(x, y, <span class="hljs-built_in">RGB</span>(<span class="hljs-number">255</span>,<span class="hljs-number">0</span>,<span class="hljs-number">0</span>));<br><br>&#125;<br><br>&#125;<br>&#125;<br></code></pre></td></tr></table></figure><p>不过这样的填充效果一般，不过写代码的时候，从中学到很多知识以及代码技巧。对这个算法也是深入了解T_T</p><figure><img src="/2019/11/18/%E5%8C%BA%E5%9F%9F%E5%A1%AB%E5%85%85/有序边表法-效果.jpg" alt="img"><figcaption aria-hidden="true">img</figcaption></figure><h2 id="图案填充">图案填充</h2><p>图案填充的准备内容和上面的一样，代码也在上面给出，我们只需要给出填充的图案，然后存放在二维数组m_patternData中即可，在本次实验中，我给了一个“A”的图案。我也是对模运算越来越觉得神奇，当初学的时候还没啥感觉~</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-type">int</span> m_patternData[<span class="hljs-number">12</span>][<span class="hljs-number">12</span>] = &#123;<br>    &#123; <span class="hljs-number">0</span>,<span class="hljs-number">0</span>,<span class="hljs-number">0</span>,<span class="hljs-number">0</span>,<span class="hljs-number">0</span>,<span class="hljs-number">0</span>,<span class="hljs-number">0</span>,<span class="hljs-number">0</span>,<span class="hljs-number">0</span>,<span class="hljs-number">0</span>,<span class="hljs-number">0</span>,<span class="hljs-number">0</span> &#125;,<br>    &#123; <span class="hljs-number">0</span>,<span class="hljs-number">0</span>,<span class="hljs-number">0</span>,<span class="hljs-number">0</span>,<span class="hljs-number">0</span>,<span class="hljs-number">1</span>,<span class="hljs-number">1</span>,<span class="hljs-number">0</span>,<span class="hljs-number">0</span>,<span class="hljs-number">0</span>,<span class="hljs-number">0</span>,<span class="hljs-number">0</span> &#125;,<br>    &#123; <span class="hljs-number">0</span>,<span class="hljs-number">0</span>,<span class="hljs-number">0</span>,<span class="hljs-number">0</span>,<span class="hljs-number">1</span>,<span class="hljs-number">1</span>,<span class="hljs-number">1</span>,<span class="hljs-number">1</span>,<span class="hljs-number">0</span>,<span class="hljs-number">0</span>,<span class="hljs-number">0</span>,<span class="hljs-number">0</span> &#125;,<br>    &#123; <span class="hljs-number">0</span>,<span class="hljs-number">0</span>,<span class="hljs-number">0</span>,<span class="hljs-number">1</span>,<span class="hljs-number">1</span>,<span class="hljs-number">0</span>,<span class="hljs-number">0</span>,<span class="hljs-number">1</span>,<span class="hljs-number">1</span>,<span class="hljs-number">0</span>,<span class="hljs-number">0</span>,<span class="hljs-number">0</span> &#125;,<br>    &#123; <span class="hljs-number">0</span>,<span class="hljs-number">0</span>,<span class="hljs-number">0</span>,<span class="hljs-number">1</span>,<span class="hljs-number">1</span>,<span class="hljs-number">0</span>,<span class="hljs-number">0</span>,<span class="hljs-number">1</span>,<span class="hljs-number">1</span>,<span class="hljs-number">0</span>,<span class="hljs-number">0</span>,<span class="hljs-number">0</span> &#125;,<br>    &#123; <span class="hljs-number">0</span>,<span class="hljs-number">0</span>,<span class="hljs-number">1</span>,<span class="hljs-number">1</span>,<span class="hljs-number">0</span>,<span class="hljs-number">0</span>,<span class="hljs-number">0</span>,<span class="hljs-number">0</span>,<span class="hljs-number">1</span>,<span class="hljs-number">1</span>,<span class="hljs-number">0</span>,<span class="hljs-number">0</span> &#125;,<br>    &#123; <span class="hljs-number">0</span>,<span class="hljs-number">0</span>,<span class="hljs-number">1</span>,<span class="hljs-number">1</span>,<span class="hljs-number">0</span>,<span class="hljs-number">0</span>,<span class="hljs-number">0</span>,<span class="hljs-number">0</span>,<span class="hljs-number">1</span>,<span class="hljs-number">1</span>,<span class="hljs-number">0</span>,<span class="hljs-number">0</span> &#125;,<br>    &#123; <span class="hljs-number">0</span>,<span class="hljs-number">0</span>,<span class="hljs-number">1</span>,<span class="hljs-number">1</span>,<span class="hljs-number">1</span>,<span class="hljs-number">1</span>,<span class="hljs-number">1</span>,<span class="hljs-number">1</span>,<span class="hljs-number">1</span>,<span class="hljs-number">1</span>,<span class="hljs-number">0</span>,<span class="hljs-number">0</span> &#125;,<br>    &#123; <span class="hljs-number">0</span>,<span class="hljs-number">1</span>,<span class="hljs-number">1</span>,<span class="hljs-number">1</span>,<span class="hljs-number">1</span>,<span class="hljs-number">1</span>,<span class="hljs-number">1</span>,<span class="hljs-number">1</span>,<span class="hljs-number">1</span>,<span class="hljs-number">1</span>,<span class="hljs-number">1</span>,<span class="hljs-number">0</span> &#125;,<br>    &#123; <span class="hljs-number">0</span>,<span class="hljs-number">1</span>,<span class="hljs-number">1</span>,<span class="hljs-number">0</span>,<span class="hljs-number">0</span>,<span class="hljs-number">0</span>,<span class="hljs-number">0</span>,<span class="hljs-number">0</span>,<span class="hljs-number">0</span>,<span class="hljs-number">1</span>,<span class="hljs-number">1</span>,<span class="hljs-number">0</span> &#125;,<br>    &#123; <span class="hljs-number">0</span>,<span class="hljs-number">1</span>,<span class="hljs-number">1</span>,<span class="hljs-number">0</span>,<span class="hljs-number">0</span>,<span class="hljs-number">0</span>,<span class="hljs-number">0</span>,<span class="hljs-number">0</span>,<span class="hljs-number">0</span>,<span class="hljs-number">1</span>,<span class="hljs-number">1</span>,<span class="hljs-number">0</span> &#125;,<br>    &#123; <span class="hljs-number">0</span>,<span class="hljs-number">0</span>,<span class="hljs-number">0</span>,<span class="hljs-number">0</span>,<span class="hljs-number">0</span>,<span class="hljs-number">0</span>,<span class="hljs-number">0</span>,<span class="hljs-number">0</span>,<span class="hljs-number">0</span>,<span class="hljs-number">0</span>,<span class="hljs-number">0</span>,<span class="hljs-number">0</span> &#125;<br><br>&#125;;<span class="hljs-comment">//图案</span><br></code></pre></td></tr></table></figure><figure><img src="/2019/11/18/%E5%8C%BA%E5%9F%9F%E5%A1%AB%E5%85%85/图案填充-效果.jpg" alt="img"><figcaption aria-hidden="true">img</figcaption></figure><h2 id="逐点法">逐点法</h2><p>在逐点法实现中，用到了两个老师给好的函数。</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-keyword">struct</span> <span class="hljs-title class_">BoxRect_t</span> &#123;<br><span class="hljs-type">int</span> minX;<br><span class="hljs-type">int</span> minY;<br><span class="hljs-type">int</span> maxX;<br><span class="hljs-type">int</span> maxY;<br>&#125;;<br><br><span class="hljs-function"><span class="hljs-type">void</span> <span class="hljs-title">CcgPDHFillPolyView::PointFillpoly</span><span class="hljs-params">(<span class="hljs-type">int</span> pNumbers, CPoint *points, CDC *pDC)</span></span><br><span class="hljs-function"></span>&#123;<br> BoxRect_t polyRect;<br> <br> polyRect = <span class="hljs-built_in">GetPolygonRect</span>(pNumbers, points);<br> <br> m_pDC-&gt;<span class="hljs-built_in">MoveTo</span>(polyRect.minX, polyRect.minY);<br> m_pDC-&gt;<span class="hljs-built_in">LineTo</span>(polyRect.minX, polyRect.maxY);<br> m_pDC-&gt;<span class="hljs-built_in">LineTo</span>(polyRect.maxX, polyRect.maxY);<br> m_pDC-&gt;<span class="hljs-built_in">LineTo</span>(polyRect.maxX, polyRect.minY);<br> m_pDC-&gt;<span class="hljs-built_in">LineTo</span>(polyRect.minX, polyRect.minY);<br> <br> CPoint testPoint;<br> <span class="hljs-comment">//从最小点到最大点一次判断，如果在该多边形内部，则进行填充</span><br> <span class="hljs-keyword">for</span> (testPoint.x = polyRect.minX; testPoint.x &lt; polyRect.maxX; testPoint.x++) <br> <span class="hljs-keyword">for</span> (testPoint.y = polyRect.minY; testPoint.y &lt; polyRect.maxY; testPoint.y++) &#123;<br> <span class="hljs-keyword">if</span> (<span class="hljs-built_in">PointInPolygon</span>(m_pNumbers, m_pAccord, testPoint))<br> pDC-&gt;<span class="hljs-built_in">SetPixel</span>(testPoint.x, testPoint.y, <span class="hljs-built_in">RGB</span>(<span class="hljs-number">255</span>,<span class="hljs-number">0</span>,<span class="hljs-number">0</span>));<br>      &#125;<br>&#125;<br><br><span class="hljs-comment">//得到该多边形的最大、最小的y、x值</span><br><span class="hljs-function">BoxRect_t <span class="hljs-title">CcgPDHFillPolyView::GetPolygonRect</span><span class="hljs-params">(<span class="hljs-type">int</span> pointNumOfPolygon, CPoint* tarPolygon)</span></span><br><span class="hljs-function"></span>&#123;<br>BoxRect_t boxRect;<br><br>boxRect.minX = <span class="hljs-number">50000</span>;<br>boxRect.minY = <span class="hljs-number">50000</span>;<br>boxRect.maxX = <span class="hljs-number">-50000</span>;<br>boxRect.maxY = <span class="hljs-number">-50000</span>;<br><br><span class="hljs-keyword">for</span> (<span class="hljs-type">int</span> i = <span class="hljs-number">0</span>; i &lt; pointNumOfPolygon; i++) &#123;<br><span class="hljs-keyword">if</span> (tarPolygon[i].x &lt; boxRect.minX) boxRect.minX = tarPolygon[i].x;<br><span class="hljs-keyword">if</span> (tarPolygon[i].y &lt; boxRect.minY) boxRect.minY = tarPolygon[i].y;<br><span class="hljs-keyword">if</span> (tarPolygon[i].x &gt; boxRect.maxX) boxRect.maxX = tarPolygon[i].x;<br><span class="hljs-keyword">if</span> (tarPolygon[i].y &gt; boxRect.maxY) boxRect.maxY = tarPolygon[i].y;<br>&#125;<br><span class="hljs-keyword">return</span> boxRect;<br>&#125;<br><br><span class="hljs-comment">//判断点是否位于区域内</span><br><span class="hljs-function">BOOL <span class="hljs-title">CcgPDHFillPolyView::PointInPolygon</span><span class="hljs-params">(<span class="hljs-type">int</span> pointNumOfPolygon, CPoint tarPolygon[], CPoint testPoint)</span></span><br><span class="hljs-function"></span>&#123;<br><span class="hljs-keyword">if</span> (pointNumOfPolygon &lt; <span class="hljs-number">3</span>)<br><span class="hljs-keyword">return</span> <span class="hljs-literal">false</span>;<br><br><span class="hljs-type">bool</span>  inSide = FALSE;<br><span class="hljs-type">float</span> lineSlope,interSectX;<br><span class="hljs-type">int</span>   i = <span class="hljs-number">0</span>, j = pointNumOfPolygon<span class="hljs-number">-1</span>;<br><br><span class="hljs-keyword">for</span> (i = <span class="hljs-number">0</span>; i &lt; pointNumOfPolygon; i++) &#123;<br><span class="hljs-keyword">if</span> ((tarPolygon[i].y &lt; testPoint.y &amp;&amp; tarPolygon[j].y &gt;= testPoint.y  ||<br>tarPolygon[j].y &lt; testPoint.y &amp;&amp; tarPolygon[i].y &gt;= testPoint.y) &amp;&amp;<br>(tarPolygon[i].x &lt;= testPoint.x || tarPolygon[j].x &lt;= testPoint.x)) &#123;<br><span class="hljs-keyword">if</span> (tarPolygon[j].x != tarPolygon[i].x) &#123;<br>lineSlope = (<span class="hljs-type">float</span>)(tarPolygon[j].y-tarPolygon[i].y)/(tarPolygon[j].x-tarPolygon[i].x);<br>interSectX = (<span class="hljs-type">int</span>)(tarPolygon[i].x + (testPoint.y-tarPolygon[i].y) / lineSlope);<br>&#125; <span class="hljs-keyword">else</span> <br>interSectX = tarPolygon[i].x;<br><span class="hljs-keyword">if</span> (interSectX &lt; testPoint.x)<br>inSide = !inSide;<br>&#125;<br>j = i;<br>&#125;<br><br><span class="hljs-keyword">return</span> inSide;<br>&#125;<br></code></pre></td></tr></table></figure><p>其中的实现原理十分简单，不过PointInPolygon(int pointNumOfPolygon,CPoint tarPolygon[], CPointtestPoint)函数的实现比较复杂，不过这个函数也十分实用，在后面的三角剖分中我也有使用。</p><figure><img src="/2019/11/18/%E5%8C%BA%E5%9F%9F%E5%A1%AB%E5%85%85/逐点法-效果.jpg" alt="img"><figcaption aria-hidden="true">img</figcaption></figure><h2 id="种子填充">种子填充</h2><p>点填充有好几种方法，其中比较简单实现的是四连通泛填充算法。基本思路就是给定种子，然后去填充种子上下左右四个方向的像素点，如果为空，则进行填充。</p><p>实现代码如下~</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-function"><span class="hljs-type">void</span> <span class="hljs-title">FloodFill4</span><span class="hljs-params">(<span class="hljs-type">int</span> x, <span class="hljs-type">int</span> y, <span class="hljs-type">int</span> fillColor, <span class="hljs-type">int</span> oldColor)</span></span><br><span class="hljs-function"></span>&#123;<br>    <span class="hljs-type">int</span> current;<br>    current = <span class="hljs-built_in">GetPixel</span>(x,y);<br>    <span class="hljs-keyword">if</span>( current == oldColor )&#123;<br>        <span class="hljs-built_in">SetPixel</span>(x,y,fillColor);<br>        <span class="hljs-built_in">FloodFill4</span>(x+<span class="hljs-number">1</span>,y,fillColor,oldColor);<br>        <span class="hljs-built_in">FloodFill4</span>(x<span class="hljs-number">-1</span>,y,fillColor,oldColor);<br>        <span class="hljs-built_in">FloodFill4</span>(x,y+<span class="hljs-number">1</span>,fillColor,oldColor);<br>        <span class="hljs-built_in">FloodFill4</span>(x,y<span class="hljs-number">-1</span>,fillColor,oldColor);<br>    &#125;<br>&#125;<br></code></pre></td></tr></table></figure><p>不过如果运行这一段代码很容易就会导致栈溢出，，，虽然代码简单，但是没法用。</p><p>所以更多的是使用扫描线种子填充算法。</p><ul><li>种子像素入栈。</li><li>当栈非空时，重复执行一下操作。<ul><li>栈顶像素出栈。</li><li>沿扫描线对出栈像素的左右像素进行填充，直到遇到边界像素为止。</li><li>将上述区间内的最左、最右像素记为 <span class="math inline">\(x_{left}\)</span> 和 <span class="math inline">\(x_{right}\)</span></li><li>在区间[ <span class="math inline">\(x_{left}\)</span> , <span class="math inline">\(x_{right}\)</span>]内检查与当前扫描线相邻的上下两条扫描线是否全为边界像素或已填充的像素，若为非边界和未填充，则把每一区间的最右像素<span class="math inline">\(x_{right}\)</span>作为种子像素压入堆栈，重复执行上述操作。</li></ul></li></ul><figure><img src="/2019/11/18/%E5%8C%BA%E5%9F%9F%E5%A1%AB%E5%85%85/种子填充-1.png" alt="img"><figcaption aria-hidden="true">img</figcaption></figure><figure><img src="/2019/11/18/%E5%8C%BA%E5%9F%9F%E5%A1%AB%E5%85%85/种子填充-2.png" alt="img"><figcaption aria-hidden="true">img</figcaption></figure><figure><img src="/2019/11/18/%E5%8C%BA%E5%9F%9F%E5%A1%AB%E5%85%85/种子填充-3.png" alt="img"><figcaption aria-hidden="true">img</figcaption></figure><p>实现代码如下~</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-comment">//扫描线种子填充</span><br><span class="hljs-function"><span class="hljs-type">int</span> <span class="hljs-title">SetRP</span><span class="hljs-params">(<span class="hljs-type">int</span> x, <span class="hljs-type">int</span> y, COLORREF color, COLORREF mColor, CDC *pDC)</span> </span>&#123;<br><span class="hljs-keyword">while</span> (pDC-&gt;<span class="hljs-built_in">GetPixel</span>(<span class="hljs-built_in">CPoint</span>(x, y)) == mColor) &#123;<br>pDC-&gt;<span class="hljs-built_in">SetPixel</span>(x, y, color);<br>x++;<br>&#125;<br><span class="hljs-keyword">return</span> x - <span class="hljs-number">1</span>;<br>&#125;<br><span class="hljs-function"><span class="hljs-type">int</span> <span class="hljs-title">SetLP</span><span class="hljs-params">(<span class="hljs-type">int</span> x, <span class="hljs-type">int</span> y, COLORREF color, COLORREF mColor, CDC *pDC)</span> </span>&#123;<br><span class="hljs-keyword">while</span> (pDC-&gt;<span class="hljs-built_in">GetPixel</span>(<span class="hljs-built_in">CPoint</span>(x - <span class="hljs-number">1</span>, y)) == mColor) &#123;<br>pDC-&gt;<span class="hljs-built_in">SetPixel</span>(--x, y, color);<br>&#125;<br><span class="hljs-keyword">return</span> x+<span class="hljs-number">1</span>;<br>&#125;<br><span class="hljs-function"><span class="hljs-type">void</span> <span class="hljs-title">NewLineSeed</span><span class="hljs-params">(std::stack&lt;CPoint&gt; *stk, <span class="hljs-type">int</span> lx, <span class="hljs-type">int</span> rx, <span class="hljs-type">int</span> y, COLORREF color, COLORREF mColor, CDC *pDC)</span> </span>&#123;<br><span class="hljs-type">int</span> x, e;<br><span class="hljs-keyword">for</span> (x = lx + <span class="hljs-number">1</span>, e = rx + <span class="hljs-number">1</span>; x&lt;e; x++) &#123;<br><span class="hljs-comment">//找出每一个区间的最右像素，入栈</span><br><span class="hljs-keyword">if</span> (pDC-&gt;<span class="hljs-built_in">GetPixel</span>(<span class="hljs-built_in">CPoint</span>(x, y)) != mColor) &#123;<br><span class="hljs-keyword">if</span> (pDC-&gt;<span class="hljs-built_in">GetPixel</span>(<span class="hljs-built_in">CPoint</span>(x - <span class="hljs-number">1</span>, y)) == mColor)<br>stk-&gt;<span class="hljs-built_in">push</span>(<span class="hljs-built_in">CPoint</span>(x - <span class="hljs-number">1</span>, y));<br>&#125;<br>&#125;<br><span class="hljs-comment">//把rx所在点入栈</span><br><span class="hljs-keyword">if</span> (pDC-&gt;<span class="hljs-built_in">GetPixel</span>(<span class="hljs-built_in">CPoint</span>(x - <span class="hljs-number">1</span>, y)) == mColor)<br>stk-&gt;<span class="hljs-built_in">push</span>(<span class="hljs-built_in">CPoint</span>(x - <span class="hljs-number">1</span>, y));<br>&#125;<br><span class="hljs-function"><span class="hljs-type">void</span> <span class="hljs-title">CcgPDHFillPolyView::ScanLineFill4</span><span class="hljs-params">(<span class="hljs-type">int</span> x, <span class="hljs-type">int</span> y, COLORREF color, CDC *pDC)</span></span><br><span class="hljs-function"></span>&#123;<br><span class="hljs-type">int</span> pRight, pLeft;<br>std::stack&lt;CPoint&gt; stk;<br><span class="hljs-type">int</span> mColor = pDC-&gt;<span class="hljs-built_in">GetPixel</span>(x, y);<span class="hljs-comment">//给定种子</span><br><br>stk.<span class="hljs-built_in">push</span>(<span class="hljs-built_in">CPoint</span>(x, y));<br><span class="hljs-keyword">while</span> (!stk.<span class="hljs-built_in">empty</span>()) &#123;<br>CPoint p = stk.<span class="hljs-built_in">top</span>();<span class="hljs-comment">//栈顶像素出栈</span><br>stk.<span class="hljs-built_in">pop</span>();<br><br>pRight = <span class="hljs-built_in">SetRP</span>(p.x, p.y, color, mColor, pDC);<span class="hljs-comment">//向左向右进行填充</span><br>pLeft = <span class="hljs-built_in">SetLP</span>(p.x, p.y, color, mColor, pDC);<br><br>        <span class="hljs-comment">//上下两条扫描线处理</span><br><span class="hljs-built_in">NewLineSeed</span>(&amp;stk, pLeft, pRight, p.y + <span class="hljs-number">1</span>, color, mColor, pDC);<br><span class="hljs-built_in">NewLineSeed</span>(&amp;stk, pLeft, pRight, p.y - <span class="hljs-number">1</span>, color, mColor, pDC);<br>&#125;<br>&#125;<br></code></pre></td></tr></table></figure><p>扫描线种子填充算法的填充效果很好，不过填充速度会慢很多，，能够在屏幕上看到一个个像素点在填充。</p><figure><img src="/2019/11/18/%E5%8C%BA%E5%9F%9F%E5%A1%AB%E5%85%85/种子填充-效果.jpg" alt="img"><figcaption aria-hidden="true">img</figcaption></figure><h2 id="多边形三角剖分">多边形三角剖分</h2><p>关于这个拓展功能的实现，当时在网上看了不少资料，，然后大部分都没有看懂。不过上课的时候老师给了一种使用递归实现的方法，后来发现和网上的耳切法比较相似。老师给出的实现思路如下~</p><ul><li>选择多边形的最左顶点L+前后顶点，构成一个三角形。</li><li>检查该三角形内是否有其他顶点。<ul><li>不包含其他顶点。则分割该三角形，递归调用第一步。</li><li>若包含其他顶点。则连接L与进入的顶点中最左侧的点，这样便会分割该多边形，然后对两个多边形再递归调用第一步。</li></ul></li></ul><p>实现代码如下~</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-function"><span class="hljs-type">void</span> <span class="hljs-title">CcgPDHFillPolyView::Triangulation</span><span class="hljs-params">(CPoint* points, <span class="hljs-type">int</span> pNumbers, <span class="hljs-type">int</span> number)</span></span><br><span class="hljs-function"></span>&#123;<br><span class="hljs-keyword">if</span> (pNumbers == <span class="hljs-number">3</span>) &#123;<span class="hljs-comment">//出口</span><br><span class="hljs-keyword">return</span>;<br>&#125;<br><br><span class="hljs-type">int</span> k, xMin = <span class="hljs-number">1000</span>;<br><span class="hljs-keyword">for</span> (<span class="hljs-type">int</span> j = <span class="hljs-number">0</span>; j &lt; pNumbers; j++)<span class="hljs-comment">//找出当前多边形的最左侧顶点</span><br>&#123;<br><span class="hljs-keyword">if</span> (points[j].x &lt; xMin) &#123;<br>k = j;<br>xMin = points[j].x;<br>&#125;<br>&#125;<br><br>CPoint arry[<span class="hljs-number">3</span>];<br>arry[<span class="hljs-number">0</span>] = points[k];<span class="hljs-comment">//最左侧顶点</span><br><span class="hljs-type">int</span> next = (k + <span class="hljs-number">1</span>) % pNumbers;<br>arry[<span class="hljs-number">1</span>] = points[next];<span class="hljs-comment">//后一个顶点</span><br><span class="hljs-type">int</span> pre = k - <span class="hljs-number">1</span>;<br><span class="hljs-keyword">if</span> (pre &lt; <span class="hljs-number">0</span>) <br>pre += pNumbers;<br>arry[<span class="hljs-number">2</span>] = points[pre];<span class="hljs-comment">//前一个顶点</span><br><br><span class="hljs-comment">//围成的该三角形内是否有其他顶点</span><br>CPoint in_point[N];<br><span class="hljs-type">int</span> in_number = <span class="hljs-number">0</span>, i = <span class="hljs-number">0</span>;<br>xMin = <span class="hljs-number">1000</span>; <br><span class="hljs-keyword">for</span> (<span class="hljs-type">int</span> j = <span class="hljs-number">0</span>; j &lt; pNumbers; j++)<br>&#123;<br><span class="hljs-keyword">if</span> (j == k || j == next || j == pre)<span class="hljs-comment">//三角形的三个顶点不算</span><br><span class="hljs-keyword">continue</span>;<br><span class="hljs-keyword">if</span> (<span class="hljs-built_in">PointInPolygon</span>(<span class="hljs-number">3</span>,arry, points[j])) &#123;<span class="hljs-comment">//找出在该三角形内的顶点，并找到其中的最左侧顶点</span><br>in_point[in_number] = points[j];<br><span class="hljs-keyword">if</span> (in_point[in_number].x &lt; xMin) &#123;<br>i = j;<br>xMin = in_point[in_number].x;<br>&#125;<br>in_number++;<br>&#125;<br>&#125;<br><span class="hljs-keyword">if</span> (in_number &gt;= <span class="hljs-number">1</span>) &#123;<span class="hljs-comment">//若存在，则链接三角形内的最左侧点，分割多边形，递归调用</span><br><br>m_pDC-&gt;<span class="hljs-built_in">MoveTo</span>(arry[<span class="hljs-number">0</span>].x, arry[<span class="hljs-number">0</span>].y);<span class="hljs-comment">//连接线到该最左侧顶点</span><br>m_pDC-&gt;<span class="hljs-built_in">LineTo</span>(points[i].x, points[i].y);<br><br>CPoint arry_1[N], arry_2[N];<span class="hljs-comment">//分割成两个多边形</span><br><span class="hljs-type">int</span> pNumbers_1 = <span class="hljs-number">0</span>, pNumbers_2 = <span class="hljs-number">0</span>;<br><br><span class="hljs-keyword">if</span> (k &gt; i) &#123;<br><span class="hljs-type">int</span> temp = i; i = k; k = temp;<br>&#125;<br><br><span class="hljs-keyword">for</span> (<span class="hljs-type">int</span> j = <span class="hljs-number">0</span>; j &lt; pNumbers; j++)<br>&#123;<br><span class="hljs-keyword">if</span> (j &gt;= k&amp;&amp;j &lt;= i) &#123;<br>arry_1[pNumbers_1++] = points[j];<br>&#125;<br><span class="hljs-keyword">if</span> (j &lt;= k || j &gt;= i) &#123;<br>arry_2[pNumbers_2++] = points[j];<br>&#125;<br>&#125;<br><span class="hljs-built_in">Triangulation</span>(arry_1, pNumbers_1, pNumbers_1);<br><span class="hljs-built_in">Triangulation</span>(arry_2, pNumbers_2, pNumbers_2);<br>&#125;<br><span class="hljs-keyword">else</span> &#123;<span class="hljs-comment">//若不存在，则分割该三角形，递归调用</span><br>m_pDC-&gt;<span class="hljs-built_in">MoveTo</span>(arry[<span class="hljs-number">1</span>].x, arry[<span class="hljs-number">1</span>].y);<span class="hljs-comment">//剖分线</span><br>m_pDC-&gt;<span class="hljs-built_in">LineTo</span>(arry[<span class="hljs-number">2</span>].x, arry[<span class="hljs-number">2</span>].y);<br><br><span class="hljs-keyword">for</span> (<span class="hljs-type">int</span> j = k; j &lt; pNumbers - <span class="hljs-number">1</span>; j++)<span class="hljs-comment">//删除k顶点</span><br>&#123;<br>points[j] = points[j + <span class="hljs-number">1</span>];<br>&#125;<br>pNumbers--;<br><br><span class="hljs-built_in">Triangulation</span>(points, pNumbers, number);<br>&#125;<br>&#125;<br></code></pre></td></tr></table></figure><p>这一部分代码在编写的时候，还是比较掉头发的，当时在处理多边形分割的时候遇到了很多问题，后来也是通过单步调试解决的。最后成功之后，还是蛮有成就感的！</p><figure><img src="/2019/11/18/%E5%8C%BA%E5%9F%9F%E5%A1%AB%E5%85%85/三角剖分-效果.jpg" alt="img"><figcaption aria-hidden="true">img</figcaption></figure>]]></content>
    
    
    <categories>
      
      <category>study</category>
      
    </categories>
    
    
    <tags>
      
      <tag>MFC</tag>
      
      <tag>c++</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>空间域与频率域滤波-基础知识</title>
    <link href="/2019/11/18/%E7%A9%BA%E9%97%B4%E5%9F%9F%E4%B8%8E%E9%A2%91%E7%8E%87%E5%9F%9F%E6%BB%A4%E6%B3%A2-%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/"/>
    <url>/2019/11/18/%E7%A9%BA%E9%97%B4%E5%9F%9F%E4%B8%8E%E9%A2%91%E7%8E%87%E5%9F%9F%E6%BB%A4%E6%B3%A2-%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/</url>
    
    <content type="html"><![CDATA[<p>最近学习接触了在空间域和频率域对图像处理的知识，在使用matlab操作的时候也是有很多收获，在此记录并进一步学习整理一下啦~</p><span id="more"></span><h2 id="空间域操作">空间域操作</h2><p>对于空间域对图像进行操作主要包括这几个方面：</p><ul><li>灰度变换<ul><li>灰度变换函数 <span class="math inline">\(s=T(r)\)</span>，这种技术有时被称为对比度拉伸</li><li>在极限情况下，会产生一幅二值图像。这种形式的映射称为阈值处理函数</li><li>灰度变换函数的曲线：<ul><li>图像反转（得到底片）、恒等变换、对数变换等，还有分段线性变换函数（对比度拉伸、灰度级分层等效果）</li></ul></li><li><figure><img src="/2019/11/18/%E7%A9%BA%E9%97%B4%E5%9F%9F%E4%B8%8E%E9%A2%91%E7%8E%87%E5%9F%9F%E6%BB%A4%E6%B3%A2-%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/1.png" alt="img"><figcaption aria-hidden="true">img</figcaption></figure></li></ul></li><li>直方图（对灰度百分比的统计）<ul><li>归一化的直方图：表示灰度级r在图像中出现的概率的一个估计。其所有分量之和等于1</li><li>可用于图像增强</li><li>处理过程：<ul><li>首先可以得到一个灰度分布和直方图的值（均一化后，<span class="math inline">\(p_r(r_k)\)</span>）。例如灰度级个数为L=7</li><li>然后去计算变换函数，即 <span class="math inline">\(s_i =(L-1)\sum_{j=o}^i p_r(r_j)\)</span> ，i从0开始</li><li>对 <span class="math inline">\(s_i\)</span> 取整</li><li>在取整后，灰度级可能会减少，即会被映射到不同的灰度级上。根据映射关系来确定变换后的灰度级对应的像素值，会出现多个灰度级被映射到一个灰度级的情况，那么像素值相加即可。</li></ul></li></ul></li><li>滤波器<ul><li>对应位置相乘后累加，得到当前像素点的值（中心像素）</li><li>平滑滤波器（全是正数，要乘上滤波器中所有系数的和，这是计算平均值所要求的）<ul><li>这种滤波器有时候也叫均值滤波器，就是实现一种模糊操作，相当于降低了“尖锐”的变化部分。这样看的话，其实也可以理解为一种频率波的低通滤波器，减少了高频的部分。（也可以用于处理含有椒盐噪声的图像）</li></ul></li><li>中值滤波器<ul><li>统计排序滤波器（非线性），使用包围区域内排列统计结果决定的值来替代中心像素的值</li><li>中值滤波器就是使用中值来替代</li><li>能够很好的处理椒盐噪声</li></ul></li><li>锐化滤波器（有正有负）<ul><li>突出灰度的过渡部分，其推导过程是通过离散函数的二阶微分得到的，通过微分得到原函数的变化趋势，即像素值的变化快慢。也可以进一步推导到二次微分，即拉普拉斯算子。</li><li>二阶微分 = <span class="math inline">\(f(x+1) + f(x-1) -2f(x)\)</span></li><li>使用二阶微分进行图像锐化——拉普拉斯算子</li><li>两个变量的拉普拉斯算子 = <span class="math inline">\(f(x+1,y)+f(x-1,y)+f(x,y+1)+f(x,y-1)-4f(x,y)\)</span></li><li>由于拉普拉斯图像中既有正值也有负值，所以负值标为0</li></ul></li></ul></li><li>梯度<ul><li>一阶微分进行图像锐化（边缘检测、边缘增强）</li><li>梯度经常用robert交叉微分算子，Prewitt微分算子，sobel算子来实现</li></ul></li></ul><p>在空间域进行滤波操作的时候，可能会出现滤波器和图像运算不全面的情况，所以需要进行padding操作。</p><h3 id="常用空间滤波器代码matlab">常用空间滤波器代码(matlab)</h3><figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><code class="hljs matlab"><span class="hljs-comment">% 显示原图像：</span><br>f=imread(str_path); <br>subplot(<span class="hljs-number">1</span>,<span class="hljs-number">2</span>,<span class="hljs-number">1</span>),imshow(f); <br><br><span class="hljs-comment">%fspecial：用于产生预定义滤波器</span><br>h1=fspecial(<span class="hljs-string">&#x27;sobel&#x27;</span>);<span class="hljs-comment">%sobel水平边缘增强滤波器</span><br>h2=fspecial(<span class="hljs-string">&#x27;gaussian&#x27;</span>);<span class="hljs-comment">%高斯低通滤波器</span><br>h3=fspecial(<span class="hljs-string">&#x27;laplacian&#x27;</span>);<span class="hljs-comment">%拉普拉斯滤波器</span><br>h4=fspecial(<span class="hljs-string">&#x27;log&#x27;</span>);<span class="hljs-comment">%高斯拉普拉斯（LoG）滤波器</span><br>h5=fspecial(<span class="hljs-string">&#x27;average&#x27;</span>);<span class="hljs-comment">%均值滤波器</span><br>g6=medfilt2(f);<span class="hljs-comment">%中值滤波</span><br><br>g=filter2(h1,f,<span class="hljs-string">&#x27;same&#x27;</span>);<br>g = uint8(g);<br><br>subplot(<span class="hljs-number">1</span>,<span class="hljs-number">2</span>,<span class="hljs-number">2</span>),imshow(g); <br><br><span class="hljs-comment">% 利用Sobel算子锐化图像</span><br>h=[<span class="hljs-number">1</span>,<span class="hljs-number">2</span>,<span class="hljs-number">1</span>;<br><span class="hljs-number">0</span>,<span class="hljs-number">0</span>,<span class="hljs-number">0</span>;<br><span class="hljs-number">-1</span>,<span class="hljs-number">-2</span>,<span class="hljs-number">-1</span><br>];<span class="hljs-comment">%Sobel算子</span><br><span class="hljs-built_in">j</span>=filter2(h,f);<br><br><span class="hljs-comment">% 利用拉氏算子锐化图像</span><br><span class="hljs-built_in">j</span>=double(f);<br>h=[<span class="hljs-number">0</span>,<span class="hljs-number">1</span>,<span class="hljs-number">0</span>;<span class="hljs-number">1</span>,<span class="hljs-number">-4</span>,<span class="hljs-number">1</span>;<span class="hljs-number">0</span>,<span class="hljs-number">1</span>,<span class="hljs-number">0</span>];<span class="hljs-comment">%拉氏算子</span><br>k=conv2(<span class="hljs-built_in">j</span>,h,<span class="hljs-string">&#x27;same&#x27;</span>);<br><br><span class="hljs-comment">% 图像边缘检测</span><br><span class="hljs-built_in">j</span> = edge(f,<span class="hljs-string">&#x27;sobel&#x27;</span>,thresh)<span class="hljs-comment">% sobel算子</span><br><span class="hljs-built_in">j</span> = edge(f,<span class="hljs-string">&#x27;prewitt&#x27;</span>,thresh)<span class="hljs-comment">%prewitt算子</span><br><span class="hljs-built_in">j</span> = edge(f,<span class="hljs-string">&#x27;roberts&#x27;</span>,thresh)<span class="hljs-comment">%roberts算子</span><br><span class="hljs-built_in">j</span> = edge(f,<span class="hljs-string">&#x27;log&#x27;</span>,thresh)<span class="hljs-comment">%log算子</span><br><span class="hljs-built_in">j</span> = edge(f,<span class="hljs-string">&#x27;canny&#x27;</span>,thresh)<span class="hljs-comment">%canny算子</span><br><span class="hljs-built_in">j</span> = edge(f,<span class="hljs-string">&#x27;zerocross&#x27;</span>,thresh)<span class="hljs-comment">%Zero-Cross算子</span><br><br></code></pre></td></tr></table></figure><h2 id="频率域操作">频率域操作</h2><p>关于如何从空间域变换到频率域的数学推导就不写了，自己也是大概理解，在频率域上操作可以实现一些在空间域难以实现的操作，比如在频率域我们能够很明了的看出图像的高频部分（变化较快）和低频部分（变化较慢），从而来设计高通、低通、带通和带阻滤波器等来对图像进行不同的操作。</p><p>这里直接给出进行频率域操作的代码，感觉太过于公式化或是理论也不太好，从编写代码的过程中中感觉能学到更多。（我就不放实验结果了，呜呜呜 ~ ）</p><h3 id="使用高通低通滤波器">使用高通/低通滤波器</h3><figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><code class="hljs matlab"><span class="hljs-comment">% 代码中用到的paddedsize、lpfilter、hpfilter以及dftfilt函数都可以直接从网上找到。</span><br>strFile = <span class="hljs-string">&#x27;test.jpg&#x27;</span>;<br><br>o_img = imread(strFile);<br>subplot(<span class="hljs-number">1</span>,<span class="hljs-number">2</span>,<span class="hljs-number">1</span>);<br>imshow(o_img);<br>img = o_img;<br><br><span class="hljs-comment">% 彩色图像</span><br><span class="hljs-keyword">for</span> <span class="hljs-built_in">i</span>=<span class="hljs-number">1</span>:<span class="hljs-number">3</span><br>    temp_img = img(:,:,<span class="hljs-built_in">i</span>);<br>    temp_img = <span class="hljs-built_in">squeeze</span>(temp_img);<br>    PQ = paddedsize(<span class="hljs-built_in">size</span>(temp_img)); <span class="hljs-comment">%产生滤波时所需大小的矩阵</span><br>    D0 = <span class="hljs-number">0.05</span> * PQ(<span class="hljs-number">1</span>);    <span class="hljs-comment">%设定高斯滤波器的阈值</span><br>    H = lpfilter(<span class="hljs-string">&#x27;gaussian&#x27;</span>,PQ(<span class="hljs-number">1</span>),PQ(<span class="hljs-number">2</span>),D0); <span class="hljs-comment">%产生低斯高通滤波器</span><br>    <span class="hljs-comment">%H = hpfilter(&#x27;gaussian&#x27;,PQ(1),PQ(2),D0); %产生高斯高通滤波器</span><br><br>    temp_img = dftfilt(temp_img,H);<br>    img(:,:,<span class="hljs-built_in">i</span>) = temp_img;<br><span class="hljs-keyword">end</span><br><br>subplot(<span class="hljs-number">1</span>,<span class="hljs-number">2</span>,<span class="hljs-number">2</span>);<br>imshow(img);<br></code></pre></td></tr></table></figure><h3 id="使用带阻滤波器">使用带阻滤波器</h3><figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><code class="hljs matlab">str_path = <span class="hljs-string">&#x27;test.png&#x27;</span>;<br><br><span class="hljs-comment">% 无padding过程</span><br><br><span class="hljs-comment">% 显示原图像：</span><br>f=imread(str_path); <br><span class="hljs-built_in">figure</span>,subplot(<span class="hljs-number">2</span>,<span class="hljs-number">2</span>,<span class="hljs-number">1</span>),imshow(f); <br>title(<span class="hljs-string">&#x27;原图像&#x27;</span>);<br>F=fft2(f);  <span class="hljs-comment">% 二维快速傅里叶变换</span><br>[M, N]=<span class="hljs-built_in">size</span>(f);<br>f=double(f);<br><br>fc=fftshift(F); <span class="hljs-comment">%将零频分量移动到数组中心,重新排列傅里叶变换 </span><br>s=<span class="hljs-built_in">log</span>(<span class="hljs-number">1</span>+<span class="hljs-built_in">abs</span>(fc)); <br>subplot(<span class="hljs-number">2</span>,<span class="hljs-number">2</span>,<span class="hljs-number">2</span>),imshow(s,[]);<br>title(<span class="hljs-string">&#x27;原图像频谱&#x27;</span>);<br><br><span class="hljs-comment">%理想带阻滤波器滤波效果：</span><br>[u,v]=dftuv(M,N);<span class="hljs-comment">% 确定滤波函数大小</span><br>d1=<span class="hljs-built_in">sqrt</span>(u.^<span class="hljs-number">2</span>+v.^<span class="hljs-number">2</span>); <br><br><span class="hljs-comment">% 设计带阻滤波器</span><br>w=<span class="hljs-number">5</span>;<br>D0=<span class="hljs-number">15</span>;<br>ba= d1&lt;(D0-w/<span class="hljs-number">2</span>) | d1&gt;(D0+w/<span class="hljs-number">2</span>);<br>H=double(ba);<span class="hljs-comment">% 滤波函数</span><br><br><span class="hljs-comment">% 设计巴特沃斯带阻滤波器</span><br><span class="hljs-comment">%n=3;</span><br><span class="hljs-comment">%H = 1./(1 + (w*d1./(d1.^2-D0^2)).^(2*n));</span><br><br><span class="hljs-comment">% 高斯带阻滤波：</span><br><span class="hljs-comment">%H = 1-exp(-1/2*(((d1.^2)-D0^2)./(d1*w)).^2);</span><br><br>F = fft2(f);<span class="hljs-comment">% 频域率滤波</span><br><br>g=<span class="hljs-built_in">real</span>(ifft2(H.*F));<span class="hljs-comment">% 反变换回空间域</span><br>subplot(<span class="hljs-number">2</span>,<span class="hljs-number">2</span>,<span class="hljs-number">3</span>),imshow(g,[]); <br>title(<span class="hljs-string">&#x27;理想带阻&#x27;</span>);<br>F=fft2(g); <br>fc=fftshift(F); <br>s=<span class="hljs-built_in">log</span>(<span class="hljs-number">1</span>+<span class="hljs-built_in">abs</span>(fc));  <br>subplot(<span class="hljs-number">2</span>,<span class="hljs-number">2</span>,<span class="hljs-number">4</span>),imshow(s,[]); <br>title(<span class="hljs-string">&#x27;理想带阻频谱&#x27;</span>);<br><br></code></pre></td></tr></table></figure>]]></content>
    
    
    <categories>
      
      <category>study</category>
      
    </categories>
    
    
    <tags>
      
      <tag>matlab</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>二维图像变换-裁剪</title>
    <link href="/2019/11/18/%E4%BA%8C%E7%BB%B4%E5%9B%BE%E5%83%8F%E5%8F%98%E6%8D%A2-%E8%A3%81%E5%89%AA/"/>
    <url>/2019/11/18/%E4%BA%8C%E7%BB%B4%E5%9B%BE%E5%83%8F%E5%8F%98%E6%8D%A2-%E8%A3%81%E5%89%AA/</url>
    
    <content type="html"><![CDATA[<p>实现二位图像的平移、旋转及缩放等操作，并在指定的显示框中进行裁剪和显示。</p><span id="more"></span><h2 id="定义的数据结构">定义的数据结构</h2><p>对于每一个被裁剪的多边形和线段，分别保存原来的顶点信息、变换后的顶点信息以及被裁剪后的顶点信息，并根据这些数据来进行显示。</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><code class="hljs c++">CPoint P1, P2;                 <span class="hljs-comment">// Original Space Line</span><br>CPoint TP1, TP2;               <span class="hljs-comment">// 变换后的线段端点</span><br>CPoint CP1, CP2;               <span class="hljs-comment">// 位于window框中切割出来的线段端点</span><br><span class="hljs-type">float</span> m_lineMatrix[<span class="hljs-number">3</span>][<span class="hljs-number">2</span>];<br><span class="hljs-comment">// Liang-Barsky Line Space Model</span><br>CPoint LB_P1, LB_P2;           <span class="hljs-comment">// Original Space Line</span><br>CPoint LB_TP1, LB_TP2;         <span class="hljs-comment">// Trans Space Line</span><br>CPoint LB_CP1, LB_CP2;         <span class="hljs-comment">// Clip Space Line</span><br><span class="hljs-type">float</span> m_LB_LineMatrix[<span class="hljs-number">3</span>][<span class="hljs-number">2</span>];<br><span class="hljs-comment">// General Polygon Space Model</span><br><span class="hljs-type">int</span> m_pointNumber;              <span class="hljs-comment">// Original Space Polygon point number</span><br><span class="hljs-type">int</span> m_clipPointNumber;          <span class="hljs-comment">// Clip Space Polygon point number</span><br>CPoint m_spPolygon[N];          <span class="hljs-comment">// Original Space Polygon</span><br>CPoint m_transPolygon[N];       <span class="hljs-comment">// Trans Space Polygon</span><br>CPoint m_clipPolygon[N];        <span class="hljs-comment">// Clip Space Polygon</span><br><span class="hljs-type">float</span> m_polygonMatrix[<span class="hljs-number">3</span>][<span class="hljs-number">2</span>];<br><span class="hljs-comment">// Weiler-Atherton Polygon Space Model</span><br><span class="hljs-type">int</span> m_WA_pointNumber;            <span class="hljs-comment">// Original Space Polygon point number</span><br><span class="hljs-type">int</span> m_WA_clipPointNumber;        <span class="hljs-comment">// Clip Space Polygon point number</span><br>CPoint m_WA_spPolygon[N];        <span class="hljs-comment">// Original Space Polygon</span><br>CPoint m_WA_transPolygon[N];     <span class="hljs-comment">// Trans Space Polygon</span><br>CPoint m_WA_clipPolygon[N];      <span class="hljs-comment">// Clip Space Polygon</span><br><span class="hljs-type">float</span> m_WA_polygonMatrix[<span class="hljs-number">3</span>][<span class="hljs-number">2</span>];<br><br>BOOL m_lineVisible, m_polygonVisible;<br>BOOL m_LB_lineVisible, m_WA_polygonVisible;<br><br><span class="hljs-type">int</span> m_wndLx, m_wndLy, m_wndRx, m_wndRy;     <span class="hljs-comment">// Space Window (Lx, Ly)-(Rx,Ry)</span><br><span class="hljs-type">int</span>  m_transDir, m_transMode, m_transSelect;<span class="hljs-comment">//0:-X  1:+X  2:-Y 3:+Y0:Translate  1:rotate  2:scale</span><br><br></code></pre></td></tr></table></figure><h2 id="线段变换裁剪">线段变换&amp;裁剪</h2><p>对于线段的变换，可以通过变换矩阵来实现。并且在实验中实现的是二维变换，所以在这里只需要使用3 x 3矩阵便可以。其内部的具体原理就不再解释了，不过通过矩阵运算确实可以得到各种变换结果。</p><h3 id="平移">平移</h3><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs c++">  |<span class="hljs-number">1</span>,<span class="hljs-number">0</span>,<span class="hljs-number">0</span> |<br>(x,y,<span class="hljs-number">1</span>)x | <span class="hljs-number">0</span>,<span class="hljs-number">1</span>,<span class="hljs-number">0</span> |= (x+Tx , y+Ty , <span class="hljs-number">1</span>)<br>              | Tx,Ty,<span class="hljs-number">1</span> |<br><span class="hljs-comment">// 通过这样的矩阵运算便可以实现基本的平移变换</span><br><span class="hljs-comment">// 不过从上面也可以看出，其运算可以进一步简化，将变换矩阵变为 3 x 2 的矩阵。</span><br></code></pre></td></tr></table></figure><h3 id="旋转">旋转</h3><p>对于旋转的变换矩阵，我们可以进行下面简单的推导。</p><p><img src="/2019/11/18/%E4%BA%8C%E7%BB%B4%E5%9B%BE%E5%83%8F%E5%8F%98%E6%8D%A2-%E8%A3%81%E5%89%AA/1.png"></p><p>对于一个顶点(x,y)，其距离顶点距离为R = sqrt( x^2 +y^2)，初始其和x轴形成的夹角为a，可以得到sin a = y / R , cos a = x /R。令其顺时针旋转角度θ，则旋转后角度为c = a - θ 。则变换后的顶点 y1 = R* sin c , x1 = R * cos c。利用三角公式展开，然后进一步化简可以得到变换后的顶点 y = x * cos b + y* sin b ，x = - x * sin b + y * cos b.</p><p>这样便可以得到我们的旋转变换矩阵。</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs c++">  |cos b,- sin b|<br>(x,y,<span class="hljs-number">1</span>)x | sin b,cos b|<br>             | <span class="hljs-number">0</span> ,<span class="hljs-number">0</span>|<br></code></pre></td></tr></table></figure><h3 id="缩放">缩放</h3><p>缩放变换的矩阵类似，我们可以直接给出~</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs c++">  |Sx,<span class="hljs-number">0</span>  |<br>(x,y,<span class="hljs-number">1</span>)x | <span class="hljs-number">0</span> ,Sy |<br>             | <span class="hljs-number">0</span> ,<span class="hljs-number">0</span>  |<br></code></pre></td></tr></table></figure><h3 id="复合变换">复合变换</h3><p>实现复合变换，就是各种基本操作矩阵的相乘~</p><h3 id="变换部分代码实现">变换部分代码实现</h3><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-function"><span class="hljs-type">void</span> <span class="hljs-title">CcgPHD2DTransView::ScaleMatrix</span><span class="hljs-params">(<span class="hljs-type">float</span> Sx, <span class="hljs-type">float</span> Sy, <span class="hljs-type">float</span> m[<span class="hljs-number">3</span>][<span class="hljs-number">2</span>])</span></span><br><span class="hljs-function"></span>&#123;<br><span class="hljs-keyword">for</span> (<span class="hljs-type">int</span> i = <span class="hljs-number">0</span>; i &lt; <span class="hljs-number">3</span>; i++) &#123;<br>m[i][<span class="hljs-number">0</span>] *= Sx;<br>m[i][<span class="hljs-number">1</span>] *= Sy;<br>&#125;<br>&#125;<br><br><span class="hljs-function"><span class="hljs-type">void</span> <span class="hljs-title">CcgPHD2DTransView::RotateMatrix</span><span class="hljs-params">(<span class="hljs-type">float</span> S, <span class="hljs-type">float</span> C, <span class="hljs-type">float</span> m[<span class="hljs-number">3</span>][<span class="hljs-number">2</span>])</span></span><br><span class="hljs-function"></span>&#123;<br><span class="hljs-type">float</span> temp;<br><span class="hljs-comment">// 小技巧，学习一下</span><br><span class="hljs-keyword">for</span> (<span class="hljs-type">int</span> i = <span class="hljs-number">0</span>; i &lt; <span class="hljs-number">3</span>; i++) &#123;<br>temp = m[i][<span class="hljs-number">0</span>];<br>m[i][<span class="hljs-number">0</span>] = temp * C - m[i][<span class="hljs-number">1</span>] * S;<br>m[i][<span class="hljs-number">1</span>] = temp * S + m[i][<span class="hljs-number">1</span>] * C;<br>&#125;<br>&#125;<br><br><span class="hljs-function"><span class="hljs-type">void</span> <span class="hljs-title">CcgPHD2DTransView::TranslateMatrix</span><span class="hljs-params">(<span class="hljs-type">float</span> Tx, <span class="hljs-type">float</span> Ty, <span class="hljs-type">float</span> m[<span class="hljs-number">3</span>][<span class="hljs-number">2</span>])</span></span><br><span class="hljs-function"></span>&#123;<br><span class="hljs-comment">// 矩阵平移变换的位移量</span><br>m[<span class="hljs-number">2</span>][<span class="hljs-number">0</span>] += Tx;<br>m[<span class="hljs-number">2</span>][<span class="hljs-number">1</span>] += Ty;<br>&#125;<br><br><span class="hljs-function"><span class="hljs-type">void</span> <span class="hljs-title">CcgPHD2DTransView::TransLine</span><span class="hljs-params">(CPoint p1, CPoint p2, CPoint *tp1, CPoint *tp2, <span class="hljs-type">float</span> transMatrix[<span class="hljs-number">3</span>][<span class="hljs-number">2</span>])</span></span><br><span class="hljs-function"></span>&#123;<br><span class="hljs-comment">// 更改移动后的线段端点坐标,矩阵乘法(x,y,1) * transMatrix[3][2]</span><br>tp1-&gt;x = p1.x * transMatrix[<span class="hljs-number">0</span>][<span class="hljs-number">0</span>] + p1.y * transMatrix[<span class="hljs-number">1</span>][<span class="hljs-number">0</span>] + transMatrix[<span class="hljs-number">2</span>][<span class="hljs-number">0</span>];<br>tp1-&gt;y = p1.x * transMatrix[<span class="hljs-number">0</span>][<span class="hljs-number">1</span>] + p1.y * transMatrix[<span class="hljs-number">1</span>][<span class="hljs-number">1</span>] + transMatrix[<span class="hljs-number">2</span>][<span class="hljs-number">1</span>];<br>tp2-&gt;x = p2.x * transMatrix[<span class="hljs-number">0</span>][<span class="hljs-number">0</span>] + p2.y * transMatrix[<span class="hljs-number">1</span>][<span class="hljs-number">0</span>] + transMatrix[<span class="hljs-number">2</span>][<span class="hljs-number">0</span>];<br>tp2-&gt;y = p2.x * transMatrix[<span class="hljs-number">0</span>][<span class="hljs-number">1</span>] + p2.y * transMatrix[<span class="hljs-number">1</span>][<span class="hljs-number">1</span>] + transMatrix[<span class="hljs-number">2</span>][<span class="hljs-number">1</span>];<br><br>&#125;<br><br><span class="hljs-comment">// 下面给出线段的显示函数</span><br><span class="hljs-function"><span class="hljs-type">void</span> <span class="hljs-title">CcgPHD2DTransView::DisplayLine</span><span class="hljs-params">(CDC* pDC, CPoint p1, CPoint p2, COLORREF rgbColor)</span></span><br><span class="hljs-function"></span>&#123;<br>CcgPHD2DTransDoc* pDoc = <span class="hljs-built_in">GetDocument</span>();<br><br>CPen newPen;<br>CPen *oldPen;<br>CPoint VP1, VP2;<br><br>newPen.<span class="hljs-built_in">CreatePen</span>(PS_SOLID, <span class="hljs-number">1</span>, rgbColor);<br>oldPen = (CPen *)pDC-&gt;<span class="hljs-built_in">SelectObject</span>(&amp;newPen);<br><span class="hljs-comment">// MFC坐标原点处理</span><br>VP1.x = pDoc-&gt;m_width / <span class="hljs-number">2</span> + p1.x;<br>VP1.y = pDoc-&gt;m_height / <span class="hljs-number">2</span> - p1.y;<br>VP2.x = pDoc-&gt;m_width / <span class="hljs-number">2</span> + p2.x;<br>VP2.y = pDoc-&gt;m_height / <span class="hljs-number">2</span> - p2.y;<br><br>pDC-&gt;<span class="hljs-built_in">MoveTo</span>(VP1);<br>pDC-&gt;<span class="hljs-built_in">LineTo</span>(VP2);<br><br>pDC-&gt;<span class="hljs-built_in">SelectObject</span>(oldPen);<br>newPen.<span class="hljs-built_in">DeleteObject</span>();<br>&#125;<br></code></pre></td></tr></table></figure><h3 id="cohen-sutherland裁剪实现">Cohen-Sutherland裁剪实现</h3><p>首先我们对窗口进行一下分割，分割为9个部分。然后给一个区域进行编码，这样直接对线段的两个端点的区域码进行判断，可以得到该条线段的取舍情况。</p><p><img src="/2019/11/18/%E4%BA%8C%E7%BB%B4%E5%9B%BE%E5%83%8F%E5%8F%98%E6%8D%A2-%E8%A3%81%E5%89%AA/2.png"></p><p>对端点进行区域码的确定之后，对端点的区域码进行运算判断：</p><ul><li>两端点全为0000时，完全位于窗口内；</li><li>两端点编码逐位逻辑与不为0，说明线段的两个端点位于窗口外同侧，即完全位于窗口外；</li><li>两端点编码逐位逻辑与为0时，说明此线段部分可见，或者完全不可见。</li></ul><p>在以上基础上，我们可以实现该算法。算法思路如下：</p><ul><li>从裁剪窗口的四条边界出发，依次去对被裁剪线段进行处理；</li><li>首先判断该线段是否可见，实现思路如上；</li><li>如果完全可见或完全不可见，则直接return；</li><li>对于部分可见或完全不可见的线段，用当前裁剪窗口的边界对该线段进行裁剪，具体如下：<ul><li>首先看是否穿过该边界；</li><li><strong>如果穿过，则进行裁剪，向窗口内部"靠近"</strong>;</li><li>然后继续第一步循环，判断是否可见，或是继续裁剪，直到可以判断出完全可见或完全不可见。</li></ul></li></ul><p>实现代码如下~</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-comment">// Cohn-Sutherland Subdivision Line Clip</span><br><span class="hljs-function"><span class="hljs-type">int</span>  <span class="hljs-title">CcgPHD2DTransView::ClipLine</span><span class="hljs-params">(<span class="hljs-type">int</span> *x1, <span class="hljs-type">int</span> *y1, <span class="hljs-type">int</span> *x2, <span class="hljs-type">int</span> *y2)</span></span><br><span class="hljs-function"></span>&#123;<br><span class="hljs-type">int</span> visible, m_window[<span class="hljs-number">4</span>];<br>CcgPHD2DTransDoc* pDoc = <span class="hljs-built_in">GetDocument</span>();<br><br>m_window[<span class="hljs-number">0</span>] = pDoc-&gt;m_wndLx;    m_window[<span class="hljs-number">1</span>] = pDoc-&gt;m_wndRx;<br>m_window[<span class="hljs-number">2</span>] = pDoc-&gt;m_wndRy;    m_window[<span class="hljs-number">3</span>] = pDoc-&gt;m_wndLy;<br><span class="hljs-keyword">for</span> (<span class="hljs-type">int</span> i = <span class="hljs-number">0</span>; i &lt; <span class="hljs-number">4</span>; i++) &#123; <span class="hljs-comment">// Along the WIN Border</span><br>visible = <span class="hljs-built_in">LineVisible</span>(x1, y1, x2, y2);<br><span class="hljs-keyword">if</span> (visible == <span class="hljs-number">1</span>) <span class="hljs-keyword">return</span> <span class="hljs-number">1</span>;         <span class="hljs-comment">// Total Visible</span><br><span class="hljs-keyword">if</span> (visible == <span class="hljs-number">0</span>) <span class="hljs-keyword">return</span> <span class="hljs-number">0</span>;         <span class="hljs-comment">// Total Unvisible</span><br><span class="hljs-comment">// 部分可见，且第一个点在框外</span><br><span class="hljs-keyword">if</span> (<span class="hljs-built_in">LineCross</span>(*x1, *y1, *x2, *y2, i)) &#123;<span class="hljs-comment">// 判断是否经过该边界</span><br><br><span class="hljs-keyword">if</span> (i &lt; <span class="hljs-number">2</span> &amp;&amp; *x2 - *x1) &#123;                       <span class="hljs-comment">// Left , Right</span><br><span class="hljs-type">float</span> m = (<span class="hljs-type">float</span>)(*y2 - *y1) / (*x2 - *x1);<br><span class="hljs-type">float</span> iy = m * (m_window[i] - *x1) + *y1;<span class="hljs-comment">// 计算出边界端点</span><br><span class="hljs-comment">// 根据端点大小，来进行端点的更新，舍弃window框之外的部分</span><br><span class="hljs-keyword">if</span> (i == <span class="hljs-number">0</span>) &#123;<br><span class="hljs-keyword">if</span> (*x1 &lt; *x2) &#123;<br>*x1 = m_window[i];<br>*y1 = iy;<br>&#125;<br><span class="hljs-keyword">else</span> &#123;<br>*x2 = m_window[i];<br>*y2 = iy;<br>&#125;<br>&#125;<br><span class="hljs-keyword">else</span> &#123;<br><span class="hljs-keyword">if</span> (*x1 &gt; *x2) &#123;<br>*x1 = m_window[i];<br>*y1 = iy;<br>&#125;<br><span class="hljs-keyword">else</span> &#123;<br>*x2 = m_window[i];<br>*y2 = iy;<br>&#125;<br>&#125;<br>&#125;<br><span class="hljs-keyword">else</span> <span class="hljs-keyword">if</span> (*y2 - *y1) &#123;                         <span class="hljs-comment">// Top    Bottom</span><br><span class="hljs-type">float</span> m = (<span class="hljs-type">float</span>)(*x2 - *x1) / (*y2 - *y1);<br><span class="hljs-type">float</span> ix = m * (m_window[i] - *y1) + *x1;<br><span class="hljs-keyword">if</span> (i == <span class="hljs-number">2</span>) &#123;<br><span class="hljs-keyword">if</span> (*y1 &gt; *y2) &#123;<br>*x1 = ix;<br>*y1 = m_window[i];<br>&#125;<br><span class="hljs-keyword">else</span> &#123;<br>*x2 = ix;<br>*y2 = m_window[i];<br>&#125;<br>&#125;<br><span class="hljs-keyword">else</span> &#123;<br><span class="hljs-keyword">if</span> (*y1 &lt; *y2) &#123;<br>*x1 = ix;<br>*y1 = m_window[i];<br>&#125;<br><span class="hljs-keyword">else</span> &#123;<br>*x2 = ix;<br>*y2 = m_window[i];<br>&#125;<br>&#125;<br>&#125;<br>&#125;<br>&#125;<br><span class="hljs-keyword">return</span> <span class="hljs-number">1</span>;<br>&#125;<br><br><span class="hljs-function"><span class="hljs-type">int</span> <span class="hljs-title">CcgPHD2DTransView::LineVisible</span><span class="hljs-params">(<span class="hljs-type">int</span> *x1, <span class="hljs-type">int</span> *y1, <span class="hljs-type">int</span> *x2, <span class="hljs-type">int</span> *y2)</span></span><br><span class="hljs-function"></span>&#123;<br><span class="hljs-type">int</span> pcode1, pcode2;<br><span class="hljs-comment">// 为0表示位于框内部</span><br><span class="hljs-comment">//| 0101 | 0100 | 0110 |</span><br><span class="hljs-comment">//| 0001 | 0000 | 0010 |</span><br><span class="hljs-comment">//| 1001 | 1000 | 1010 |</span><br>pcode1 = <span class="hljs-built_in">pCode</span>(x1, y1);<br>pcode2 = <span class="hljs-built_in">pCode</span>(x2, y2);<br><br><span class="hljs-keyword">if</span> (!pcode1 &amp;&amp; !pcode2)    <span class="hljs-keyword">return</span> <span class="hljs-number">1</span>;     <span class="hljs-comment">// 均可见</span><br><span class="hljs-keyword">if</span> ((pcode1&amp;pcode2) != <span class="hljs-number">0</span>)  <span class="hljs-keyword">return</span> <span class="hljs-number">0</span>;     <span class="hljs-comment">// 两个端点位于同一侧，均不可见</span><br><span class="hljs-keyword">if</span> (pcode1 == <span class="hljs-number">0</span>) &#123;<span class="hljs-comment">//部分可见，如果第一个点位于框内，进行交换</span><br><span class="hljs-type">float</span> temp;<br>temp = *x1;  *x1 = *x2;  *x2 = temp;<br>temp = *y1;  *y1 = *y2;  *y2 = temp;<br>&#125;<br><span class="hljs-keyword">return</span> <span class="hljs-number">2</span>;<br>&#125;<br><br><span class="hljs-function"><span class="hljs-type">int</span> <span class="hljs-title">CcgPHD2DTransView::pCode</span><span class="hljs-params">(<span class="hljs-type">int</span> *x, <span class="hljs-type">int</span> *y)</span></span><br><span class="hljs-function"></span>&#123;<br><span class="hljs-type">int</span> code = <span class="hljs-number">0</span>;<br>CcgPHD2DTransDoc* pDoc = <span class="hljs-built_in">GetDocument</span>();<br><br><span class="hljs-keyword">if</span> (*x &lt;= pDoc-&gt;m_wndLx)  code |= <span class="hljs-number">1</span>;<br><span class="hljs-keyword">if</span> (*x &gt;= pDoc-&gt;m_wndRx)  code |= <span class="hljs-number">2</span>;<br><span class="hljs-keyword">if</span> (*y &gt;= pDoc-&gt;m_wndRy)  code |= <span class="hljs-number">4</span>;<br><span class="hljs-keyword">if</span> (*y &lt;= pDoc-&gt;m_wndLy)  code |= <span class="hljs-number">8</span>;<br><br><span class="hljs-keyword">return</span> code;<br>&#125;<br><br><span class="hljs-function"><span class="hljs-type">int</span> <span class="hljs-title">CcgPHD2DTransView::LineCross</span><span class="hljs-params">(<span class="hljs-type">int</span> x1, <span class="hljs-type">int</span> y1, <span class="hljs-type">int</span> x2, <span class="hljs-type">int</span> y2, <span class="hljs-type">int</span> i)</span></span><br><span class="hljs-function"></span>&#123;<br><span class="hljs-type">int</span> visible1, visible2;<br><br>visible1 = <span class="hljs-built_in">pVisible</span>(x1, y1, i);<br>visible2 = <span class="hljs-built_in">pVisible</span>(x2, y2, i);<br><br><span class="hljs-keyword">if</span> (visible1 != visible2) <span class="hljs-keyword">return</span> <span class="hljs-number">1</span>;<br><span class="hljs-keyword">else</span>                      <span class="hljs-keyword">return</span> <span class="hljs-number">0</span>;<br><br>&#125;<br><br><span class="hljs-function"><span class="hljs-type">int</span> <span class="hljs-title">CcgPHD2DTransView::pVisible</span><span class="hljs-params">(<span class="hljs-type">int</span> x, <span class="hljs-type">int</span> y, <span class="hljs-type">int</span> i)</span></span><br><span class="hljs-function"></span>&#123;<br><span class="hljs-type">int</span> visible = <span class="hljs-number">0</span>;<br>CcgPHD2DTransDoc* pDoc = <span class="hljs-built_in">GetDocument</span>();<br><br><span class="hljs-keyword">switch</span> (i) &#123;<br><span class="hljs-keyword">case</span> <span class="hljs-number">0</span>: <span class="hljs-comment">// Left</span><br><span class="hljs-keyword">if</span> (x &gt;= pDoc-&gt;m_wndLx)  visible = <span class="hljs-number">1</span>; <span class="hljs-keyword">break</span>;<br><span class="hljs-keyword">case</span> <span class="hljs-number">1</span>: <span class="hljs-comment">// Right</span><br><span class="hljs-keyword">if</span> (x &lt;= pDoc-&gt;m_wndRx)  visible = <span class="hljs-number">1</span>; <span class="hljs-keyword">break</span>;<br><span class="hljs-keyword">case</span> <span class="hljs-number">2</span>: <span class="hljs-comment">// Top</span><br><span class="hljs-keyword">if</span> (y &lt;= pDoc-&gt;m_wndRy)  visible = <span class="hljs-number">1</span>; <span class="hljs-keyword">break</span>;<br><span class="hljs-keyword">case</span> <span class="hljs-number">3</span>: <span class="hljs-comment">// Bottom</span><br><span class="hljs-keyword">if</span> (y &gt;= pDoc-&gt;m_wndLy)  visible = <span class="hljs-number">1</span>; <span class="hljs-keyword">break</span>;<br>&#125;<br><span class="hljs-keyword">return</span> visible;<br>&#125;<br></code></pre></td></tr></table></figure><h3 id="liang-barsky-裁剪实现">Liang-Barsky 裁剪实现</h3><p>其实自己并没有十分理解其实现原理，不过并不妨碍对该裁剪算法的实现~</p><p><img src="/2019/11/18/%E4%BA%8C%E7%BB%B4%E5%9B%BE%E5%83%8F%E5%8F%98%E6%8D%A2-%E8%A3%81%E5%89%AA/3.png"></p><p>对于任意线段，进行延申，与四个裁剪窗口分别相交。我们将左边和下边视为入边，右边和上边为出边。</p><p>在这里我们只讨论斜率k &gt;0的情况(与x或y轴直接平行的情况比较简单，直接拿出来单独处理就好啦，看代码吧)，然后k&lt; 0的情况类似~</p><p>首先我们得到两个值： L = max( min(x_a, x_b), x_min ) R= min( max(x_a,x_b), x_max )</p><p>在斜率大于零的情况下，线段的延长线与边界的交点中 max(u_b, u_l) =u_l；min(u_t, u_r) = u_t。在此基础上，如果满足一下条件，则说明有可见部分，有一条不满足，就没有可见部分；</p><p>① L &lt;= R</p><p>② L &lt;= x_u</p><p>③ x_t &lt;= R</p><p>如果存在可见部分，对端点直接裁剪即可，直接看代码就好啦~</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-comment">// Liang-Barsky Line Clip</span><br><span class="hljs-function"><span class="hljs-type">int</span> <span class="hljs-title">CcgPHD2DTransView::LB_ClipLine</span><span class="hljs-params">(<span class="hljs-type">int</span> * x1, <span class="hljs-type">int</span> * y1, <span class="hljs-type">int</span> * x2, <span class="hljs-type">int</span> * y2)</span></span><br><span class="hljs-function"></span>&#123;<br>CcgPHD2DTransDoc* pDoc = <span class="hljs-built_in">GetDocument</span>();<br><br><span class="hljs-type">float</span> k, L, R;<span class="hljs-comment">//斜率 左侧 右侧</span><br><span class="hljs-type">float</span> xu, xt, temp;<br><span class="hljs-type">float</span> dx = *x1 - *x2, dy = *y1 - *y2;<br>k = dy / dx;<br><span class="hljs-keyword">if</span> (dy == <span class="hljs-number">0</span>) &#123;<span class="hljs-comment">// 水平</span><br><span class="hljs-keyword">if</span> (*x1 &gt; *x2) &#123;<span class="hljs-comment">// 交换一下</span><br><span class="hljs-type">int</span> temp = *x2;*x2 = *x1;*x1 = temp;<br>&#125;<br><span class="hljs-keyword">if</span> (*y1 &gt;= pDoc-&gt;m_wndRy || *y1 &lt;= pDoc-&gt;m_wndLy)<span class="hljs-comment">//超出范围</span><br><span class="hljs-keyword">return</span> <span class="hljs-number">0</span>;<br><span class="hljs-keyword">if</span> (*x1 &lt;= pDoc-&gt;m_wndLx)<br>*x1 = pDoc-&gt;m_wndLx;<br><span class="hljs-keyword">if</span> (*x2 &gt;= pDoc-&gt;m_wndRx)<br>*x2 = pDoc-&gt;m_wndRx;<br>&#125;<br><span class="hljs-keyword">else</span> <span class="hljs-keyword">if</span> (dx == <span class="hljs-number">0</span>) &#123;<br><span class="hljs-keyword">if</span> (*y1 &gt; *y2) &#123;<span class="hljs-comment">// 交换一下</span><br><span class="hljs-type">int</span> temp = *y2;*y2 = *y1;*y1 = temp;<br>&#125;<br><span class="hljs-keyword">if</span> (*x1 &gt;= pDoc-&gt;m_wndRy || *x1 &lt;= pDoc-&gt;m_wndLy)<span class="hljs-comment">//超出范围</span><br><span class="hljs-keyword">return</span> <span class="hljs-number">0</span>;<br><span class="hljs-keyword">if</span> (*y1 &lt;= pDoc-&gt;m_wndLx)<br>*y1 = pDoc-&gt;m_wndLx;<br><span class="hljs-keyword">if</span> (*y2 &gt;= pDoc-&gt;m_wndRx)<br>*y2 = pDoc-&gt;m_wndRx;<br>&#125;<br><span class="hljs-keyword">else</span> &#123;<br>L = <span class="hljs-built_in">max</span>(pDoc-&gt;m_wndLx, <span class="hljs-built_in">min</span>(*x1, *x2));<br>R = <span class="hljs-built_in">min</span>(pDoc-&gt;m_wndRx, <span class="hljs-built_in">max</span>(*x1, *x2));<br>xu = dx * (pDoc-&gt;m_wndRy - *y1) / dy + *x1;<br>xt = dx * (pDoc-&gt;m_wndLy - *y1) / dy + *x1;<br><br><span class="hljs-keyword">if</span> (L &gt; R)<span class="hljs-comment">// 无可见部分，直接退出</span><br><span class="hljs-keyword">return</span> <span class="hljs-number">0</span>;<br><span class="hljs-keyword">if</span> (k &gt; <span class="hljs-number">0</span>) &#123;<br><span class="hljs-keyword">if</span> (L &lt;= xu &amp;&amp; xt &lt;= R) &#123;<br>temp = <span class="hljs-built_in">max</span>(L, xt);<br>*y1 = dy * (temp - *x1) / dx + *y1;<br>*x1 = temp;<br><br>temp = <span class="hljs-built_in">min</span>(R, xu);<br>*y2 = dy * (temp - *x1) / dx + *y1;<br>*x2 = temp;<br>&#125;<br><span class="hljs-keyword">else</span><br><span class="hljs-keyword">return</span> <span class="hljs-number">0</span>;<br>&#125;<br><span class="hljs-keyword">else</span> <span class="hljs-keyword">if</span> (k &lt; <span class="hljs-number">0</span>) &#123;<br><span class="hljs-keyword">if</span> (L &lt;= xt &amp;&amp; xu &lt;= R) &#123;<br>temp = <span class="hljs-built_in">max</span>(L, xu);<br>*y1 = dy * (temp - *x1) / dx + *y1;<br>*x1 = temp;<br><br>temp = <span class="hljs-built_in">min</span>(R, xt);<br>*y2 = dy * (temp - *x1) / dx + *y1;<br>*x2 = temp;<br>&#125;<br><span class="hljs-keyword">else</span><br><span class="hljs-keyword">return</span> <span class="hljs-number">0</span>;<br>&#125;<br>&#125;<br><span class="hljs-keyword">return</span> <span class="hljs-number">1</span>;<br>&#125;<br></code></pre></td></tr></table></figure><h2 id="多边形变换裁剪">多边形变换&amp;裁剪</h2><p>多边形也是一条一条线段边所组成的，不过在处理过程中，确实也麻烦了很多。</p><h3 id="多边形变换">多边形变换</h3><p>其实和线段的都一样，，，没啥好些的，变换矩阵和上面讲的一样，直接贴代码吧~</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-function"><span class="hljs-type">void</span> <span class="hljs-title">CcgPHD2DTransView::TransPolygon</span><span class="hljs-params">(<span class="hljs-type">int</span> pointNumber, CPoint spPolygon[N], CPoint transPolygon[N], <span class="hljs-type">float</span> transMatrix[<span class="hljs-number">3</span>][<span class="hljs-number">2</span>])</span></span><br><span class="hljs-function"></span>&#123;<br>CcgPHD2DTransDoc* pDoc = <span class="hljs-built_in">GetDocument</span>();<br><br><span class="hljs-keyword">for</span> (<span class="hljs-type">int</span> i = <span class="hljs-number">0</span>; i &lt; pointNumber; i++) &#123;<br>transPolygon[i].x = <br>spPolygon[i].x * transMatrix[<span class="hljs-number">0</span>][<span class="hljs-number">0</span>] + spPolygon[i].y * transMatrix[<span class="hljs-number">1</span>][<span class="hljs-number">0</span>] + transMatrix[<span class="hljs-number">2</span>][<span class="hljs-number">0</span>];<br>transPolygon[i].y = <br>spPolygon[i].x * transMatrix[<span class="hljs-number">0</span>][<span class="hljs-number">1</span>] + spPolygon[i].y * transMatrix[<span class="hljs-number">1</span>][<span class="hljs-number">1</span>] + transMatrix[<span class="hljs-number">2</span>][<span class="hljs-number">1</span>];<br><br>&#125;<br><br>&#125;<br><br><span class="hljs-function"><span class="hljs-type">void</span> <span class="hljs-title">CcgPHD2DTransView::DisplayPolygon</span><span class="hljs-params">(CDC* pDC, <span class="hljs-type">int</span> pointNumber, CPoint transPolygon[N], COLORREF rgbColor)</span></span><br><span class="hljs-function"></span>&#123;<br>CcgPHD2DTransDoc* pDoc = <span class="hljs-built_in">GetDocument</span>();<br><br>CPen newPen;<br>CPen *oldPen;<br>CPoint VPolygon[N];<br><br>newPen.<span class="hljs-built_in">CreatePen</span>(PS_SOLID, <span class="hljs-number">1</span>, rgbColor);<br>oldPen = (CPen *)pDC-&gt;<span class="hljs-built_in">SelectObject</span>(&amp;newPen);<br><br><span class="hljs-keyword">for</span> (<span class="hljs-type">int</span> i = <span class="hljs-number">0</span>; i &lt; pointNumber; i++) &#123;<br>VPolygon[i].x = pDoc-&gt;m_width / <span class="hljs-number">2</span> + transPolygon[i].x;<br>VPolygon[i].y = pDoc-&gt;m_height / <span class="hljs-number">2</span> - transPolygon[i].y;<br>&#125;<br><br>pDC-&gt;<span class="hljs-built_in">MoveTo</span>(VPolygon[<span class="hljs-number">0</span>]);<br><span class="hljs-keyword">for</span> (<span class="hljs-type">int</span> i = <span class="hljs-number">1</span>; i &lt; pointNumber; i++) <br>pDC-&gt;<span class="hljs-built_in">LineTo</span>(VPolygon[i]);<br><br>pDC-&gt;<span class="hljs-built_in">SelectObject</span>(oldPen);<br>newPen.<span class="hljs-built_in">DeleteObject</span>();<br>&#125;<br></code></pre></td></tr></table></figure><h3 id="sutherland-hodgman-多边形裁剪实现">Sutherland-Hodgman多边形裁剪实现</h3><p>实现思路很清晰，直接写思路吧，感觉有大一部分思路是和线段裁剪相似的。</p><ul><li>顺序存储多边形的顶点；</li><li>依次用每一条裁剪边对输入的顶点序列进行如下处理：<ul><li>若当前顶点可见（在裁剪框内部），则<strong>将该顶点加入到输出顶点序列</strong>中（output()）；</li><li>对当前顶点和下一顶点构成的线段进行裁剪判断；</li><li>如果这条线段边和裁剪边界有交点，则<strong>将该交点加入到输出序列</strong>中；</li></ul></li><li>最后进行封闭检查，使得输出序列中的顶点信息的头和尾是相同的。</li></ul><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-comment">// Sutherland-Hodgman Polygon Clip</span><br><span class="hljs-function"><span class="hljs-type">int</span> <span class="hljs-title">CcgPHD2DTransView::ClipPolygon</span><span class="hljs-params">(<span class="hljs-type">int</span> n, CPoint *tPoints, <span class="hljs-type">int</span> *cn, CPoint *cPoints)</span></span><br><span class="hljs-function"></span>&#123;<br><span class="hljs-type">int</span> Nin, Nout, ix, iy, Sx, Sy;<br>CcgPHD2DTransDoc* pDoc = <span class="hljs-built_in">GetDocument</span>();<br><br>Nin = n;<br><span class="hljs-keyword">for</span> (<span class="hljs-type">int</span> i = <span class="hljs-number">0</span>; i &lt; <span class="hljs-number">4</span>; i++) &#123;  <span class="hljs-comment">// Along the window border</span><br>*cn = <span class="hljs-number">0</span>;<br><span class="hljs-keyword">for</span> (<span class="hljs-type">int</span> j = <span class="hljs-number">0</span>; j &lt; Nin; j++) &#123;  <span class="hljs-comment">// Scan polygon every point and line.</span><br><span class="hljs-keyword">if</span> (j &gt; <span class="hljs-number">0</span>) &#123;<br><span class="hljs-comment">// 如果存在边穿过边界，则更新顶点</span><br><span class="hljs-keyword">if</span> (<span class="hljs-built_in">LineCross</span>(Sx, Sy, tPoints[j].x, tPoints[j].y, i)) &#123;<br><span class="hljs-built_in">interSect</span>(Sx, Sy, tPoints[j].x, tPoints[j].y, i, &amp;ix, &amp;iy);<br><span class="hljs-built_in">outPut</span>(ix, iy, cn, cPoints);<br>&#125;<br>&#125;<br>            <span class="hljs-comment">//当前顶点可见</span><br>Sx = tPoints[j].x;<br>Sy = tPoints[j].y;<br><span class="hljs-keyword">if</span> (<span class="hljs-built_in">pVisible</span>(Sx, Sy, i)) <br><span class="hljs-built_in">outPut</span>(Sx, Sy, cn, cPoints);<br>&#125;<br>    <br>Nin = *cn;<br><span class="hljs-keyword">if</span> (*cn == <span class="hljs-number">0</span>) <span class="hljs-keyword">return</span> <span class="hljs-number">0</span>;<span class="hljs-comment">// 均不可见</span><br><span class="hljs-keyword">for</span> (<span class="hljs-type">int</span> j = <span class="hljs-number">0</span>; j &lt; Nin; j++) &#123;<br>tPoints[j].x = cPoints[j].x;<br>tPoints[j].y = cPoints[j].y;<br>&#125;<br><br><span class="hljs-keyword">if</span> (cPoints[<span class="hljs-number">0</span>].x != cPoints[Nin - <span class="hljs-number">1</span>].x ||<br>cPoints[<span class="hljs-number">0</span>].y != cPoints[Nin - <span class="hljs-number">1</span>].y) &#123;<br><br>tPoints[Nin].x = cPoints[Nin].x = cPoints[<span class="hljs-number">0</span>].x;<br>tPoints[Nin].y = cPoints[Nin].y = cPoints[<span class="hljs-number">0</span>].y;<br><br>Nin++;<br>*cn = Nin;<br>&#125;<br>&#125;<br><span class="hljs-keyword">return</span> <span class="hljs-number">1</span>;<br>&#125;<br><br><span class="hljs-function"><span class="hljs-type">void</span> <span class="hljs-title">CcgPHD2DTransView::interSect</span><span class="hljs-params">(<span class="hljs-type">int</span> Sx, <span class="hljs-type">int</span>  Sy, <span class="hljs-type">int</span> Px, <span class="hljs-type">int</span> Py, <span class="hljs-type">int</span>  i, <span class="hljs-type">int</span> *ix, <span class="hljs-type">int</span> *iy)</span></span><br><span class="hljs-function"></span>&#123;<br>CcgPHD2DTransDoc* pDoc = <span class="hljs-built_in">GetDocument</span>();<br><span class="hljs-comment">// 这里因为整数除法，出了好多问题，捣鼓了好久才发现，，emmm</span><br><span class="hljs-keyword">switch</span> (i) &#123;<br><span class="hljs-keyword">case</span> <span class="hljs-number">0</span>: <span class="hljs-comment">// Left</span><br>*ix = pDoc-&gt;m_wndLx;<br>*iy = (Sy - Py) * (pDoc-&gt;m_wndLx - Px) / (Sx - Px) + Py;<br><span class="hljs-keyword">break</span>;<br><span class="hljs-keyword">case</span> <span class="hljs-number">1</span>: <span class="hljs-comment">// Right</span><br>*ix = pDoc-&gt;m_wndRx;<br>*iy = (Sy - Py) * (pDoc-&gt;m_wndRx - Px) / (Sx - Px) + Py;<br><span class="hljs-keyword">break</span>;<br><span class="hljs-keyword">case</span> <span class="hljs-number">2</span>: <span class="hljs-comment">// Top</span><br>*iy = pDoc-&gt;m_wndRy;<br>*ix = (Sx - Px) * (pDoc-&gt;m_wndRy - Py) / (Sy - Py) + Px;<br><span class="hljs-keyword">break</span>;<br><span class="hljs-keyword">case</span> <span class="hljs-number">3</span>: <span class="hljs-comment">// Bottom</span><br>*iy = pDoc-&gt;m_wndLy;<br>*ix = (Sx - Px) * (pDoc-&gt;m_wndLy - Py) / (Sy - Py) + Px;<br><span class="hljs-keyword">break</span>;<br>&#125;<br>&#125;<br><br><span class="hljs-function"><span class="hljs-type">void</span> <span class="hljs-title">CcgPHD2DTransView::outPut</span><span class="hljs-params">(<span class="hljs-type">int</span> x, <span class="hljs-type">int</span> y, <span class="hljs-type">int</span> *cn, CPoint *cPoints)</span></span><br><span class="hljs-function"></span>&#123;<br>cPoints[*cn].x = x;<br>cPoints[*cn].y = y;<br>(*cn)++;<br>&#125;<br></code></pre></td></tr></table></figure>]]></content>
    
    
    <categories>
      
      <category>study</category>
      
    </categories>
    
    
    <tags>
      
      <tag>MFC</tag>
      
      <tag>c++</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>CNN-简单图片分类</title>
    <link href="/2019/11/17/CNN-%E7%AE%80%E5%8D%95%E5%9B%BE%E7%89%87%E5%88%86%E7%B1%BB/"/>
    <url>/2019/11/17/CNN-%E7%AE%80%E5%8D%95%E5%9B%BE%E7%89%87%E5%88%86%E7%B1%BB/</url>
    
    <content type="html"><![CDATA[<p>假期的时候跟着专知的一个深度学习课程学习了一些深度学习的内容，也是愈发觉得神经网络十分神奇，最近看了一份简单的图片分类的CNN网络，记录学习一下，从简单学起~</p><span id="more"></span><p>大部分神经网络的基础就不再写了，网上也有很多介绍，这里就照着代码，顺一遍基本的使用方法~</p><h2 id="简略介绍">简略介绍</h2><p>训练样本50张，分为3类：</p><ul><li>0 =&gt; 飞机</li><li>1 =&gt; 汽车</li><li>2 =&gt; 鸟图片都放在data文件夹中，按照label_id.jpg进行命名，例如2_111.jpg代表图片类别为2（鸟），id为111。</li></ul><h2 id="导入相应库">导入相应库</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment">#coding=utf-8</span><br><br><span class="hljs-keyword">import</span> os<br><span class="hljs-comment">#图像读取库</span><br><span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image<br><span class="hljs-comment">#矩阵运算库</span><br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf<br></code></pre></td></tr></table></figure><h2 id="读取数据">读取数据</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 数据文件夹</span><br>data_dir = <span class="hljs-string">&quot;data&quot;</span><br>test_data_dir = <span class="hljs-string">&quot;test_data&quot;</span><br><span class="hljs-comment"># 训练还是测试,true为训练</span><br>train = <span class="hljs-literal">False</span><br><span class="hljs-comment"># 模型文件路径</span><br>model_path = <span class="hljs-string">&quot;model/image_model&quot;</span><br><br><br><span class="hljs-comment"># 从文件夹读取图片和标签到numpy数组中</span><br><span class="hljs-comment"># 标签信息在文件名中，例如1_40.jpg表示该图片的标签为1</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">read_data</span>(<span class="hljs-params">data_dir,test_data_dir</span>):<br>    datas = []<br>    test_datas = []<br>    labels = []<br>    test_labels = []<br>    fpaths = []<br>    test_fpaths = []<br>    <span class="hljs-comment"># 读入图片信息，并且从图片名中得到类别</span><br>    <span class="hljs-keyword">for</span> fname <span class="hljs-keyword">in</span> os.listdir(data_dir):<br>        fpath = os.path.join(data_dir, fname)<br>        fpaths.append(fpath)    <span class="hljs-comment"># 向尾部添加</span><br>        image = Image.<span class="hljs-built_in">open</span>(fpath)<br>        data = np.array(image) / <span class="hljs-number">255.0</span><br>        label = <span class="hljs-built_in">int</span>(fname.split(<span class="hljs-string">&quot;_&quot;</span>)[<span class="hljs-number">0</span>])<br>        datas.append(data)<br>        labels.append(label)<br><br>    <span class="hljs-keyword">for</span> fname <span class="hljs-keyword">in</span> os.listdir(test_data_dir):<br>        fpath = os.path.join(test_data_dir, fname)<br>        test_fpaths.append(fpath)  <span class="hljs-comment"># 向尾部添加</span><br>        image = Image.<span class="hljs-built_in">open</span>(fpath)<br>        data = np.array(image) / <span class="hljs-number">255.0</span><br>        label = <span class="hljs-built_in">int</span>(fname.split(<span class="hljs-string">&quot;_&quot;</span>)[<span class="hljs-number">0</span>])<br>        test_datas.append(data)<br>        test_labels.append(label)<br><br>    datas = np.array(datas)<br>    test_datas = np.array(test_datas)<br>    labels = np.array(labels)<br>    test_labels = np.array(test_labels)<br>    <span class="hljs-comment"># 输出矩阵维度</span><br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;shape of datas: &#123;&#125;\tshape of labels: &#123;&#125;&quot;</span>.<span class="hljs-built_in">format</span>(datas.shape, labels.shape))<br>    <span class="hljs-keyword">return</span> fpaths, test_fpaths, datas, labels, test_labels, test_datas<br><br><br>fpaths, test_fpaths, datas, labels, test_labels, test_datas = read_data(data_dir,test_data_dir)<br><br><span class="hljs-comment"># 计算有多少类图片</span><br>num_classes = <span class="hljs-built_in">len</span>(<span class="hljs-built_in">set</span>(labels))<br></code></pre></td></tr></table></figure><p>这一部分代码主要就是实现将文件夹中的训练样本读取出来，保存在numpy数组中。</p><h2 id="定义placeholder占位符">定义placeholder(占位符)</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 定义Placeholder，存放输入和标签</span><br>datas_placeholder = tf.placeholder(tf.float32, [<span class="hljs-literal">None</span>, <span class="hljs-number">32</span>, <span class="hljs-number">32</span>, <span class="hljs-number">3</span>])<br>labels_placeholder = tf.placeholder(tf.int32, [<span class="hljs-literal">None</span>])<br><br><span class="hljs-comment"># 存放DropOut参数的容器，训练时为0.25，测试时为0</span><br>dropout_placeholdr = tf.placeholder(tf.float32)<br></code></pre></td></tr></table></figure><h3 id="palceholder">palceholder</h3><p>可以将placeholder理解为一种形参吧，然后不会被直接运行，只有在调用tf.run方法的时候才会被调用，这个时候需要向placeholder传递参数。函数形式：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python">tf.placeholder(<br>    dtype,<span class="hljs-comment"># 数据类型。常用的是tf.float32,tf.float64等数值类型</span><br>    shape=<span class="hljs-literal">None</span>,<span class="hljs-comment">#数据形状。默认是None，就是一维值，也可以是多维（比如[2,3], [None, 3]表示列是3，行不定）</span><br>    name=<span class="hljs-literal">None</span>  <span class="hljs-comment">#名称</span><br>)<br></code></pre></td></tr></table></figure><h3 id="dropout">dropout</h3><p>dropout主要是为了解决过拟合的情况，过拟合就是将训练集中的一些不是通用特征的特征当作了通用特征，这样可能会在训练集上的损失函数很小，不过在测试的时候，就会导致损失函数很大。举个例子吧，就像用一个网络来判断这是不是一只猫，而训练集中的猫都是白色的，这样就可能会导致整个网络将白色当作判断猫的一个重要特征，而测试集中的猫可能什么颜色都有，这样就会导致损失函数很大。</p><ul><li>而dropout的解决方法就是在训练的时候，以一定的概率让部分神经元停止工作，这样就可以减少特征检测器（隐层节点）间的相互作用，避免过拟合情况的发生。</li><li>测试时整合所有神经元</li></ul><p><img src="/2019/11/17/CNN-%E7%AE%80%E5%8D%95%E5%9B%BE%E7%89%87%E5%88%86%E7%B1%BB/1.png"></p><h2 id="定义卷积神经网络卷积层和pooling层">定义卷积神经网络（卷积层和pooling层）</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 定义卷积层, 20个卷积核, 卷积核大小为5，用Relu激活</span><br>conv0 = tf.layers.conv2d(datas_placeholder, <span class="hljs-number">20</span>, <span class="hljs-number">5</span>, activation=tf.nn.relu)<br><span class="hljs-comment"># 定义max-pooling层，pooling窗口为2x2，步长为2x2</span><br>pool0 = tf.layers.max_pooling2d(conv0, [<span class="hljs-number">2</span>, <span class="hljs-number">2</span>], [<span class="hljs-number">2</span>, <span class="hljs-number">2</span>])<br><br><span class="hljs-comment"># 定义卷积层, 40个卷积核, 卷积核大小为4，用Relu激活</span><br>conv1 = tf.layers.conv2d(pool0, <span class="hljs-number">40</span>, <span class="hljs-number">4</span>, activation=tf.nn.relu)<br><span class="hljs-comment"># 定义max-pooling层，pooling窗口为2x2，步长为2x2</span><br>pool1 = tf.layers.max_pooling2d(conv1, [<span class="hljs-number">2</span>, <span class="hljs-number">2</span>], [<span class="hljs-number">2</span>, <span class="hljs-number">2</span>])<br></code></pre></td></tr></table></figure><h3 id="sigmoid与relu">sigmoid与relu</h3><p><img src="/2019/11/17/CNN-%E7%AE%80%E5%8D%95%E5%9B%BE%E7%89%87%E5%88%86%E7%B1%BB/2.png"> relu函数解决了梯度消失问题。</p><ul><li>在BP算法中，反向传播的过程是一个链式求导的过程（误差通过梯度传播），不过使用sigmoid函数，其中可能某一项的导数存在极小值，这样就会导致整个偏导的导数值较小，导致误差无法向前传播，这样的话，前几层的参数无法得到更新。</li><li>不过relu函数就解决了这个问题啦</li></ul><h3 id="max-pooling">max-pooling</h3><p>池化层有mean-pooling、max-pooling啥的。pooling层主要是保留特征，减少下一层的参数量和计算量，同时也能防止过拟合。例如：max-pooling层的大小为2x2，那么就会对一个2x2的像素快进行取样，得到这个小区域的最大值，并将这个值来代表这个区域块传递到下一层中。通过这样的方式就可以来减少参数量和计算量，并且还保持某种不变性，包括translation(平移)，rotation(旋转)，scale(尺度)</p><h2 id="定义全连接部分">定义全连接部分</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 将3维特征转换为1维向量</span><br>flatten = tf.layers.flatten(pool1)<br><br><span class="hljs-comment"># 全连接层，转换为长度为100的特征向量</span><br>fc = tf.layers.dense(flatten, <span class="hljs-number">400</span>, activation=tf.nn.relu)<br><br><span class="hljs-comment"># 加上DropOut，防止过拟合</span><br>dropout_fc = tf.layers.dropout(fc, dropout_placeholdr)<br><br><span class="hljs-comment"># 未激活的输出层</span><br>logits = tf.layers.dense(dropout_fc, num_classes)<br><span class="hljs-comment"># 预测输出</span><br>predicted_labels = tf.arg_max(logits, <span class="hljs-number">1</span>)<br></code></pre></td></tr></table></figure><p>全连接层，加上dropout，然后最后定义输出层。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python">arg_max(a, axis=<span class="hljs-literal">None</span>, out=<span class="hljs-literal">None</span>)<br><span class="hljs-comment"># a 表示array</span><br><span class="hljs-comment"># axis 表示指定的轴，默认是None，表示把array平铺，</span><br><span class="hljs-comment"># out 默认为None，如果指定，那么返回的结果会插入其中</span><br></code></pre></td></tr></table></figure><p>返回沿轴axis最大值的索引值，也就是最可能的标签值。</p><h2 id="定义损失函数和优化器">定义损失函数和优化器</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 利用交叉熵定义损失</span><br>losses = tf.nn.softmax_cross_entropy_with_logits(<br>    labels=tf.one_hot(labels_placeholder, num_classes),<br>    logits=logits<br>)<br><span class="hljs-comment"># 平均损失</span><br>mean_loss = tf.reduce_mean(losses)<br><br><span class="hljs-comment"># 定义优化器，指定要优化的损失函数</span><br>optimizer = tf.train.AdamOptimizer(learning_rate=<span class="hljs-number">1e-2</span>).minimize(losses)<br></code></pre></td></tr></table></figure><p>代码中的one_hot()函数的作用是将一个值化为一个概率分布的向量，一般用于分类问题。然后再用reduce_mean()得到平均值。</p><h2 id="执行阶段">执行阶段</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 用于保存和载入模型</span><br>saver = tf.train.Saver()<br><br><span class="hljs-keyword">with</span> tf.Session() <span class="hljs-keyword">as</span> sess:<br><br>    <span class="hljs-keyword">if</span> train:<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;训练模式&quot;</span>)<br>        <span class="hljs-comment"># 如果是训练，初始化参数</span><br>        sess.run(tf.global_variables_initializer())<br>        <span class="hljs-comment"># 定义输入和Label以填充容器，训练时dropout为0.25</span><br>        train_feed_dict = &#123;<br>            datas_placeholder: datas,<br>            labels_placeholder: labels,<br>            dropout_placeholdr: <span class="hljs-number">0.25</span><br>        &#125;<br>        <span class="hljs-keyword">for</span> step <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">136</span>):<br>            _, mean_loss_val = sess.run([optimizer, mean_loss], feed_dict=train_feed_dict)<br><br>            <span class="hljs-keyword">if</span> step % <span class="hljs-number">10</span> == <span class="hljs-number">0</span>:<br>                <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;step = &#123;&#125;\tmean loss = &#123;&#125;&quot;</span>.<span class="hljs-built_in">format</span>(step, mean_loss_val))<br>        saver.save(sess, model_path)<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;训练结束，保存模型到&#123;&#125;&quot;</span>.<span class="hljs-built_in">format</span>(model_path))<br>    <span class="hljs-keyword">else</span>:<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;测试模式&quot;</span>)<br>        <span class="hljs-comment"># 如果是测试，载入参数</span><br>        saver.restore(sess, model_path)<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;从&#123;&#125;载入模型&quot;</span>.<span class="hljs-built_in">format</span>(model_path))<br>        <span class="hljs-comment"># label和名称的对照关系</span><br>        label_name_dict = &#123;<br>            <span class="hljs-number">0</span>: <span class="hljs-string">&quot;飞机&quot;</span>,<br>            <span class="hljs-number">1</span>: <span class="hljs-string">&quot;汽车&quot;</span>,<br>            <span class="hljs-number">2</span>: <span class="hljs-string">&quot;鸟&quot;</span><br>        &#125;<br>        <span class="hljs-comment"># 定义输入和Label以填充容器，测试时dropout为0</span><br>        test_feed_dict = &#123;<br>            datas_placeholder: test_datas,<br>            labels_placeholder: labels,<br>            dropout_placeholdr: <span class="hljs-number">0</span><br>        &#125;<br>        predicted_labels_val = sess.run(predicted_labels, feed_dict=test_feed_dict)<br>        <span class="hljs-comment"># 真实label与模型预测label</span><br>        <span class="hljs-keyword">for</span> fpath, real_label, predicted_label <span class="hljs-keyword">in</span> <span class="hljs-built_in">zip</span>(test_fpaths, test_labels, predicted_labels_val):<br>            <span class="hljs-comment"># 将label id转换为label名</span><br>            real_label_name = label_name_dict[real_label]<br>            predicted_label_name = label_name_dict[predicted_label]<br>            <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;&#123;&#125;\t&#123;&#125; =&gt; &#123;&#125;&quot;</span>.<span class="hljs-built_in">format</span>(fpath, real_label_name, predicted_label_name))<br><br></code></pre></td></tr></table></figure><p>这一部分代码感觉没啥特别好写的，大部分都写在注释里了，而且一些写法应该也是比较固定的，，，，</p><h2 id="总结">总结</h2><p>可以看一下运行结果 <img src="/2019/11/17/CNN-%E7%AE%80%E5%8D%95%E5%9B%BE%E7%89%87%E5%88%86%E7%B1%BB/3.jpg">感觉效果还是挺好的，真的挺神奇的。在这个过程中也是学习到了很多知识，这个整理总结归纳的过程也是收获颇多，以后继续加油~</p>]]></content>
    
    
    <categories>
      
      <category>study</category>
      
    </categories>
    
    
    <tags>
      
      <tag>深度学习</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>小波变换</title>
    <link href="/2019/05/12/%E5%B0%8F%E6%B3%A2%E5%8F%98%E6%8D%A2/"/>
    <url>/2019/05/12/%E5%B0%8F%E6%B3%A2%E5%8F%98%E6%8D%A2/</url>
    
    <content type="html"><![CDATA[<p>最近在数字图像处理课上学习了关于小波变换的一些知识，不过对于其中的很多地方不是十分理解，所以也是去查看了很多人的blog，所以在这里算是做一个总结归纳吧，并且也整理一下自己的理解，也算是自己比较用心地一篇整理吧~</p><span id="more"></span><h2 id="变换和傅里叶变换">变换和傅里叶变换</h2><p>在图像处理中，最基本的就是直接对图像的二维数据进行操作，比较神奇的就是卷积操作，能够完成许多图像处理操作，不过这种操作方式有很明显的一些不足之处。所以也是利用很多数学方法来改进图像处理方式，其中包括将图像处理从空间域变换到频率域去进行操作。在变换的具体实现原理中，有很多线性代数的知识，比较主要的就是要理解何为线性独立的向量集basis，而在这个空间中的其他任何向量都可以用这个向量集中的向量来表示。所以说对于这种basis的选择便格外重要，因为选择的basis的特点便决定了可以对图像进行何种类型的操作。</p><p>说到这里，就可以解释一下傅里叶变换了，在傅里叶变换中，使用的是一系列三角函数来拟合出任何波形！这里看到一个大佬写的一篇关于傅里叶变换的blog，感觉写的贼棒！<a href="https://www.cnblogs.com/h2zZhou/p/8405717.html">链接</a></p><p>而对于一幅图像我们也可以对其进行傅里叶变换，当我们对一副图像进行完傅里叶变换之后，便能够很直接的得到图像的主要变换部分和次要变换部分，即高频信息和低频信息。然后我们可以直接在matlab中的到一幅图像的频谱图，在频谱图像中能够更加直观的看出这一整幅图像中的高频部分和低频部分的比例关系。所以说在频率域上，我们能够很直接的对图像的高频信息和低频信息进行操作，比如去除椒盐噪声等。</p><h2 id="小波变换">小波变换</h2><p>可以这样理解小波变换和傅里叶变换的关系吧，在傅里叶变换中，使用的是正弦波，，，不过正弦波是可以无限延申的，这样的特性带了好处的同时也有一定的局限性，比如在处理下面这种突变信号的时候：<img src="/2019/05/12/%E5%B0%8F%E6%B3%A2%E5%8F%98%E6%8D%A2/1.jpg"></p><p>使用三角函数的正弦波去拟合的话，则需要使用许多的三角函数去拟合这样一个简单的突变信号。</p><p><img src="/2019/05/12/%E5%B0%8F%E6%B3%A2%E5%8F%98%E6%8D%A2/2.png"></p><p>同时在使用傅里叶变换的时候，可以很直接的得到频率信息，但是却不能够得到对应图像中的时频信息，即某一段高频具体出现在图像中的何处位置，如下：<img src="/2019/05/12/%E5%B0%8F%E6%B3%A2%E5%8F%98%E6%8D%A2/4.jpg"></p><p>而对于小波变换，则不会存在这样的情况，所谓小波，就是一种能量十分集中的波形，就是说他并不是无限拓展的，而是集中在一定的区域内。<img src="/2019/05/12/%E5%B0%8F%E6%B3%A2%E5%8F%98%E6%8D%A2/5.jpg">而这样的形式便能够很好的处理这种突变的信号。小波做的改变就在于，将无限长的三角函数基换成了有限长的会衰减的小波基。在小波变换中，有两个变量：其中一个为尺度变量，用来控制小波的伸缩，一个变量为平移变量，用来控制小波函数的平移距离。而这样也就解决了上面所说的傅里叶变换无法得到时频信息的问题，因为通过平移变量可以直到某一频率信息是对应在时域的具体位置。同时对于突变信号的处理也能很好的解决： <img src="/2019/05/12/%E5%B0%8F%E6%B3%A2%E5%8F%98%E6%8D%A2/6.png"></p><p>通过傅里叶变换，我们进能够得到一副频谱图，没有时域信息，但通过小波变换，我们不仅是可以得到频谱图，更是可以得到一副时频谱！<img src="/2019/05/12/%E5%B0%8F%E6%B3%A2%E5%8F%98%E6%8D%A2/7.jpg"></p><p>除此之外，因为傅里叶变换的基是确定的，即正弦波，而小波变换中的基并不是确定的，所以会更加灵活，对于不同的信息进行处理的时候，也可以去选择专门针对这种信号处理的基波。总结来说，傅立叶变换适合周期性的，统计特性不随时间变化的信号;而小波变换则适用于大部分信号，尤其是瞬时信号。它针对绝大部分信号的压缩，去噪，检测效果都特别好。</p><p>这里再附上一篇深入讲解小波变换的blog，有很多数学推导了，也更加深入。<a href="https://blog.csdn.net/qq_20823641/article/details/51829981">链接</a></p><h2 id="图像二维快速小波变换">图像二维快速小波变换</h2><p>在数字图像处理实验中，使用小波变换对图像进行处理，这里总结一下大概处理思路：</p><ul><li>对图像在水平方向上进行滤波处理，得到高频信息部分H和低频信息部分L。</li><li>然后再对图像的每一列进行滤波处理，这样就在原有信息的基础上，得到了四幅图像，分别是：HH，HL，LH，LL。</li><li>如果是对图像进行多阶的小波变换，那么就对LL部分重复进行上述操作即可。</li></ul><p>在重构过程中，首先对变换结果的每一列进行以为离散小波逆变换，再对变换所得数据的每一行进行一维离散小波逆变换，即可获得重构图像。（如果是多阶的小波变换，那么就需要进行多次的重构）<img src="/2019/05/12/%E5%B0%8F%E6%B3%A2%E5%8F%98%E6%8D%A2/8.jpg"></p><p>通过这样的方式可以分理处高频和低频信息，并且得到对应的时域信息，所以更为直观，也更方便操作。</p><p>下面给出实验的matlab代码（其中用的函数，很多能直接从网上找到，我就不再放上来了），以及实验结果：</p><figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><code class="hljs matlab">f=rgb2gray(imread(<span class="hljs-string">&#x27;laoshan.jpg&#x27;</span>));<br><span class="hljs-built_in">figure</span>,imshow(f);<br><br><span class="hljs-comment">% 小波变换次数</span><br>time = <span class="hljs-number">4</span>;<br><br>[c,s]=wavefast(f,time,<span class="hljs-string">&#x27;haar&#x27;</span>);<br><span class="hljs-built_in">figure</span>; wave2gray(c,s,<span class="hljs-number">-6</span>);<br><br><span class="hljs-comment">%     &#x27;a&#x27;         Approximation coefficients</span><br><span class="hljs-comment">%     &#x27;h&#x27;         Horizontal details</span><br><span class="hljs-comment">%     &#x27;v&#x27;         Vertical details</span><br><span class="hljs-comment">%     &#x27;d&#x27;         Diagonal details</span><br><br><span class="hljs-comment">% 重构所有系数，得到原始图像</span><br>[c,s]=waveback(c,s,<span class="hljs-string">&#x27;haar&#x27;</span>,time);<br>o_f=wavecopy(<span class="hljs-string">&#x27;a&#x27;</span>,c,s);<br><span class="hljs-built_in">figure</span>,imshow(mat2gray(o_f));<br><br></code></pre></td></tr></table></figure><p>原始图像: <img src="/2019/05/12/%E5%B0%8F%E6%B3%A2%E5%8F%98%E6%8D%A2/10.jpg"></p><p>快速小波变换后的时频信息： <img src="/2019/05/12/%E5%B0%8F%E6%B3%A2%E5%8F%98%E6%8D%A2/11.jpg"></p><p>重构后的图像： <img src="/2019/05/12/%E5%B0%8F%E6%B3%A2%E5%8F%98%E6%8D%A2/12.jpg"></p>]]></content>
    
    
    <categories>
      
      <category>study</category>
      
    </categories>
    
    
    <tags>
      
      <tag>图像学相关</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>条纹干扰&amp;椒盐噪声处理</title>
    <link href="/2019/04/29/%E6%9D%A1%E7%BA%B9%E5%B9%B2%E6%89%B0-%E6%A4%92%E7%9B%90%E5%99%AA%E5%A3%B0%E5%A4%84%E7%90%86/"/>
    <url>/2019/04/29/%E6%9D%A1%E7%BA%B9%E5%B9%B2%E6%89%B0-%E6%A4%92%E7%9B%90%E5%99%AA%E5%A3%B0%E5%A4%84%E7%90%86/</url>
    
    <content type="html"><![CDATA[<p>最近做了数字图像处理的实验，通过频率域上的带阻滤波器和自适应中值滤波器来处理图像的条纹干扰和椒盐噪声，效果也是挺好的，takea note~</p><span id="more"></span><h2 id="带阻滤波器">带阻滤波器</h2><p>对于去除条纹干扰，可以在频率域上使用带阻滤波器来处理，常用的带阻滤波器有理想带阻滤波器、巴特沃斯带阻滤波器和高斯带阻滤波器，设计思路如下：</p><ul><li>首先对原图像进行傅里叶变换，然后通过fftshift()函数将零频分量移动到数组中心,重新排列傅里叶变换。</li></ul><figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs matlab"><span class="hljs-comment">% 显示原图像：</span><br>f=imread(str_path); <br><span class="hljs-built_in">figure</span>,subplot(<span class="hljs-number">2</span>,<span class="hljs-number">2</span>,<span class="hljs-number">1</span>),imshow(f); <br>title(<span class="hljs-string">&#x27;原图像&#x27;</span>);<br>F=fft2(f);  <span class="hljs-comment">% 二维快速傅里叶变换</span><br>[M, N]=<span class="hljs-built_in">size</span>(f);<br>f=double(f);<br>fc=fftshift(F); <span class="hljs-comment">%将零频分量移动到数组中心,重新排列傅里叶变换 </span><br>s=<span class="hljs-built_in">log</span>(<span class="hljs-number">1</span>+<span class="hljs-built_in">abs</span>(fc)); <br>subplot(<span class="hljs-number">2</span>,<span class="hljs-number">2</span>,<span class="hljs-number">2</span>),imshow(s,[]);<br>title(<span class="hljs-string">&#x27;原图像频谱&#x27;</span>);<br></code></pre></td></tr></table></figure><ul><li>然后设计滤波函数，几种滤波函数设计过程如下：</li></ul><figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs matlab"><span class="hljs-comment">%理想带阻滤波器滤波效果：</span><br>[u,v]=dftuv(M,N); <br><br>d1=<span class="hljs-built_in">sqrt</span>(u.^<span class="hljs-number">2</span>+v.^<span class="hljs-number">2</span>); <br>w=<span class="hljs-number">5</span>;<br>D0=<span class="hljs-number">15</span>;<br>ba= d1&lt;(D0-w/<span class="hljs-number">2</span>) | d1&gt;(D0+w/<span class="hljs-number">2</span>);  <span class="hljs-comment">% 设计带阻滤波器</span><br>H=double(ba);   <span class="hljs-comment">% 滤波函数</span><br><br><span class="hljs-comment">%巴特沃斯带阻滤波：</span><br>n=<span class="hljs-number">3</span>;<br>H = <span class="hljs-number">1.</span>/(<span class="hljs-number">1</span> + (w*d1./(d1.^<span class="hljs-number">2</span>-D0^<span class="hljs-number">2</span>)).^(<span class="hljs-number">2</span>*n));<br><br><span class="hljs-comment">% 高斯带阻滤波：</span><br>H = <span class="hljs-number">1</span>-<span class="hljs-built_in">exp</span>(<span class="hljs-number">-1</span>/<span class="hljs-number">2</span>*(((d1.^<span class="hljs-number">2</span>)-D0^<span class="hljs-number">2</span>)./(d1*w)).^<span class="hljs-number">2</span>);<br></code></pre></td></tr></table></figure><ul><li>然后进行频域率滤波，之后反变换回空间域即可。</li></ul><figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs matlab">g=<span class="hljs-built_in">real</span>(ifft2(H.*F));<span class="hljs-comment">% 反变换回空间域</span><br>subplot(<span class="hljs-number">2</span>,<span class="hljs-number">2</span>,<span class="hljs-number">3</span>),imshow(g,[]); <br>title(<span class="hljs-string">&#x27;理想带阻&#x27;</span>);<br>F=fft2(g); <br>fc=fftshift(F); <br>s=<span class="hljs-built_in">log</span>(<span class="hljs-number">1</span>+<span class="hljs-built_in">abs</span>(fc));  <br>subplot(<span class="hljs-number">2</span>,<span class="hljs-number">2</span>,<span class="hljs-number">4</span>),imshow(s,[]); <br>title(<span class="hljs-string">&#x27;理想带阻频谱&#x27;</span>);<br></code></pre></td></tr></table></figure><ul><li>效果如下： <img src="/2019/04/29/%E6%9D%A1%E7%BA%B9%E5%B9%B2%E6%89%B0-%E6%A4%92%E7%9B%90%E5%99%AA%E5%A3%B0%E5%A4%84%E7%90%86/1.png" alt="img"> <img src="/2019/04/29/%E6%9D%A1%E7%BA%B9%E5%B9%B2%E6%89%B0-%E6%A4%92%E7%9B%90%E5%99%AA%E5%A3%B0%E5%A4%84%E7%90%86/2.png" alt="img"></li></ul><h2 id="自适应中值滤波器">自适应中值滤波器</h2><p>在处理椒盐噪声的过程中，使用了自适应中值滤波器，自适应中值滤波思路如下。</p><ul><li>首先我们定义一个滤波窗口的最大值Rmax。</li><li>为了方便处理边界像素，对图像进行拓展，并对拓展的部分使用与图像中的图像进行填充。</li><li>初始化窗口大小为1，计算出当前窗口中的像素的最大值和最小值，然后与当前窗口的中值进行比较，如果该中止在最大值与最小值之间，那么可以说明该像素点不是噪声点，所以可以使用。</li><li>否则的话，就扩大窗口，直到到达最大窗口值。</li><li>这时候就是用该窗口的中值来替代该像素值。</li><li>不过在实验过程中，发现仅通过一次自适应中值滤波的效果并不是很好，所以将中值滤波写成了一个单独的函数，然后通过多次调用，来增强去除椒盐噪声的效果。</li></ul><figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><code class="hljs matlab"><span class="hljs-function"><span class="hljs-keyword">function</span> <span class="hljs-title">I</span> = <span class="hljs-title">myfuc</span><span class="hljs-params">( img, Rmax )</span></span><br><span class="hljs-comment">%MYFUC 此处显示有关此函数的摘要</span><br><span class="hljs-comment">%   此处显示详细说明</span><br>[m, n]=<span class="hljs-built_in">size</span>(img);<br>imgn=<span class="hljs-built_in">zeros</span>(m+<span class="hljs-number">2</span>*Rmax+<span class="hljs-number">1</span>,n+<span class="hljs-number">2</span>*Rmax+<span class="hljs-number">1</span>);<br>imgn(Rmax+<span class="hljs-number">1</span>:m+Rmax,Rmax+<span class="hljs-number">1</span>:n+Rmax)=img(:,:);<br><br>imgn(<span class="hljs-number">1</span>:Rmax,Rmax+<span class="hljs-number">1</span>:n+Rmax)=img(<span class="hljs-number">1</span>:Rmax,<span class="hljs-number">1</span>:n);                 <span class="hljs-comment">%扩展上边界</span><br>imgn(<span class="hljs-number">1</span>:m+Rmax,n+Rmax+<span class="hljs-number">1</span>:n+<span class="hljs-number">2</span>*Rmax+<span class="hljs-number">1</span>)=imgn(<span class="hljs-number">1</span>:m+Rmax,n:n+Rmax);    <span class="hljs-comment">%扩展右边界</span><br>imgn(m+Rmax+<span class="hljs-number">1</span>:m+<span class="hljs-number">2</span>*Rmax+<span class="hljs-number">1</span>,Rmax+<span class="hljs-number">1</span>:n+<span class="hljs-number">2</span>*Rmax+<span class="hljs-number">1</span>)=imgn(m:m+Rmax,Rmax+<span class="hljs-number">1</span>:n+<span class="hljs-number">2</span>*Rmax+<span class="hljs-number">1</span>);    <span class="hljs-comment">%扩展下边界</span><br>imgn(<span class="hljs-number">1</span>:m+<span class="hljs-number">2</span>*Rmax+<span class="hljs-number">1</span>,<span class="hljs-number">1</span>:Rmax)=imgn(<span class="hljs-number">1</span>:m+<span class="hljs-number">2</span>*Rmax+<span class="hljs-number">1</span>,Rmax+<span class="hljs-number">1</span>:<span class="hljs-number">2</span>*Rmax);       <span class="hljs-comment">%扩展左边界</span><br><br>I=imgn;<br><span class="hljs-keyword">for</span> <span class="hljs-built_in">i</span>=Rmax+<span class="hljs-number">1</span>:m+Rmax<br>    <span class="hljs-keyword">for</span> <span class="hljs-built_in">j</span>=Rmax+<span class="hljs-number">1</span>:n+Rmax<br>    <br>        r=<span class="hljs-number">1</span>;                <span class="hljs-comment">%初始滤波半径</span><br>        <span class="hljs-keyword">while</span> r~=Rmax<br>            W=imgn(<span class="hljs-built_in">i</span>-r:<span class="hljs-built_in">i</span>+r,<span class="hljs-built_in">j</span>-r:<span class="hljs-built_in">j</span>+r);<br>            W=<span class="hljs-built_in">sort</span>(W);<br>            Imin=<span class="hljs-built_in">min</span>(W(:));<br>            Imax=<span class="hljs-built_in">max</span>(W(:));<br>            Imed=W(uint8((<span class="hljs-number">2</span>*r+<span class="hljs-number">1</span>)^<span class="hljs-number">2</span>/<span class="hljs-number">2</span>));<br>            <span class="hljs-keyword">if</span> Imin&lt;Imed &amp;&amp; Imed&lt;Imax       <span class="hljs-comment">%如果当前邻域中值不是噪声点，那么就用此次的邻域</span><br>               <span class="hljs-keyword">break</span>;<br>            <span class="hljs-keyword">else</span><br>                r=r+<span class="hljs-number">1</span>;              <span class="hljs-comment">%否则扩大窗口，继续判断</span><br>            <span class="hljs-keyword">end</span>      <br>        <span class="hljs-keyword">end</span><br>    <br>        <span class="hljs-keyword">if</span> Imin&lt;imgn(<span class="hljs-built_in">i</span>,<span class="hljs-built_in">j</span>) &amp;&amp; imgn(<span class="hljs-built_in">i</span>,<span class="hljs-built_in">j</span>)&lt;Imax         <span class="hljs-comment">%如果当前这个像素不是噪声，原值输出</span><br>            I(<span class="hljs-built_in">i</span>,<span class="hljs-built_in">j</span>)=imgn(<span class="hljs-built_in">i</span>,<span class="hljs-built_in">j</span>);<br>        <span class="hljs-keyword">else</span>                                        <span class="hljs-comment">%否则输出邻域中值</span><br>            I(<span class="hljs-built_in">i</span>,<span class="hljs-built_in">j</span>)=Imed;<br>        <span class="hljs-keyword">end</span><br>    <br>    <span class="hljs-keyword">end</span><br><span class="hljs-keyword">end</span><br>I = I(Rmax+<span class="hljs-number">1</span>:m+Rmax,Rmax+<span class="hljs-number">1</span>:n+Rmax);<br><span class="hljs-keyword">end</span><br></code></pre></td></tr></table></figure><ul><li>效果如下： <img src="/2019/04/29/%E6%9D%A1%E7%BA%B9%E5%B9%B2%E6%89%B0-%E6%A4%92%E7%9B%90%E5%99%AA%E5%A3%B0%E5%A4%84%E7%90%86/3.png" alt="img"></li></ul>]]></content>
    
    
    <categories>
      
      <category>study</category>
      
    </categories>
    
    
    <tags>
      
      <tag>图像学相关</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>图片像素化处理</title>
    <link href="/2019/03/26/%E5%9B%BE%E7%89%87%E5%83%8F%E7%B4%A0%E5%8C%96%E5%A4%84%E7%90%86/"/>
    <url>/2019/03/26/%E5%9B%BE%E7%89%87%E5%83%8F%E7%B4%A0%E5%8C%96%E5%A4%84%E7%90%86/</url>
    
    <content type="html"><![CDATA[<p>对一张图片进行像素化操作，用很多张小图片来替换原有图片中的像素部分，从而拼凑成整张图片（有像素化效果）。</p><span id="more"></span><p>先列一下自己的大体思路：</p><ul><li>收集一定数量的素材图片，用来进行图片替换，然后确定一张被替换的图片Image。对于素材图片，首先将其进行缩放处理，例如，我们可以将其缩放成7* 7的图像块，然后我们提取其RGB值，求得该图像块的平均值。</li><li>然后我们对被替换的图片Image进行操作，将其进行分割操作，划分为一个个7*7的小块，然后对划分出来的每一个小区域同样进行RGB平均值的计算，然后拿来和素材像素块进行比较，找到一个误差值（在代码中我是使用的计算方差的方式）较小（颜色更为接近）的像素图像块，然后将其替换。</li><li>重复上述操作，知道对整张图片操作完，然后我们将图像保存下来即可。</li></ul><p>思路比较清晰，不过是一次编写matlab的代码，所以更多的问题都是出现在代码编写上，还是要多动手啊。</p><figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br></pre></td><td class="code"><pre><code class="hljs matlab"><br>file_path =  <span class="hljs-string">&#x27;素材/&#x27;</span>;<span class="hljs-comment">% 图像文件夹路径</span><br>N = <span class="hljs-number">7</span>;<span class="hljs-comment">% 图像块大小</span><br><br>img_path_list = dir(strcat(file_path,<span class="hljs-string">&#x27;*.jpg&#x27;</span>));<span class="hljs-comment">%获取该文件夹中所有jpg格式的图像</span><br>img_num = <span class="hljs-built_in">length</span>(img_path_list);<span class="hljs-comment">%获取图像总数量</span><br><span class="hljs-comment">% disp(img_num)</span><br>img_RGB = <span class="hljs-built_in">zeros</span>(img_num,N,N,<span class="hljs-number">3</span>);<span class="hljs-comment">%用来存放小的图像块</span><br>img_rgb = <span class="hljs-built_in">zeros</span>(img_num,<span class="hljs-number">3</span>); <span class="hljs-comment">%用来存放平均RGB值</span><br><br><span class="hljs-keyword">if</span> img_num &gt; <span class="hljs-number">0</span> <span class="hljs-comment">%有满足条件的图像</span><br>        <span class="hljs-keyword">for</span> <span class="hljs-built_in">j</span> = <span class="hljs-number">1</span>:img_num <span class="hljs-comment">%逐一读取图像</span><br>            <span class="hljs-built_in">disp</span>(<span class="hljs-built_in">j</span>);<br>            img_name = img_path_list(<span class="hljs-built_in">j</span>).name;<span class="hljs-comment">% 图像名</span><br>            img =  imread(strcat(file_path,img_name));<br>            img = imresize(img,[N N]);<span class="hljs-comment">% 改变图像尺寸的大小</span><br>            img_RGB(<span class="hljs-built_in">j</span>,:,:,:) = img; <br><span class="hljs-comment">            %&#123;</span><br><span class="hljs-comment">            ccc = img_RGB(j,:,:,:);</span><br><span class="hljs-comment">            ccc = squeeze(ccc);</span><br><span class="hljs-comment">            if img == ccc</span><br><span class="hljs-comment">               disp(&#x27;sdfasdf&#x27;); </span><br><span class="hljs-comment">            end</span><br><span class="hljs-comment">            ccc = uint8(ccc);</span><br><span class="hljs-comment">            imwrite(ccc,&#x27;ccc.jpg&#x27;);</span><br><span class="hljs-comment">            imwrite(img,&#x27;img.jpg&#x27;);</span><br><span class="hljs-comment">            %&#125;</span><br>            s = <span class="hljs-built_in">size</span>(img);<span class="hljs-comment">%获取图片尺寸</span><br>            <span class="hljs-comment">% 分别读取RGB三个颜色</span><br>            R = img(:,:,<span class="hljs-number">1</span>);<br>            G = img(:,:,<span class="hljs-number">2</span>);<br>            B = img(:,:,<span class="hljs-number">3</span>);<br>            <span class="hljs-comment">% 转换成一维数组</span><br>            R=<span class="hljs-built_in">reshape</span>(R,[s(<span class="hljs-number">1</span>),s(<span class="hljs-number">2</span>)]);<br>            G=<span class="hljs-built_in">reshape</span>(G,[s(<span class="hljs-number">1</span>),s(<span class="hljs-number">2</span>)]);<br>            B=<span class="hljs-built_in">reshape</span>(B,[s(<span class="hljs-number">1</span>),s(<span class="hljs-number">2</span>)]);<br>            r=<span class="hljs-built_in">mean</span>(<span class="hljs-built_in">mean</span>(R));<span class="hljs-comment">%红色均值</span><br>            g=<span class="hljs-built_in">mean</span>(<span class="hljs-built_in">mean</span>(G));<span class="hljs-comment">%绿色均值</span><br>            b=<span class="hljs-built_in">mean</span>(<span class="hljs-built_in">mean</span>(B));<span class="hljs-comment">%蓝色均值</span><br>            <span class="hljs-comment">% 储存均值</span><br>            img_rgb(<span class="hljs-built_in">j</span>,<span class="hljs-number">1</span>) = r;<br>            img_rgb(<span class="hljs-built_in">j</span>,<span class="hljs-number">2</span>) = g;<br>            img_rgb(<span class="hljs-built_in">j</span>,<span class="hljs-number">3</span>) = b;<br>        <span class="hljs-keyword">end</span><br><span class="hljs-keyword">end</span><br><span class="hljs-comment">%disp(img_rgb);</span><br><br>Image_test = imread(<span class="hljs-string">&#x27;test.jpg&#x27;</span>);<br>Image_size = <span class="hljs-built_in">size</span>(Image_test);<br><br>row_num = Image_size(<span class="hljs-number">1</span>)/N;<br>col_num = Image_size(<span class="hljs-number">2</span>)/N;<br><br>Image_RGB = <span class="hljs-built_in">zeros</span>(Image_size(<span class="hljs-number">1</span>),Image_size(<span class="hljs-number">2</span>),<span class="hljs-number">3</span>);<br>Image_RGB(:,:,:) = Image_test(:,:,:);<br><br><span class="hljs-keyword">for</span> <span class="hljs-built_in">i</span> = <span class="hljs-number">0</span>:row_num<span class="hljs-number">-1</span><br>    row = <span class="hljs-built_in">i</span>*N+<span class="hljs-number">1</span>;<br>    <span class="hljs-keyword">for</span> <span class="hljs-built_in">j</span> = <span class="hljs-number">0</span>:col_num<span class="hljs-number">-1</span><br>       col = <span class="hljs-built_in">j</span>*N+<span class="hljs-number">1</span>;<br>       rgb_mean = <span class="hljs-built_in">zeros</span>(<span class="hljs-number">3</span>);<br>       <span class="hljs-keyword">for</span> k = <span class="hljs-number">1</span>:<span class="hljs-number">3</span><br>            <span class="hljs-comment">% 从图片中截取处理块</span><br>            temp_rgb = Image_RGB(:,:,:);<br>            temp = temp_rgb(row:row+N<span class="hljs-number">-1</span> ,col:col+N<span class="hljs-number">-1</span>,k);<br>            <span class="hljs-comment">% 转换成一维数组</span><br>            temp = <span class="hljs-built_in">reshape</span>(temp,[N,N]);<br>            <span class="hljs-comment">% 得到均值</span><br>            rgb_mean(k) = <span class="hljs-built_in">mean</span>(<span class="hljs-built_in">mean</span>(temp));<br>       <span class="hljs-keyword">end</span><br>       <span class="hljs-comment">% 计算图像和图像块的RGB误差</span><br>       wucha = <span class="hljs-built_in">zeros</span>(<span class="hljs-number">1</span>,img_num);<br>       <span class="hljs-keyword">for</span> k = <span class="hljs-number">1</span>:img_num<br>           wucha(k) = (<span class="hljs-built_in">abs</span>(img_rgb(k,<span class="hljs-number">1</span>) - rgb_mean(<span class="hljs-number">1</span>))/<span class="hljs-number">256</span>)^<span class="hljs-number">2</span> + (<span class="hljs-built_in">abs</span>(img_rgb(k,<span class="hljs-number">2</span>) - rgb_mean(<span class="hljs-number">2</span>))/<span class="hljs-number">256</span>)^<span class="hljs-number">2</span> + (<span class="hljs-built_in">abs</span>(img_rgb(k,<span class="hljs-number">3</span>) - rgb_mean(<span class="hljs-number">3</span>))/<span class="hljs-number">256</span>)^<span class="hljs-number">2</span>;<br>       <span class="hljs-keyword">end</span><br>       <span class="hljs-comment">% 找到误差最小的图像块</span><br>       [min_wc,min_wucha] = <span class="hljs-built_in">min</span>(wucha); <br>       <span class="hljs-comment">% 进行替换</span><br>       Image_RGB(row:row+N<span class="hljs-number">-1</span> ,col:col+N<span class="hljs-number">-1</span>,:) = img_RGB(min_wucha,:,:,:);<br>    <span class="hljs-keyword">end</span><br><span class="hljs-keyword">end</span><br>Image_RGB = uint8(Image_RGB);<br>imwrite(Image_RGB,<span class="hljs-string">&#x27;result.jpg&#x27;</span>);<br><br></code></pre></td></tr></table></figure><p>然后在这里主要说一下自己遇到的问题吧~</p><ul><li>矩阵之间的赋值变换。因为是对图像处理，所以会有很多的矩阵运算，同时又需要同时对RGB三个颜色通道进行处理，所以矩阵的维数比较高，所以在操作的时候也是晕了好几次，同时在矩阵变换的过程中，也是遇到了很多比较难处理的地方。比如将一个高维数组中的后几维赋给一个变量的时候，会遇到赋值过去的数据仍是一个高维数组，然后前几维的维度都是1，这时候，可以使用squeeze（）函数来解决这个问题。</li><li>同时还有就是图像数据的处理，对于imwrite函数而言，参数中的三维数组必须是unit8类型的，在实验过程中，没有注意到这一方面，所以便直接将一个数组当作参数写了进去，，，当时调试了许久，都是泪。所以可以通过uint8（）函数来进行转化。这里在附上一个<a href="https://blog.csdn.net/fx677588/article/details/53301740/">blog地址</a>，是写着一方面的。</li><li>还有一个主要问题就是替换素材图片的选择，在代码实现中，我是采用了RGB方差最小的方式来进行素材图片的选择了，但最初发现效果并不是很好，后来想到可以通过增加素材图片数量的方式来优化效果，下图中的效果是拥有18个素材图片的结果，感觉已经比较完善了，相信如果素材图片更多或者有更好的选择方式，那么最终展现的效果也一定会好很多。</li></ul><p>实现效果如下：</p><figure><img src="/2019/03/26/%E5%9B%BE%E7%89%87%E5%83%8F%E7%B4%A0%E5%8C%96%E5%A4%84%E7%90%86/test.jpg" alt="img"><figcaption aria-hidden="true">img</figcaption></figure><figure><img src="/2019/03/26/%E5%9B%BE%E7%89%87%E5%83%8F%E7%B4%A0%E5%8C%96%E5%A4%84%E7%90%86/result.jpg" alt="img"><figcaption aria-hidden="true">img</figcaption></figure>]]></content>
    
    
    <categories>
      
      <category>study</category>
      
    </categories>
    
    
    <tags>
      
      <tag>图像学相关</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>直线段&amp;圆弧生成算法</title>
    <link href="/2018/12/26/%E7%9B%B4%E7%BA%BF%E6%AE%B5-%E5%9C%86%E5%BC%A7%E7%94%9F%E6%88%90%E7%AE%97%E6%B3%95/"/>
    <url>/2018/12/26/%E7%9B%B4%E7%BA%BF%E6%AE%B5-%E5%9C%86%E5%BC%A7%E7%94%9F%E6%88%90%E7%AE%97%E6%B3%95/</url>
    
    <content type="html"><![CDATA[<p>使用DDA算法、Bresenham算法和中点分割算法实现直线的绘制，以及Bresenham圆与圆弧生成算法</p><span id="more"></span><h2 id="dda直线段绘制算法">DDA直线段绘制算法</h2><p>数值微分分析法( Digital Differential Analyzer,DDA)，用数值方法求解微分方程。</p><p>在该算法中，我们根据已知线段的两个端点，可以得到x轴和y轴方向上的增长量Tx和Ty。令n= max( Tx, Ty)，然后可以得到该直线段的增长步长dx = Tx / n , dy = Ty / n，在这种情况下，一个步长为1，另一个步长小于1。反应了在x轴和y轴上的增长速度。然后一直绘制下去，便能够得到该直线段。</p><p>实现代码如下~</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-function"><span class="hljs-type">void</span> <span class="hljs-title">CcgPHDDrawLineView::DDAline</span><span class="hljs-params">(<span class="hljs-type">int</span> x1, <span class="hljs-type">int</span> y1, <span class="hljs-type">int</span> x2, <span class="hljs-type">int</span> y2, CDC * pDC)</span></span><br><span class="hljs-function"></span>&#123;<br><span class="hljs-type">int</span> dx, dy, n;<br><span class="hljs-type">float</span> xinc, yinc, x, y;<br><span class="hljs-type">float</span> averageError = <span class="hljs-number">0</span>;<br>CcgPHDDrawLineDoc *pDoc = (CcgPHDDrawLineDoc*)<span class="hljs-built_in">GetDocument</span>();<br><br>dx = x2 - x1; dy = y2 - y1;<br><span class="hljs-keyword">if</span>(<span class="hljs-built_in">abs</span>(dx)&gt;<span class="hljs-built_in">abs</span>(dy))<br>n = <span class="hljs-built_in">abs</span>(dx);<br><span class="hljs-keyword">else</span><br>n = <span class="hljs-built_in">abs</span>(dy);<br>xinc = (<span class="hljs-type">float</span>)dx / n;<span class="hljs-comment">//步长</span><br>yinc = (<span class="hljs-type">float</span>)dy / n;<br><br>x = (<span class="hljs-type">float</span>)x1; y = (<span class="hljs-type">float</span>)y1;<br><span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> i = <span class="hljs-number">1</span>; i &lt;= n; i++)<br>&#123;<br>pDC-&gt;<span class="hljs-built_in">SetPixel</span>(<span class="hljs-built_in">int</span>(x + <span class="hljs-number">0.5</span>), <span class="hljs-built_in">int</span>(y + <span class="hljs-number">0.5</span>), <span class="hljs-built_in">RGB</span>(<span class="hljs-number">255</span>,<span class="hljs-number">0</span>,<span class="hljs-number">0</span>));<span class="hljs-comment">//+0.5四舍五入</span><br><br><span class="hljs-comment">//误差计算</span><br>averageError += <br>(<span class="hljs-built_in">fabs</span>((y2 - y1) * x + (x2 - x1) * y + ((x2 * y1) - (x1 * y2)))) / (<span class="hljs-built_in">sqrt</span>(<span class="hljs-built_in">pow</span>(y2 - y1, <span class="hljs-number">2</span>) + <span class="hljs-built_in">pow</span>(x2 - x1, <span class="hljs-number">2</span>)));<br><br>x += xinc;<span class="hljs-comment">//x，y进行步长增长</span><br>y += yinc;<br>&#125;<br>averageError /= n;<br>pDoc-&gt;m_averageError += averageError;<br><br>pDoc-&gt;m_error = <span class="hljs-built_in">sqrt</span>(<span class="hljs-built_in">pow</span>(y - y2, <span class="hljs-number">2</span>) + <span class="hljs-built_in">pow</span>(x - x2, <span class="hljs-number">2</span>));<br>&#125;<br></code></pre></td></tr></table></figure><h2 id="bresenham直线段绘制算法">Bresenham直线段绘制算法</h2><p>在已知一个绘制点(x,y)之后，继续向后绘制，我们让x每次增长1，但是y的坐标根据其是靠近该点所处的单元格的距离来决定，如果离上边近则y加1，如果离下边近则还是y。</p><p><img src="/2018/12/26/%E7%9B%B4%E7%BA%BF%E6%AE%B5-%E5%9C%86%E5%BC%A7%E7%94%9F%E6%88%90%E7%AE%97%E6%B3%95/1.png"></p><p><img src="/2018/12/26/%E7%9B%B4%E7%BA%BF%E6%AE%B5-%E5%9C%86%E5%BC%A7%E7%94%9F%E6%88%90%E7%AE%97%E6%B3%95/2.png"></p><p>可以看出d = dy / dx，然后我们再求下一次的误差dd的计算：</p><ul><li>当选择Pd时，下一次的误差dd = dy / dx * 2 = d + dy / dx。</li><li>当选择Pu时，下一次的误差dd = dy / dx * 2 - 1 = d + dy / dx - 1;</li></ul><p>我们将d与0.5比较，改为和0比较~，然后用e来表示，这样可以看出e的初值为dy / dx - 1/2 = (2dy - dx) / 2dx。</p><p><img src="/2018/12/26/%E7%9B%B4%E7%BA%BF%E6%AE%B5-%E5%9C%86%E5%BC%A7%E7%94%9F%E6%88%90%E7%AE%97%E6%B3%95/3.png"></p><p>然后我们可以进一步优化e，将上式两边同时乘以2dx，对判断没有影响，那么有e = 2dy - dx。然后下一次增长的时候：</p><ul><li>y方向增量为0。e += 2dy。</li><li>y方向增量为1。e = e + 2dy - 2dx。</li></ul><p>不过上面讨论的情况只适用于第一八分圆域内（x正增长，y正增长）。对于全部适用的情况，我们首先需要判断直线的增长方向，来确定x和y时正增长还是反向增长。其基本原理就是这些，也比较简单~，直接贴代码~</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-function"><span class="hljs-type">void</span> <span class="hljs-title">CcgPHDDrawLineView::Bline</span><span class="hljs-params">(<span class="hljs-type">int</span> x1, <span class="hljs-type">int</span> y1, <span class="hljs-type">int</span> x2, <span class="hljs-type">int</span> y2, CDC * pDC)</span></span><br><span class="hljs-function"></span>&#123;<br><span class="hljs-type">int</span> dx, dy, xSign, ySign;<br><span class="hljs-type">int</span> interChange = <span class="hljs-number">0</span>, e;<br><span class="hljs-type">int</span> x, y;<br><br>dx = <span class="hljs-built_in">abs</span>(x2 - x1); dy = <span class="hljs-built_in">abs</span>(y2 - y1);<br>e = <span class="hljs-number">2</span> * dy - dx;<span class="hljs-comment">//反复迭代的误差因数</span><br>xSign = (x2 &gt; x1) ? <span class="hljs-number">1</span> : <span class="hljs-number">-1</span>;<span class="hljs-comment">//用来确定递增方向</span><br>ySign = (y2 &gt; y1) ? <span class="hljs-number">1</span> : <span class="hljs-number">-1</span>;<br><span class="hljs-keyword">if</span> (dx &lt; dy) &#123;<br><span class="hljs-type">int</span> exc;<br>exc = dx; dx = dy; dy = exc;<span class="hljs-comment">//dx与dy互换</span><br>interChange = <span class="hljs-number">1</span>;<br>&#125;<br>x = x1; y = y1;<br><span class="hljs-keyword">for</span> (<span class="hljs-type">int</span> i = <span class="hljs-number">0</span>; i &lt;= dx; i++)<br>&#123;<br>pDC-&gt;<span class="hljs-built_in">SetPixel</span>(x, y, <span class="hljs-built_in">RGB</span>(<span class="hljs-number">255</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>));<br><br><span class="hljs-keyword">if</span> (e &gt;= <span class="hljs-number">0</span>) &#123;<br>e -= <span class="hljs-number">2</span> * dx;<br><span class="hljs-keyword">if</span> (interChange) x += xSign;<br><span class="hljs-keyword">else</span> y += ySign;<br>&#125;<br>e += <span class="hljs-number">2</span> * dy;<br><span class="hljs-keyword">if</span> (interChange) y += ySign;<br><span class="hljs-keyword">else</span> x += xSign;<br>&#125;<br>&#125;<br></code></pre></td></tr></table></figure><h2 id="中点分割算法">中点分割算法</h2><p>emm这个算法比较简单，直接使用递归实现~</p><ul><li>已知当前线段的两个端点，然后找到线段的中点，绘制该点；</li><li>中点将线段划分为两个子线段；</li><li>将两个子线段，递归调用。</li></ul><p>实现代码如下~</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-function"><span class="hljs-type">int</span> <span class="hljs-title">CcgPHDDrawLineView::Midpoint</span><span class="hljs-params">(<span class="hljs-type">int</span> x1, <span class="hljs-type">int</span> y1, <span class="hljs-type">int</span> x2, <span class="hljs-type">int</span> y2, CDC * pDC)</span></span><br><span class="hljs-function"></span>&#123;<br><span class="hljs-type">float</span> averageError = <span class="hljs-number">0</span>;<br>CcgPHDDrawLineDoc *pDoc = (CcgPHDDrawLineDoc*)<span class="hljs-built_in">GetDocument</span>();<br>pDoc-&gt;m_averageError = <span class="hljs-number">0</span>;<br><span class="hljs-type">float</span> xMid = (x1 + x2) / <span class="hljs-number">2</span>, yMid = (y1 + y2) / <span class="hljs-number">2</span>;<br><br><span class="hljs-keyword">if</span> (<span class="hljs-built_in">abs</span>(x2 - x1) &lt; <span class="hljs-number">2</span> &amp;&amp; <span class="hljs-built_in">abs</span>(y2 - y1) &lt; <span class="hljs-number">2</span> )<br><span class="hljs-keyword">return</span> <span class="hljs-number">1</span>;<br>pDC-&gt;<span class="hljs-built_in">SetPixel</span>( <span class="hljs-built_in">int</span>(xMid+<span class="hljs-number">0.5</span>), <span class="hljs-built_in">int</span>(yMid+<span class="hljs-number">0.5</span>), <span class="hljs-built_in">RGB</span>(<span class="hljs-number">255</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>));<br><br><span class="hljs-comment">//误差计算</span><br>m_midError.time ++;<br>m_midError.error += (<span class="hljs-built_in">fabs</span>(m_midError.y2 * (xMid + <span class="hljs-number">0.5</span>) + m_midError.x2 * (yMid + <span class="hljs-number">0.5</span>))) / <br>(<span class="hljs-built_in">sqrt</span>(<span class="hljs-built_in">pow</span>(m_midError.y2, <span class="hljs-number">2</span>) + <span class="hljs-built_in">pow</span>(m_midError.x2, <span class="hljs-number">2</span>)));<br><br><span class="hljs-built_in">Midpoint</span>(x1, y1, xMid, yMid, pDC);<br><span class="hljs-built_in">Midpoint</span>(xMid, yMid, x2, y2, pDC);<br>&#125;<br></code></pre></td></tr></table></figure><p>不过通过绘制效果能看出来，很一般，，，，</p><h2 id="bresenham圆弧绘制算法">Bresenham圆弧绘制算法</h2><p>该圆弧算法和直线段绘制算法的思路基本一致。不太一样的地方就是在绘制圆弧的时候要考虑周边的三个顶点。</p><p><img src="/2018/12/26/%E7%9B%B4%E7%BA%BF%E6%AE%B5-%E5%9C%86%E5%BC%A7%E7%94%9F%E6%88%90%E7%AE%97%E6%B3%95/4.png"></p><p>在该图所示，我们需要考虑它右侧、下侧以及右下侧的三个顶点，然后从这三个顶点中选择一个作为下一个绘制顶点。这时候判断的依据是该顶点距离圆心的距离~</p><p>同理，我们也需要根据绘制圆弧的所在象限，决定x和y的增长方向。</p><p>实现代码如下~</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-function"><span class="hljs-type">void</span> <span class="hljs-title">CcgPHDDrawLineView::BresenhamArc</span><span class="hljs-params">(<span class="hljs-type">float</span> a, <span class="hljs-type">float</span> b, <span class="hljs-type">float</span> r, CDC * pDC)</span></span><br><span class="hljs-function"></span>&#123;<br><span class="hljs-type">int</span> x1, y1, x2, y2, xFlag = <span class="hljs-number">0</span>, yFlag = <span class="hljs-number">0</span>;<br><span class="hljs-type">float</span> PI = <span class="hljs-number">3.1415926</span>;<br>a = (a*PI) / <span class="hljs-number">180</span>; b = (b*PI) / <span class="hljs-number">180</span>;<br>x1 = r * <span class="hljs-built_in">cos</span>(a); y1 = r * <span class="hljs-built_in">sin</span>(a); x2 = r * <span class="hljs-built_in">cos</span>(b); y2 = r * <span class="hljs-built_in">sin</span>(b);<br><span class="hljs-type">float</span> x = x1, y = y1, r1, r2, r3, xtemp, ytemp;<br><br><span class="hljs-keyword">while</span> (<span class="hljs-built_in">sqrt</span>(<span class="hljs-built_in">pow</span>(y - y2, <span class="hljs-number">2</span>) + <span class="hljs-built_in">pow</span>(x - x2, <span class="hljs-number">2</span>)) &gt; <span class="hljs-number">1</span>)<br>&#123;<br>x &gt;= <span class="hljs-number">0</span> ? yFlag = <span class="hljs-number">1</span> : yFlag = <span class="hljs-number">-1</span>;<br>y &gt;= <span class="hljs-number">0</span> ? xFlag = <span class="hljs-number">-1</span> : xFlag = <span class="hljs-number">1</span>;<br>pDC-&gt;<span class="hljs-built_in">SetPixel</span>(x, -y, <span class="hljs-built_in">RGB</span>(<span class="hljs-number">255</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>));<br><br>xtemp = x + xFlag;<br>ytemp = y;<br>r1 = <span class="hljs-built_in">abs</span>(<span class="hljs-built_in">sqrt</span>( (xtemp*xtemp + ytemp*ytemp) ) - r);<br>xtemp = x + xFlag;<br>ytemp = y + yFlag;<br>r2 = <span class="hljs-built_in">abs</span>(<span class="hljs-built_in">sqrt</span>((xtemp*xtemp + ytemp*ytemp)) - r);<br>xtemp = x;<br>ytemp = y + yFlag;<br>r3 = <span class="hljs-built_in">abs</span>(<span class="hljs-built_in">sqrt</span>((xtemp*xtemp + ytemp*ytemp)) - r);<br><br><span class="hljs-keyword">if</span> (r1 &lt; r2 &amp;&amp; r1 &lt; r3) &#123;<br>x += xFlag;<br>&#125;<br><span class="hljs-keyword">else</span> <span class="hljs-keyword">if</span> (r2 &lt; r1 &amp;&amp; r2 &lt; r3) &#123;<br>x += xFlag;<br>y += yFlag;<br>&#125;<br><span class="hljs-keyword">else</span> &#123;<br>y += yFlag;<br>&#125;<br>&#125;<br>&#125;<br><br></code></pre></td></tr></table></figure>]]></content>
    
    
    <categories>
      
      <category>study</category>
      
    </categories>
    
    
    <tags>
      
      <tag>MFC</tag>
      
      <tag>c++</tag>
      
    </tags>
    
  </entry>
  
  
  
  
</search>
